{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69f4433b-1f3a-4778-a5c6-47c682bfbbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0066a186-0657-4cbb-9abc-1ed3324995e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "445282ad-b77a-4360-b702-c1665fa965d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total blocks: 3543\n",
      "Train: 3011 Val: 532\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "\n",
    "all_files = sorted(glob.glob(DATASET_DIR + \"/*.npz\"))\n",
    "print(\"Total blocks:\", len(all_files))\n",
    "\n",
    "# split\n",
    "split = int(len(all_files) * 0.85)\n",
    "train_files = all_files[:split]\n",
    "val_files   = all_files[split:]\n",
    "\n",
    "print(\"Train:\", len(train_files), \"Val:\", len(val_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92da727b-fc70-459f-9e26-f725e82f6f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [1, 2, 3, 6, 12]\n"
     ]
    }
   ],
   "source": [
    "def get_all_classes(files, max_files=200):\n",
    "    s = set()\n",
    "    for f in files[:max_files]:\n",
    "        d = np.load(f)\n",
    "        s.update(np.unique(d[\"y\"]).tolist())\n",
    "    return sorted(list(s))\n",
    "\n",
    "classes = get_all_classes(train_files)\n",
    "print(\"Classes:\", classes)\n",
    "\n",
    "class_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "idx_to_class = {i:c for c,i in class_to_idx.items()}\n",
    "\n",
    "NUM_CLASSES = len(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3965d56-3d5a-4708-891e-5c564f40b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, files, class_to_idx):\n",
    "        self.files = files\n",
    "        self.class_to_idx = class_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = np.load(self.files[idx])\n",
    "\n",
    "        points = d[\"X\"].astype(np.float32)\n",
    "        labels = d[\"y\"].astype(np.int64)\n",
    "\n",
    "        # SAFE label mapping\n",
    "        labels = np.array(\n",
    "            [self.class_to_idx.get(int(l), self.class_to_idx[1]) for l in labels],\n",
    "            dtype=np.int64\n",
    "        )\n",
    "\n",
    "        points = torch.from_numpy(points)\n",
    "        labels = torch.from_numpy(labels)\n",
    "\n",
    "        return points, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d11960c7-e714-48ba-ab1f-0089c4152eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: torch.Size([8, 4096, 10]) torch.Size([8, 4096])\n",
      "Labels range: 0 4\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = PointCloudDataset(train_files, class_to_idx)\n",
    "val_dataset   = PointCloudDataset(val_files, class_to_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "pts, lbl = next(iter(train_loader))\n",
    "print(\"Batch:\", pts.shape, lbl.shape)\n",
    "print(\"Labels range:\", lbl.min().item(), lbl.max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ffa8b3c-14df-4cbd-ade5-badf0198a41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: tensor([0.0265, 0.0378, 0.3009, 3.7421, 0.8927], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def compute_class_weights(loader, num_classes, max_batches=120):\n",
    "    counts = torch.zeros(num_classes)\n",
    "\n",
    "    for i, (_, y) in enumerate(loader):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "        for c in range(num_classes):\n",
    "            counts[c] += (y == c).sum()\n",
    "\n",
    "    weights = 1.0 / (counts + 1e-6)\n",
    "    weights = weights / weights.sum() * num_classes\n",
    "    return weights\n",
    "\n",
    "class_weights = compute_class_weights(train_loader, NUM_CLASSES).to(device)\n",
    "print(\"Weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad87a67a-8e90-4e03-8f4a-52501c0783a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min label: 0\n",
      "Max label: 4\n"
     ]
    }
   ],
   "source": [
    "pts, lbl = next(iter(train_loader))\n",
    "print(\"Min label:\", lbl.min().item())\n",
    "print(\"Max label:\", lbl.max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23b30eb4-6ff3-462e-b531-432c6761cd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [train]: 100%|██████████| 377/377 [00:41<00:00,  9.18it/s, loss=2.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train_loss=13.0721 | val_loss=2.1746 | val_acc=0.2949\n",
      "✅ Saved best: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 [train]: 100%|██████████| 377/377 [00:11<00:00, 32.96it/s, loss=1.67] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | train_loss=2.1163 | val_loss=2.9801 | val_acc=0.1987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 [train]: 100%|██████████| 377/377 [00:11<00:00, 33.83it/s, loss=1.46] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | train_loss=1.7750 | val_loss=1.0116 | val_acc=0.1744\n",
      "✅ Saved best: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 [train]: 100%|██████████| 377/377 [00:11<00:00, 33.64it/s, loss=1.92] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | train_loss=1.5302 | val_loss=2.0150 | val_acc=0.1540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 [train]: 100%|██████████| 377/377 [00:11<00:00, 34.07it/s, loss=1.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | train_loss=1.5314 | val_loss=0.9277 | val_acc=0.4009\n",
      "✅ Saved best: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 [train]: 100%|██████████| 377/377 [00:11<00:00, 33.99it/s, loss=1.35] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | train_loss=1.4430 | val_loss=0.5626 | val_acc=0.3863\n",
      "✅ Saved best: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 [train]: 100%|██████████| 377/377 [00:11<00:00, 32.90it/s, loss=1.36] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | train_loss=1.4219 | val_loss=0.7821 | val_acc=0.1534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 [train]: 100%|██████████| 377/377 [00:11<00:00, 33.91it/s, loss=1.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | train_loss=1.3981 | val_loss=0.6410 | val_acc=0.2604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 [train]: 100%|██████████| 377/377 [00:10<00:00, 34.39it/s, loss=1.46] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | train_loss=1.4148 | val_loss=0.6673 | val_acc=0.1713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 [train]: 100%|██████████| 377/377 [00:10<00:00, 34.32it/s, loss=1.48] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | train_loss=1.5382 | val_loss=2.0551 | val_acc=0.1540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 [train]: 100%|██████████| 377/377 [00:10<00:00, 34.43it/s, loss=1.41] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | train_loss=1.4048 | val_loss=0.5855 | val_acc=0.2101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 [train]: 100%|██████████| 377/377 [00:11<00:00, 33.72it/s, loss=1.38] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | train_loss=1.3831 | val_loss=0.5520 | val_acc=0.1873\n",
      "✅ Saved best: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 [train]: 100%|██████████| 377/377 [00:10<00:00, 34.68it/s, loss=1.76] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | train_loss=1.4457 | val_loss=1.0182 | val_acc=0.1232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 [train]: 100%|██████████| 377/377 [00:10<00:00, 36.45it/s, loss=1.69] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | train_loss=1.6021 | val_loss=0.7738 | val_acc=0.1540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 [train]: 100%|██████████| 377/377 [00:10<00:00, 34.41it/s, loss=1.52] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | train_loss=1.5578 | val_loss=1.6487 | val_acc=0.1448\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_PATH = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "\n",
    "NUM_CLASSES = 5  # because labels range 0..4 (confirmed)\n",
    "IN_FEATURES = 10\n",
    "\n",
    "class PointNetSmall(nn.Module):\n",
    "    def __init__(self, in_features=10, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Conv1d(in_features, 64, 1)\n",
    "        self.mlp2 = nn.Conv1d(64, 128, 1)\n",
    "        self.mlp3 = nn.Conv1d(128, 256, 1)\n",
    "        self.fc1  = nn.Conv1d(256, 128, 1)\n",
    "        self.fc2  = nn.Conv1d(128, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,N,F) -> (B,F,N)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu(self.mlp1(x))\n",
    "        x = F.relu(self.mlp2(x))\n",
    "        x = F.relu(self.mlp3(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)          # (B,C,N)\n",
    "        x = x.transpose(1, 2)    # (B,N,C)\n",
    "        return x\n",
    "\n",
    "model = PointNetSmall(IN_FEATURES, NUM_CLASSES).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "EPOCHS = 15\n",
    "best_val = 1e9\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "    # ---- TRAIN ----\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} [train]\")\n",
    "\n",
    "    for Xb, yb in pbar:\n",
    "        Xb = Xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        out = model(Xb)  # (B,N,C)\n",
    "\n",
    "        loss = criterion(out.reshape(-1, NUM_CLASSES), yb.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix(loss=float(loss.item()))\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # ---- VAL ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in val_loader:\n",
    "            Xb = Xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "            out = model(Xb)\n",
    "            loss = criterion(out.reshape(-1, NUM_CLASSES), yb.reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            pred = out.argmax(dim=2)\n",
    "            correct += (pred == yb).sum().item()\n",
    "            total += yb.numel()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n",
    "\n",
    "    # ---- SAVE BEST ----\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save({\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"num_classes\": NUM_CLASSES,\n",
    "            \"class_weights\": class_weights.detach().cpu()\n",
    "        }, MODEL_PATH)\n",
    "        print(\"✅ Saved best:\", MODEL_PATH)\n",
    "\n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa0498ab-80cd-4d14-9438-64c146f74c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 904455, 1: 643892, 2: 87404, 4: 25402, 3: 10015})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "for i, (_, y) in enumerate(train_loader):\n",
    "    if i > 50: break\n",
    "    u, cnt = torch.unique(y, return_counts=True)\n",
    "    for a,b in zip(u.tolist(), cnt.tolist()):\n",
    "        c[a] += b\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47694de1-a525-4273-8ee3-666233ebaeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights used: tensor([0.0265, 0.0378, 0.3009, 3.7421, 0.8927], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Class weights used:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c9e0812-06e6-4959-aa22-73b9bf8ede7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Allocated MB: 2.3447265625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Allocated MB:\", torch.cuda.memory_allocated()/1024/1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb246a1e-5898-47cb-828a-7a78b4360580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated MB: 2.3447265625\n",
      "Reserved  MB: 186.0\n",
      "Max alloc MB: 123.2470703125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Allocated MB:\", torch.cuda.memory_allocated()/1024/1024)\n",
    "print(\"Reserved  MB:\", torch.cuda.memory_reserved()/1024/1024)\n",
    "print(\"Max alloc MB:\", torch.cuda.max_memory_allocated()/1024/1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c36b6d1-5a9b-4c7d-8b64-cfc1301e4fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Model device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f99a2b65-aafe-4a0f-a5bc-f93b97d730b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "for step, (points, labels) in enumerate(train_loader):\n",
    "\n",
    "    points = points.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(points)\n",
    "    loss = criterion(outputs.reshape(-1, NUM_CLASSES), labels.reshape(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # GPU monitor every 50 steps\n",
    "    if step % 50 == 0 and device.type == \"cuda\":\n",
    "        if step == 0:\n",
    "            print(\"outputs device:\", outputs.device)\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ff1e16a-46ec-4b96-a717-c57a6b513cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1416015625\n",
      "186.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.memory_allocated()/1024/1024)\n",
    "print(torch.cuda.memory_reserved()/1024/1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "883f23d4-fdd8-48f0-9f08-52670ab7419d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found in blocks (sample): [1, 2, 3, 6, 7, 12]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "DATASET_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "files = sorted(glob.glob(DATASET_DIR + \"/*.npz\"))\n",
    "\n",
    "all_classes = set()\n",
    "for f in files[::50]:  # every 50th file (fast)\n",
    "    d = np.load(f)\n",
    "    all_classes.update(np.unique(d[\"y\"]).tolist())\n",
    "\n",
    "print(\"Classes found in blocks (sample):\", sorted(all_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d2aad7a-53b8-4c5e-99ba-71ef790fb9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"     # your .npz blocks\n",
    "MODEL_PATH  = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"\n",
    "\n",
    "# Full-scene prediction input (unclassified or stage outputs)\n",
    "PRED_IN_LAZ  = r\"D:/lidarrrrr/anbu/DX3035724 S.GIUSTO000001.laz\"\n",
    "PRED_OUT_LAS = r\"D:/lidarrrrr/anbu/New folder/dl_predicted_full.las\"\n",
    "PRED_OUT_LAZ = r\"D:/lidarrrrr/anbu/New folder/dl_predicted_full.laz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0124eb6-5d3b-44d3-ac29-84ecb294e997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "import os, glob, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "223ec1bd-3eb4-42b9-aca7-2afb0c8d5995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total blocks: 3543\n",
      "Train: 3011 Val: 532\n",
      "Example: D:/lidarrrrr/anbu/dl_dataset/blocks\\block_0000000.npz\n"
     ]
    }
   ],
   "source": [
    "all_files = sorted(glob.glob(os.path.join(DATASET_DIR, \"*.npz\")))\n",
    "if not all_files:\n",
    "    raise RuntimeError(\"No .npz blocks found in: \" + DATASET_DIR)\n",
    "\n",
    "print(\"Total blocks:\", len(all_files))\n",
    "\n",
    "split = int(len(all_files) * 0.85)\n",
    "train_files = all_files[:split]\n",
    "val_files   = all_files[split:]\n",
    "\n",
    "print(\"Train:\", len(train_files), \"Val:\", len(val_files))\n",
    "print(\"Example:\", train_files[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fdb4f99-b5e6-4266-a5e0-6cc620feb708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original classes found in blocks (sample): [1, 2, 3, 6, 7, 12, 13]\n",
      "NUM_CLASSES: 7\n",
      "class_to_idx: {1: 0, 2: 1, 3: 2, 6: 3, 7: 4, 12: 5, 13: 6}\n"
     ]
    }
   ],
   "source": [
    "def detect_classes(files, stride=30):\n",
    "    s = set()\n",
    "    for f in files[::stride]:\n",
    "        d = np.load(f)\n",
    "        s.update(np.unique(d[\"y\"]).tolist())\n",
    "    return sorted(list(s))\n",
    "\n",
    "orig_classes = detect_classes(train_files, stride=20)\n",
    "print(\"Original classes found in blocks (sample):\", orig_classes)\n",
    "\n",
    "# IMPORTANT: ensure DEFAULT class 1 exists for fallback\n",
    "if 1 not in orig_classes:\n",
    "    orig_classes = [1] + orig_classes\n",
    "\n",
    "orig_classes = sorted(orig_classes)\n",
    "\n",
    "class_to_idx = {c:i for i,c in enumerate(orig_classes)}\n",
    "idx_to_class = {i:c for c,i in class_to_idx.items()}\n",
    "\n",
    "NUM_CLASSES = len(orig_classes)\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES)\n",
    "print(\"class_to_idx:\", class_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9d484ee-bee9-4723-859c-68c7a76cff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockDataset(Dataset):\n",
    "    def __init__(self, files, class_to_idx):\n",
    "        self.files = files\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.default_idx = self.class_to_idx.get(1, 0)  # fallback to class 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = np.load(self.files[idx])\n",
    "        X = d[\"X\"].astype(np.float32)   # (N, F)\n",
    "        y = d[\"y\"].astype(np.int64)     # (N,)\n",
    "\n",
    "        # ---- per-block normalization (VERY IMPORTANT) ----\n",
    "        mu = X.mean(axis=0, keepdims=True)\n",
    "        sd = X.std(axis=0, keepdims=True) + 1e-6\n",
    "        X = (X - mu) / sd\n",
    "\n",
    "        # ---- safe label remap (unknown -> default class 1) ----\n",
    "        y = np.array([self.class_to_idx.get(int(v), self.default_idx) for v in y], dtype=np.int64)\n",
    "\n",
    "        return torch.from_numpy(X), torch.from_numpy(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c1bfb7f-b34c-473c-bd93-f783b3420923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare set used: [3, 6, 7, 12, 13]\n",
      "Sampler weights stats: 1.0 5.0\n"
     ]
    }
   ],
   "source": [
    "def block_weight(npz_path, rare_set):\n",
    "    d = np.load(npz_path)\n",
    "    y = d[\"y\"]\n",
    "    u = set(np.unique(y).tolist())\n",
    "    # weight up if block contains rare classes\n",
    "    return 5.0 if len(u.intersection(rare_set)) > 0 else 1.0\n",
    "\n",
    "# Pick rare classes (you can edit)\n",
    "# Typically: 7(outliers), 9(sea), 10(bridge), 12(overlap), 13(lakes)\n",
    "rare_classes = {7, 9, 10, 12, 13, 6, 3}  # include 6/3 to strengthen building/veg\n",
    "rare_set = set([c for c in rare_classes if c in orig_classes])\n",
    "\n",
    "weights = np.array([block_weight(f, rare_set) for f in train_files], dtype=np.float32)\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(train_files), replacement=True)\n",
    "\n",
    "print(\"Rare set used:\", sorted(list(rare_set)))\n",
    "print(\"Sampler weights stats:\", float(weights.min()), float(weights.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a7792bf-6efb-44e6-822d-3258a54ad643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: torch.Size([8, 4096, 10]) torch.Size([8, 4096])\n",
      "Label range: 0 5\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8  # RTX3050 safe; try 12 if stable\n",
    "\n",
    "train_ds = BlockDataset(train_files, class_to_idx)\n",
    "val_ds   = BlockDataset(val_files, class_to_idx)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,          # balanced sampling\n",
    "    num_workers=0,            # Windows safe\n",
    "    pin_memory=(device.type==\"cuda\")\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=(device.type==\"cuda\")\n",
    ")\n",
    "\n",
    "Xb, yb = next(iter(train_loader))\n",
    "print(\"Batch:\", Xb.shape, yb.shape)\n",
    "print(\"Label range:\", yb.min().item(), yb.max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c054563-9427-48f4-a225-b96a5696196e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting labels: 100%|██████████| 120/120 [00:02<00:00, 45.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([1.3090e-04, 1.6267e-04, 1.0486e-03, 7.3461e-03, 6.4695e+00, 3.1641e-03,\n",
      "        5.1860e-01], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_class_weights(loader, num_classes, max_batches=120):\n",
    "    counts = torch.zeros(num_classes, dtype=torch.float64)\n",
    "\n",
    "    it = iter(loader)\n",
    "    for _ in tqdm(range(max_batches), desc=\"Counting labels\"):\n",
    "        try:\n",
    "            _, y = next(it)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        y = y.reshape(-1)\n",
    "        for c in range(num_classes):\n",
    "            counts[c] += (y == c).sum().item()\n",
    "\n",
    "    counts = counts + 1.0\n",
    "    w = 1.0 / counts\n",
    "    w = w / w.sum() * num_classes\n",
    "    return w.float()\n",
    "\n",
    "class_weights = compute_class_weights(train_loader, NUM_CLASSES, max_batches=120).to(device)\n",
    "print(\"Class weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a13081e7-555a-4b07-8d01-1e827040930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetSmall(nn.Module):\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Conv1d(in_features, 64, 1)\n",
    "        self.mlp2 = nn.Conv1d(64, 128, 1)\n",
    "        self.mlp3 = nn.Conv1d(128, 256, 1)\n",
    "        self.head1 = nn.Conv1d(256, 128, 1)\n",
    "        self.head2 = nn.Conv1d(128, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,N,F) -> (B,F,N)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu(self.mlp1(x))\n",
    "        x = F.relu(self.mlp2(x))\n",
    "        x = F.relu(self.mlp3(x))\n",
    "        x = F.relu(self.head1(x))\n",
    "        x = self.head2(x)         # (B,C,N)\n",
    "        x = x.transpose(1, 2)     # (B,N,C)\n",
    "        return x\n",
    "\n",
    "IN_FEATURES = Xb.shape[-1]\n",
    "model = PointNetSmall(IN_FEATURES, NUM_CLASSES).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4faaede8-e1d1-4dab-bdf1-c42119cf17fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 30.25it/s, loss=1.81] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train_loss=1.5487 | val_loss=1.6972 | val_acc=0.2226\n",
      "✅ Saved best: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 31.06it/s, loss=0.808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | train_loss=1.1831 | val_loss=1.2063 | val_acc=0.1685\n",
      "✅ Saved best: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 29.61it/s, loss=0.936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | train_loss=1.1421 | val_loss=1.3194 | val_acc=0.1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 29.39it/s, loss=1.83] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | train_loss=1.0946 | val_loss=1.3655 | val_acc=0.2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 30.37it/s, loss=0.728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | train_loss=0.9832 | val_loss=1.1286 | val_acc=0.1989\n",
      "✅ Saved best: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 29.42it/s, loss=0.49] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | train_loss=0.9446 | val_loss=1.3426 | val_acc=0.2790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 30.46it/s, loss=0.901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | train_loss=0.9469 | val_loss=1.0853 | val_acc=0.5030\n",
      "✅ Saved best: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 29.58it/s, loss=1.09] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | train_loss=0.9218 | val_loss=0.9122 | val_acc=0.4013\n",
      "✅ Saved best: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 [train]: 100%|██████████| 377/377 [00:13<00:00, 29.00it/s, loss=1.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | train_loss=0.8993 | val_loss=0.8368 | val_acc=0.4057\n",
      "✅ Saved best: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 29.25it/s, loss=2.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | train_loss=0.8586 | val_loss=0.7715 | val_acc=0.4548\n",
      "✅ Saved best: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 29.50it/s, loss=0.578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | train_loss=0.8612 | val_loss=0.8707 | val_acc=0.4454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 29.26it/s, loss=0.767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | train_loss=0.8478 | val_loss=1.0972 | val_acc=0.5255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 29.25it/s, loss=0.656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | train_loss=0.8679 | val_loss=1.0264 | val_acc=0.4456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 29.70it/s, loss=0.87] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | train_loss=0.8634 | val_loss=0.8471 | val_acc=0.3676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 29.56it/s, loss=1.12] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | train_loss=0.8485 | val_loss=1.2059 | val_acc=0.6345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 29.78it/s, loss=0.734]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | train_loss=0.8163 | val_loss=1.2254 | val_acc=0.5245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 30.29it/s, loss=0.732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | train_loss=0.8012 | val_loss=0.8935 | val_acc=0.4126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 30.04it/s, loss=0.95] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | train_loss=0.8067 | val_loss=1.2416 | val_acc=0.4414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 29.57it/s, loss=0.275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | train_loss=0.8527 | val_loss=1.1834 | val_acc=0.4737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 29.57it/s, loss=1.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | train_loss=0.8222 | val_loss=1.3881 | val_acc=0.4734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 29.63it/s, loss=0.765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 | train_loss=0.8297 | val_loss=1.2629 | val_acc=0.4863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 30.01it/s, loss=0.662]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 | train_loss=0.8790 | val_loss=1.3061 | val_acc=0.5960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 29.35it/s, loss=0.614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 | train_loss=0.8391 | val_loss=1.3299 | val_acc=0.5522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 30.03it/s, loss=0.665]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 | train_loss=0.7962 | val_loss=1.1558 | val_acc=0.5011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25 [train]: 100%|██████████| 377/377 [00:12<00:00, 29.98it/s, loss=0.48] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 | train_loss=0.7780 | val_loss=1.3703 | val_acc=0.5870\n",
      "DONE training. Best val_loss: 0.7715395947000874\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "\n",
    "EPOCHS = 25\n",
    "best_val = 1e9\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # ---- train ----\n",
    "    model.train()\n",
    "    tr_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} [train]\")\n",
    "\n",
    "    for X, y in pbar:\n",
    "        X = X.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        out = model(X)  # (B,N,C)\n",
    "\n",
    "        loss = criterion(out.reshape(-1, NUM_CLASSES), y.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        pbar.set_postfix(loss=float(loss.item()))\n",
    "\n",
    "    tr_loss /= len(train_loader)\n",
    "\n",
    "    # ---- val ----\n",
    "    model.eval()\n",
    "    va_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X = X.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            out = model(X)\n",
    "            loss = criterion(out.reshape(-1, NUM_CLASSES), y.reshape(-1))\n",
    "            va_loss += loss.item()\n",
    "\n",
    "            pred = out.argmax(dim=2)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.numel()\n",
    "\n",
    "    va_loss /= len(val_loader)\n",
    "    va_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch} | train_loss={tr_loss:.4f} | val_loss={va_loss:.4f} | val_acc={va_acc:.4f}\")\n",
    "\n",
    "    # save best\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        torch.save({\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"in_features\": IN_FEATURES,\n",
    "            \"num_classes\": NUM_CLASSES,\n",
    "            \"orig_classes\": orig_classes,   # to map back to LAS classes\n",
    "        }, MODEL_PATH)\n",
    "        print(\"✅ Saved best:\", MODEL_PATH)\n",
    "\n",
    "print(\"DONE training. Best val_loss:\", best_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a67c2cbc-b2c5-4955-b44b-15924ab5cde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model. Classes: [1, 2, 3, 6, 7, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "import laspy\n",
    "\n",
    "ckpt = torch.load(MODEL_PATH, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "orig_classes = ckpt[\"orig_classes\"]\n",
    "idx_to_class = {i:c for i,c in enumerate(orig_classes)}\n",
    "NUM_CLASSES  = ckpt[\"num_classes\"]\n",
    "IN_FEATURES  = ckpt[\"in_features\"]\n",
    "\n",
    "print(\"Loaded model. Classes:\", orig_classes)\n",
    "\n",
    "las = laspy.read(PRED_IN_LAZ)\n",
    "xyz = np.vstack([las.x, las.y, las.z]).T.astype(np.float32)\n",
    "\n",
    "# ---- IMPORTANT ----\n",
    "# For DL inference you must use the SAME 10 features used in blocks.\n",
    "# If your blocks already store 10 features computed earlier,\n",
    "# you MUST recompute them in exactly the same way.\n",
    "#\n",
    "# If your \"X\" feature is already (z, hag, slope, intensity, returns etc),\n",
    "# you must paste your same feature-extractor here.\n",
    "#\n",
    "# For now we will assume your blocks used raw xyz+intensity+returns style:\n",
    "# (This is placeholder — replace with your actual feature builder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c11f65df-6386-41f8-b593-8d9c3eb80b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (4096, 10)\n",
      "First row: [ 1.6250000e+00 -5.5000000e+00  6.9839835e-01  1.1400001e+00\n",
      "  2.7481000e+04  1.0000000e+00  2.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "Feature mins: [-10.3125     -10.          -0.6516018   -0.07000017   0.\n",
      "   1.           1.           0.           0.           0.        ]\n",
      "Feature maxs: [4.6250000e+00 6.5000000e+00 2.5483985e+00 3.0700002e+00 4.4541000e+04\n",
      " 3.0000000e+00 3.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "d = np.load(train_files[0])\n",
    "print(\"X shape:\", d[\"X\"].shape)\n",
    "print(\"First row:\", d[\"X\"][0])\n",
    "print(\"Feature mins:\", d[\"X\"].min(axis=0))\n",
    "print(\"Feature maxs:\", d[\"X\"].max(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cba908fd-6080-4de3-aca8-cccbdba16c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model. Classes: [1, 2, 3, 6, 7, 12, 13]\n",
      "Feature matrix shape: (12374846, 10)\n",
      "Prediction finished.\n",
      "Saved LAS: D:/lidarrrrr/anbu/New folder/dl_predicted.las\n",
      "Saved LAZ: D:/lidarrrrr/anbu/New folder/dl_predicted.laz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import laspy\n",
    "\n",
    "# paths\n",
    "MODEL_PATH = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"\n",
    "IN_LAZ     = r\"D:/lidarrrrr/anbu/DX3035724 S.GIUSTO000001.laz\"\n",
    "OUT_LAS    = r\"D:/lidarrrrr/anbu/New folder/dl_predicted.las\"\n",
    "OUT_LAZ    = r\"D:/lidarrrrr/anbu/New folder/dl_predicted.laz\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------------------\n",
    "# Load model\n",
    "# ----------------------------\n",
    "ckpt = torch.load(MODEL_PATH, map_location=device)\n",
    "\n",
    "orig_classes = ckpt[\"orig_classes\"]\n",
    "NUM_CLASSES  = ckpt[\"num_classes\"]\n",
    "IN_FEATURES  = ckpt[\"in_features\"]\n",
    "\n",
    "idx_to_class = {i:c for i,c in enumerate(orig_classes)}\n",
    "\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded model. Classes:\", orig_classes)\n",
    "\n",
    "# ----------------------------\n",
    "# Load LAS\n",
    "# ----------------------------\n",
    "las = laspy.read(IN_LAZ)\n",
    "\n",
    "xyz = np.vstack([las.x, las.y, las.z]).T.astype(np.float32)\n",
    "\n",
    "z = xyz[:,2]\n",
    "\n",
    "# intensity and returns\n",
    "intensity = np.asarray(las.intensity).astype(np.float32)\n",
    "ret_num   = np.asarray(las.return_number).astype(np.float32)\n",
    "n_returns = np.asarray(las.number_of_returns).astype(np.float32)\n",
    "\n",
    "# ----------------------------\n",
    "# Rebuild SAME features as training\n",
    "# ----------------------------\n",
    "\n",
    "# simple relative height approximation\n",
    "z_min = z.min()\n",
    "hag = z - z_min\n",
    "\n",
    "# simple slope proxy\n",
    "local_range = np.zeros_like(z)\n",
    "slope = np.zeros_like(z)\n",
    "\n",
    "# placeholder columns (must match training shape)\n",
    "f8 = np.zeros_like(z)\n",
    "f9 = np.zeros_like(z)\n",
    "f10 = np.zeros_like(z)\n",
    "\n",
    "X = np.stack([\n",
    "    z,\n",
    "    hag,\n",
    "    slope,\n",
    "    local_range,\n",
    "    intensity,\n",
    "    ret_num,\n",
    "    n_returns,\n",
    "    f8,\n",
    "    f9,\n",
    "    f10\n",
    "], axis=1).astype(np.float32)\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "\n",
    "# ----------------------------\n",
    "# Predict in batches\n",
    "# ----------------------------\n",
    "BATCH = 4096\n",
    "\n",
    "pred_labels = np.zeros(len(X), dtype=np.int32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X), BATCH):\n",
    "        chunk = X[i:i+BATCH]\n",
    "\n",
    "        # normalize same way as training\n",
    "        mu = chunk.mean(axis=0, keepdims=True)\n",
    "        sd = chunk.std(axis=0, keepdims=True) + 1e-6\n",
    "        chunk = (chunk - mu) / sd\n",
    "\n",
    "        pts = torch.from_numpy(chunk).unsqueeze(0).to(device)\n",
    "        out = model(pts)\n",
    "        pred = out.argmax(dim=2).cpu().numpy()[0]\n",
    "\n",
    "        pred_labels[i:i+BATCH] = pred\n",
    "\n",
    "print(\"Prediction finished.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Map back to LAS classes\n",
    "# ----------------------------\n",
    "mapped = np.array([idx_to_class[int(p)] for p in pred_labels], dtype=np.uint8)\n",
    "\n",
    "las.classification = mapped\n",
    "\n",
    "las.write(OUT_LAS)\n",
    "print(\"Saved LAS:\", OUT_LAS)\n",
    "\n",
    "try:\n",
    "    las.write(OUT_LAZ)\n",
    "    print(\"Saved LAZ:\", OUT_LAZ)\n",
    "except:\n",
    "    print(\"LAZ writing failed (ok)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e4db2d0-0968-4b47-b05f-6d53b4d08d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: DX3011148 ULMIANO000001.laz blocks: 978\n",
      "Done: DX3011148 ULMIANO000002.laz blocks: 846\n",
      "Done: DX3011148 ULMIANO000003.laz blocks: 3549\n",
      "Done: DX3011148 ULMIANO000004.laz blocks: 797\n",
      "Done: DX3011148 ULMIANO000005.laz blocks: 6957\n",
      "Done: DX3011148 ULMIANO000006.laz blocks: 1997\n",
      "Done: DX3011148 ULMIANO000007.laz blocks: 4790\n",
      "Done: DX3011148 ULMIANO000008.laz blocks: 2718\n",
      "Done: DX3011148 ULMIANO000009.laz blocks: 1678\n",
      "Done: pt013390.laz blocks: 3387\n",
      "Total blocks saved: 27697\n",
      "OUT_DIR: D:/lidarrrrr/anbu/dl_dataset/blocks\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# 01_make_blocks.py\n",
    "# - Reads labeled LAS/LAZ\n",
    "# - Creates blocks of N points\n",
    "# - Saves .npz with keys: X, y, idx\n",
    "# ============================\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import laspy\n",
    "\n",
    "# -------------------- SETTINGS --------------------\n",
    "LABELED_DIR = r\"D:/lidarrrrr/anbu/training_labeled\"\n",
    "OUT_DIR     = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "NPTS        = 4096\n",
    "\n",
    "# keep only these classes (edit if you want)\n",
    "KEEP_CLASSES = [1, 2, 3, 6, 7, 12, 13]  # default, ground, veg, building, tower?, etc.\n",
    "\n",
    "# Feature definition (must match training + prediction)\n",
    "# 10 features total (example):\n",
    "# 0: x_centered, 1: y_centered, 2: z_centered,\n",
    "# 3: height_above_local_min (HAG approx),\n",
    "# 4: intensity,\n",
    "# 5: return_number,\n",
    "# 6: number_of_returns,\n",
    "# 7: scan_angle (or 0),\n",
    "# 8: deviation (or 0),\n",
    "# 9: z_local_range\n",
    "CELL = 2.0  # meters\n",
    "\n",
    "# -------------------- HELPERS --------------------\n",
    "def get_dim(las, name, fallback=0.0):\n",
    "    \"\"\"Safely read LAS dimension; fallback zeros if missing.\"\"\"\n",
    "    try:\n",
    "        return np.asarray(las[name])\n",
    "    except Exception:\n",
    "        try:\n",
    "            if name == \"scan_angle\":\n",
    "                return np.asarray(las[\"scan_angle_rank\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "    return np.full(len(las.x), fallback, dtype=np.float32)\n",
    "\n",
    "def grid_stats_zmin_zmax(x, y, z, cell):\n",
    "    minx, miny = float(x.min()), float(y.min())\n",
    "    gx = np.floor((x - minx) / cell).astype(np.int32)\n",
    "    gy = np.floor((y - miny) / cell).astype(np.int32)\n",
    "    key = gx.astype(np.int64) * 1_000_000 + gy.astype(np.int64)\n",
    "\n",
    "    order = np.argsort(key)\n",
    "    key_s = key[order]\n",
    "    z_s   = z[order]\n",
    "\n",
    "    uniq, start = np.unique(key_s, return_index=True)\n",
    "    zmin = np.full(len(uniq), np.inf, dtype=np.float32)\n",
    "    zmax = np.full(len(uniq), -np.inf, dtype=np.float32)\n",
    "\n",
    "    for i in range(len(uniq)):\n",
    "        a = start[i]\n",
    "        b = start[i+1] if i+1 < len(uniq) else len(key_s)\n",
    "        zs = z_s[a:b]\n",
    "        zmin[i] = zs.min()\n",
    "        zmax[i] = zs.max()\n",
    "\n",
    "    pos = np.searchsorted(uniq, key)\n",
    "    pos = np.clip(pos, 0, len(uniq)-1)\n",
    "    return zmin[pos], zmax[pos]\n",
    "\n",
    "def make_features(xyz, intensity, ret_num, n_returns, scan_angle, deviation, cell=CELL):\n",
    "    x = xyz[:,0].astype(np.float32)\n",
    "    y = xyz[:,1].astype(np.float32)\n",
    "    z = xyz[:,2].astype(np.float32)\n",
    "\n",
    "    zmin, zmax = grid_stats_zmin_zmax(x, y, z, cell)\n",
    "    hag = (z - zmin).astype(np.float32)\n",
    "    zrange = (zmax - zmin).astype(np.float32)\n",
    "\n",
    "    # center xyz to reduce scale issues\n",
    "    x0 = x - x.mean()\n",
    "    y0 = y - y.mean()\n",
    "    z0 = z - z.mean()\n",
    "\n",
    "    X = np.stack([\n",
    "        x0, y0, z0,\n",
    "        hag,\n",
    "        intensity.astype(np.float32),\n",
    "        ret_num.astype(np.float32),\n",
    "        n_returns.astype(np.float32),\n",
    "        scan_angle.astype(np.float32),\n",
    "        deviation.astype(np.float32),\n",
    "        zrange\n",
    "    ], axis=1).astype(np.float32)\n",
    "\n",
    "    return X\n",
    "\n",
    "# -------------------- MAIN --------------------\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(LABELED_DIR, \"*.la*\")))\n",
    "if not files:\n",
    "    raise RuntimeError(\"No LAS/LAZ found in LABELED_DIR\")\n",
    "\n",
    "block_id = 0\n",
    "for fp in files:\n",
    "    las = laspy.read(fp)\n",
    "\n",
    "    xyz = np.vstack([las.x, las.y, las.z]).T\n",
    "    cls = np.asarray(las.classification, dtype=np.int32)\n",
    "\n",
    "    intensity = get_dim(las, \"intensity\", 0.0)\n",
    "    ret_num   = get_dim(las, \"return_number\", 1.0)\n",
    "    n_returns = get_dim(las, \"number_of_returns\", 1.0)\n",
    "    scan_angle = get_dim(las, \"scan_angle\", 0.0)\n",
    "    deviation  = get_dim(las, \"Deviation\", 0.0)\n",
    "\n",
    "    # keep only wanted classes\n",
    "    keep = np.isin(cls, KEEP_CLASSES)\n",
    "    idx_all = np.where(keep)[0]\n",
    "    if len(idx_all) < NPTS:\n",
    "        print(\"Skip (too few points):\", os.path.basename(fp), len(idx_all))\n",
    "        continue\n",
    "\n",
    "    X_all = make_features(xyz[idx_all], intensity[idx_all], ret_num[idx_all], n_returns[idx_all],\n",
    "                          scan_angle[idx_all], deviation[idx_all])\n",
    "\n",
    "    y_all = cls[idx_all].astype(np.int32)\n",
    "\n",
    "    # shuffle for blocks\n",
    "    rng = np.random.default_rng(42)\n",
    "    perm = rng.permutation(len(idx_all))\n",
    "\n",
    "    # make blocks\n",
    "    nblocks = len(idx_all) // NPTS\n",
    "    for b in range(nblocks):\n",
    "        s = b * NPTS\n",
    "        e = s + NPTS\n",
    "        sel = perm[s:e]\n",
    "\n",
    "        Xb = X_all[sel]\n",
    "        yb = y_all[sel]\n",
    "        idxb = idx_all[sel]  # IMPORTANT: maps back to original LAS indices\n",
    "\n",
    "        out = os.path.join(OUT_DIR, f\"block_{block_id:07d}.npz\")\n",
    "        np.savez_compressed(out, X=Xb, y=yb, idx=idxb)\n",
    "        block_id += 1\n",
    "\n",
    "    print(\"Done:\", os.path.basename(fp), \"blocks:\", nblocks)\n",
    "\n",
    "print(\"Total blocks saved:\", block_id)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bca99897-33a7-4bd8-9ed3-572099725965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | GPU: NVIDIA GeForce RTX 3050\n",
      "Total blocks: 27697\n",
      "Train: 23542 Val: 4155\n",
      "Example: D:/lidarrrrr/anbu/dl_dataset/blocks\\block_0022740.npz\n",
      "Classes: [1, 2, 3, 6, 7, 12, 13]\n",
      "NUM_CLASSES: 7 | FEATS: 10\n",
      "Feature mean: [-2.3929443e-02  4.3289871e+01  3.9696936e-03  3.1222883e-01\n",
      "  3.6305730e+04  1.0526814e+00  1.0699357e+00 -1.4197598e+02\n",
      "  7.9566729e-01  9.0607953e-01]\n",
      "Feature std : [2.9278833e+02 2.3654257e+02 3.6797585e+00 9.5976192e-01 6.7891382e+03\n",
      " 2.6334301e-01 3.0616912e-01 9.0680487e+02 4.3515396e+00 1.5073984e+00]\n",
      "Class weights: tensor([5.9334e-04, 5.8850e-04, 5.3555e-03, 7.1202e-03, 5.0000e+00, 1.2393e-02,\n",
      "        5.7689e-01], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [train]: 100%|██████████| 2943/2943 [06:31<00:00,  7.52it/s, loss=0.262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | train_loss=0.4242 | val_loss=0.3931 | val_acc=0.7240\n",
      "✅ Saved best model: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 [train]: 100%|██████████| 2943/2943 [01:33<00:00, 31.49it/s, loss=0.295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 | train_loss=0.3100 | val_loss=0.4024 | val_acc=0.7325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 [train]: 100%|██████████| 2943/2943 [03:33<00:00, 13.81it/s, loss=0.187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 | train_loss=0.2862 | val_loss=0.3270 | val_acc=0.7997\n",
      "✅ Saved best model: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 [train]: 100%|██████████| 2943/2943 [05:13<00:00,  9.39it/s, loss=0.282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 | train_loss=0.2625 | val_loss=0.3059 | val_acc=0.7539\n",
      "✅ Saved best model: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 [train]: 100%|██████████| 2943/2943 [01:33<00:00, 31.59it/s, loss=0.23] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 | train_loss=0.2607 | val_loss=0.3729 | val_acc=0.7497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 [train]: 100%|██████████| 2943/2943 [01:33<00:00, 31.54it/s, loss=0.263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 | train_loss=0.2477 | val_loss=0.2892 | val_acc=0.7811\n",
      "✅ Saved best model: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 [train]: 100%|██████████| 2943/2943 [01:31<00:00, 32.15it/s, loss=0.133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 | train_loss=0.2397 | val_loss=0.2850 | val_acc=0.7707\n",
      "✅ Saved best model: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 [train]: 100%|██████████| 2943/2943 [01:41<00:00, 29.10it/s, loss=0.218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 | train_loss=0.2346 | val_loss=0.2875 | val_acc=0.7923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 [train]: 100%|██████████| 2943/2943 [01:25<00:00, 34.24it/s, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 | train_loss=0.2294 | val_loss=0.2571 | val_acc=0.7920\n",
      "✅ Saved best model: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 [train]: 100%|██████████| 2943/2943 [01:30<00:00, 32.50it/s, loss=0.191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 | train_loss=0.2304 | val_loss=0.2393 | val_acc=0.8039\n",
      "✅ Saved best model: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 [train]: 100%|██████████| 2943/2943 [01:27<00:00, 33.46it/s, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 | train_loss=0.2211 | val_loss=0.3168 | val_acc=0.7760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 [train]: 100%|██████████| 2943/2943 [01:29<00:00, 32.94it/s, loss=0.241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 | train_loss=0.2211 | val_loss=0.2911 | val_acc=0.7842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 [train]: 100%|██████████| 2943/2943 [01:26<00:00, 33.97it/s, loss=0.149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 | train_loss=0.2165 | val_loss=0.2360 | val_acc=0.8076\n",
      "✅ Saved best model: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 [train]: 100%|██████████| 2943/2943 [01:31<00:00, 32.29it/s, loss=0.199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 | train_loss=0.2213 | val_loss=0.3090 | val_acc=0.8109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 [train]: 100%|██████████| 2943/2943 [01:29<00:00, 33.02it/s, loss=0.225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 | train_loss=0.2180 | val_loss=0.3567 | val_acc=0.7800\n",
      "DONE TRAINING.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# 02_train_pointnet.py\n",
    "# - Loads blocks (.npz) with keys: X, y, idx\n",
    "# - Builds class_to_idx mapping\n",
    "# - Normalizes features using train stats\n",
    "# - Trains PointNet-small on GPU\n",
    "# - Saves best model to: MODEL_OUT\n",
    "# ============================\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------- SETTINGS ----------------\n",
    "BLOCK_DIR  = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "MODEL_OUT  = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"\n",
    "\n",
    "EPOCHS     = 15\n",
    "BATCH_SIZE = 8\n",
    "LR         = 1e-3\n",
    "NPTS       = 4096\n",
    "MAX_WEIGHT = 5.0  # clip class weights to avoid exploding loss\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE, \"| GPU:\", torch.cuda.get_device_name(0) if DEVICE==\"cuda\" else \"None\")\n",
    "\n",
    "os.makedirs(os.path.dirname(MODEL_OUT), exist_ok=True)\n",
    "\n",
    "# ---------------- DATA ----------------\n",
    "all_files = sorted(glob.glob(os.path.join(BLOCK_DIR, \"*.npz\")))\n",
    "if not all_files:\n",
    "    raise RuntimeError(\"No .npz blocks found. Run 01_make_blocks.py first.\")\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(all_files)\n",
    "\n",
    "split = int(0.85 * len(all_files))\n",
    "train_files = all_files[:split]\n",
    "val_files   = all_files[split:]\n",
    "\n",
    "print(\"Total blocks:\", len(all_files))\n",
    "print(\"Train:\", len(train_files), \"Val:\", len(val_files))\n",
    "print(\"Example:\", train_files[0])\n",
    "\n",
    "# ---- find classes from a sample (fast) ----\n",
    "def scan_classes(files, max_files=300):\n",
    "    s = set()\n",
    "    for fp in files[:max_files]:\n",
    "        d = np.load(fp)\n",
    "        u = np.unique(d[\"y\"])\n",
    "        for v in u.tolist():\n",
    "            s.add(int(v))\n",
    "    return sorted(list(s))\n",
    "\n",
    "classes = scan_classes(train_files, max_files=400)\n",
    "class_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "idx_to_class = {i:c for c,i in class_to_idx.items()}\n",
    "NUM_CLASSES = len(classes)\n",
    "FEATS = 10\n",
    "\n",
    "print(\"Classes:\", classes)\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES, \"| FEATS:\", FEATS)\n",
    "\n",
    "# ---- compute feature mean/std on train sample ----\n",
    "def compute_norm(files, max_blocks=400):\n",
    "    xs = []\n",
    "    for fp in files[:max_blocks]:\n",
    "        d = np.load(fp)\n",
    "        X = d[\"X\"].astype(np.float32)  # (N,10)\n",
    "        xs.append(X)\n",
    "    Xcat = np.concatenate(xs, axis=0)\n",
    "    mean = Xcat.mean(axis=0)\n",
    "    std  = Xcat.std(axis=0) + 1e-6\n",
    "    return mean.astype(np.float32), std.astype(np.float32)\n",
    "\n",
    "Xmean, Xstd = compute_norm(train_files, max_blocks=300)\n",
    "print(\"Feature mean:\", Xmean)\n",
    "print(\"Feature std :\", Xstd)\n",
    "\n",
    "# ---- dataset ----\n",
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, files, class_to_idx, Xmean, Xstd):\n",
    "        self.files = files\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.Xmean = Xmean\n",
    "        self.Xstd  = Xstd\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        d = np.load(self.files[i])\n",
    "        X = d[\"X\"].astype(np.float32)  # (N,10)\n",
    "        y = d[\"y\"].astype(np.int64)    # (N,)\n",
    "\n",
    "        # normalize features\n",
    "        X = (X - self.Xmean) / self.Xstd\n",
    "\n",
    "        # map labels -> 0..C-1 (avoid CUDA assert)\n",
    "        # any unknown label -> ignore (-1)\n",
    "        mapped = np.full_like(y, fill_value=-1, dtype=np.int64)\n",
    "        for orig, idx in self.class_to_idx.items():\n",
    "            mapped[y == orig] = idx\n",
    "\n",
    "        X = torch.from_numpy(X)          # (N,10)\n",
    "        y = torch.from_numpy(mapped)     # (N,)\n",
    "        return X, y\n",
    "\n",
    "# Windows safer first: num_workers=0 (later you can increase)\n",
    "train_ds = PointCloudDataset(train_files, class_to_idx, Xmean, Xstd)\n",
    "val_ds   = PointCloudDataset(val_files,   class_to_idx, Xmean, Xstd)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# ---------------- CLASS WEIGHTS ----------------\n",
    "@torch.no_grad()\n",
    "def compute_class_weights(loader, num_classes, max_batches=200):\n",
    "    counts = torch.zeros(num_classes, dtype=torch.float64)\n",
    "    nb = 0\n",
    "    for X, y in loader:\n",
    "        nb += 1\n",
    "        y = y.view(-1)\n",
    "        y = y[y >= 0]\n",
    "        if y.numel() > 0:\n",
    "            binc = torch.bincount(y.cpu(), minlength=num_classes).to(torch.float64)\n",
    "            counts += binc\n",
    "        if nb >= max_batches:\n",
    "            break\n",
    "    counts = torch.clamp(counts, min=1.0)\n",
    "    inv = counts.sum() / counts\n",
    "    w = inv / inv.mean()\n",
    "    w = torch.clamp(w, max=MAX_WEIGHT)\n",
    "    return w.to(torch.float32)\n",
    "\n",
    "class_weights = compute_class_weights(train_loader, NUM_CLASSES).to(DEVICE)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# ---------------- MODEL ----------------\n",
    "class PointNetSmall(nn.Module):\n",
    "    def __init__(self, in_ch, num_classes):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 64, 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(64, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(256 + 256, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv1d(256, num_classes, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,N,F) -> (B,F,N)\n",
    "        x = x.transpose(1,2)\n",
    "        f = self.mlp1(x)\n",
    "        f = self.mlp2(f)  # (B,256,N)\n",
    "        g = torch.max(f, dim=2, keepdim=True)[0]  # (B,256,1)\n",
    "        g = g.repeat(1, 1, f.size(2))             # (B,256,N)\n",
    "        out = self.head(torch.cat([f, g], dim=1)) # (B,C,N)\n",
    "        return out.transpose(1,2)                 # (B,N,C)\n",
    "\n",
    "model = PointNetSmall(in_ch=FEATS, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "# ---------------- TRAIN / EVAL ----------------\n",
    "def accuracy(logits, y):\n",
    "    # logits: (B,N,C), y: (B,N)\n",
    "    pred = logits.argmax(dim=-1)\n",
    "    mask = (y >= 0)\n",
    "    if mask.sum() == 0:\n",
    "        return 0.0\n",
    "    return (pred[mask] == y[mask]).float().mean().item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    tot_loss, tot_acc, n = 0.0, 0.0, 0\n",
    "    for X, y in val_loader:\n",
    "        X = X.to(DEVICE, non_blocking=True)\n",
    "        y = y.to(DEVICE, non_blocking=True)\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits.reshape(-1, NUM_CLASSES), y.reshape(-1))\n",
    "        tot_loss += float(loss.item())\n",
    "        tot_acc  += accuracy(logits, y)\n",
    "        n += 1\n",
    "    return tot_loss / max(n,1), tot_acc / max(n,1)\n",
    "\n",
    "best_val = 1e9\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} [train]\", leave=True)\n",
    "    tot_loss, n = 0.0, 0\n",
    "\n",
    "    for step, (X, y) in enumerate(pbar):\n",
    "        X = X.to(DEVICE, non_blocking=True)\n",
    "        y = y.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=(DEVICE==\"cuda\")):\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits.reshape(-1, NUM_CLASSES), y.reshape(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        tot_loss += float(loss.item())\n",
    "        n += 1\n",
    "        pbar.set_postfix(loss=float(loss.item()))\n",
    "\n",
    "    tr_loss = tot_loss / max(n,1)\n",
    "    va_loss, va_acc = evaluate()\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | train_loss={tr_loss:.4f} | val_loss={va_loss:.4f} | val_acc={va_acc:.4f}\")\n",
    "\n",
    "    # save best\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        ckpt = {\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"classes\": classes,\n",
    "            \"class_to_idx\": class_to_idx,\n",
    "            \"Xmean\": Xmean,\n",
    "            \"Xstd\": Xstd,\n",
    "            \"feats\": FEATS,\n",
    "        }\n",
    "        torch.save(ckpt, MODEL_OUT)\n",
    "        print(\"✅ Saved best model:\", MODEL_OUT)\n",
    "\n",
    "print(\"DONE TRAINING.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d24cd91-1c73-483d-b625-251fa7cb5a19",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy._core.multiarray._reconstruct])` or the `torch.serialization.safe_globals([numpy._core.multiarray._reconstruct])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 126\u001b[0m\n\u001b[0;32m    123\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(torch\u001b[38;5;241m.\u001b[39mcat([f, g], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 126\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m classes \u001b[38;5;241m=\u001b[39m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    128\u001b[0m Xmean \u001b[38;5;241m=\u001b[39m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXmean\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\lidar\\lib\\site-packages\\torch\\serialization.py:1548\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1540\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1541\u001b[0m                     opened_zipfile,\n\u001b[0;32m   1542\u001b[0m                     map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1545\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1546\u001b[0m                 )\n\u001b[0;32m   1547\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1548\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1549\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1550\u001b[0m             opened_zipfile,\n\u001b[0;32m   1551\u001b[0m             map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1554\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1555\u001b[0m         )\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy._core.multiarray._reconstruct])` or the `torch.serialization.safe_globals([numpy._core.multiarray._reconstruct])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# 03_predict_las.py\n",
    "# - Loads LAS/LAZ\n",
    "# - Builds SAME 10 features\n",
    "# - Splits into blocks with idx mapping\n",
    "# - Runs model -> per-point prediction (stable)\n",
    "# - Writes output LAS/LAZ with predicted classification\n",
    "# ============================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import laspy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "ckpt = torch.load(MODEL_PT, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "\n",
    "IN_LAZ   = r\"D:/lidarrrrr/anbu/DX3035724 S.GIUSTO000001.laz\"\n",
    "MODEL_PT = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"\n",
    "OUT_LAS  = r\"D:/lidarrrrr/anbu/New folder/dl_predicted_clean.las\"\n",
    "OUT_LAZ  = r\"D:/lidarrrrr/anbu/New folder/dl_predicted_clean.laz\"\n",
    "\n",
    "NPTS = 4096\n",
    "CELL = 2.0\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_dim(las, name, fallback=0.0):\n",
    "    try:\n",
    "        return np.asarray(las[name])\n",
    "    except Exception:\n",
    "        try:\n",
    "            if name == \"scan_angle\":\n",
    "                return np.asarray(las[\"scan_angle_rank\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "    return np.full(len(las.x), fallback, dtype=np.float32)\n",
    "\n",
    "def grid_stats_zmin_zmax(x, y, z, cell):\n",
    "    minx, miny = float(x.min()), float(y.min())\n",
    "    gx = np.floor((x - minx) / cell).astype(np.int32)\n",
    "    gy = np.floor((y - miny) / cell).astype(np.int32)\n",
    "    key = gx.astype(np.int64) * 1_000_000 + gy.astype(np.int64)\n",
    "\n",
    "    order = np.argsort(key)\n",
    "    key_s = key[order]\n",
    "    z_s   = z[order]\n",
    "\n",
    "    uniq, start = np.unique(key_s, return_index=True)\n",
    "    zmin = np.full(len(uniq), np.inf, dtype=np.float32)\n",
    "    zmax = np.full(len(uniq), -np.inf, dtype=np.float32)\n",
    "\n",
    "    for i in range(len(uniq)):\n",
    "        a = start[i]\n",
    "        b = start[i+1] if i+1 < len(uniq) else len(key_s)\n",
    "        zs = z_s[a:b]\n",
    "        zmin[i] = zs.min()\n",
    "        zmax[i] = zs.max()\n",
    "\n",
    "    pos = np.searchsorted(uniq, key)\n",
    "    pos = np.clip(pos, 0, len(uniq)-1)\n",
    "    return zmin[pos], zmax[pos]\n",
    "\n",
    "def make_features(xyz, intensity, ret_num, n_returns, scan_angle, deviation, cell=CELL):\n",
    "    x = xyz[:,0].astype(np.float32)\n",
    "    y = xyz[:,1].astype(np.float32)\n",
    "    z = xyz[:,2].astype(np.float32)\n",
    "\n",
    "    zmin, zmax = grid_stats_zmin_zmax(x, y, z, cell)\n",
    "    hag = (z - zmin).astype(np.float32)\n",
    "    zrange = (zmax - zmin).astype(np.float32)\n",
    "\n",
    "    x0 = x - x.mean()\n",
    "    y0 = y - y.mean()\n",
    "    z0 = z - z.mean()\n",
    "\n",
    "    X = np.stack([\n",
    "        x0, y0, z0,\n",
    "        hag,\n",
    "        intensity.astype(np.float32),\n",
    "        ret_num.astype(np.float32),\n",
    "        n_returns.astype(np.float32),\n",
    "        scan_angle.astype(np.float32),\n",
    "        deviation.astype(np.float32),\n",
    "        zrange\n",
    "    ], axis=1).astype(np.float32)\n",
    "    return X\n",
    "\n",
    "# ---- model ----\n",
    "class PointNetSmall(nn.Module):\n",
    "    def __init__(self, in_ch, num_classes):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 64, 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(64, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(256 + 256, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv1d(256, num_classes, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1,2)\n",
    "        f = self.mlp1(x)\n",
    "        f = self.mlp2(f)\n",
    "        g = torch.max(f, dim=2, keepdim=True)[0]\n",
    "        g = g.repeat(1, 1, f.size(2))\n",
    "        out = self.head(torch.cat([f, g], dim=1))\n",
    "        return out.transpose(1,2)\n",
    "\n",
    "ckpt = torch.load(MODEL_PT, map_location=DEVICE)\n",
    "classes = ckpt[\"classes\"]\n",
    "Xmean = ckpt[\"Xmean\"]\n",
    "Xstd  = ckpt[\"Xstd\"]\n",
    "FEATS = ckpt[\"feats\"]\n",
    "NUM_CLASSES = len(classes)\n",
    "idx_to_class = {i:c for i,c in enumerate(classes)}\n",
    "\n",
    "model = PointNetSmall(FEATS, NUM_CLASSES).to(DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"Loaded model. Classes:\", classes)\n",
    "\n",
    "# ---- load LAS ----\n",
    "las = laspy.read(IN_LAZ)\n",
    "xyz = np.vstack([las.x, las.y, las.z]).T\n",
    "N = len(xyz)\n",
    "\n",
    "intensity = get_dim(las, \"intensity\", 0.0)\n",
    "ret_num   = get_dim(las, \"return_number\", 1.0)\n",
    "n_returns = get_dim(las, \"number_of_returns\", 1.0)\n",
    "scan_angle = get_dim(las, \"scan_angle\", 0.0)\n",
    "deviation  = get_dim(las, \"Deviation\", 0.0)\n",
    "\n",
    "X = make_features(xyz, intensity, ret_num, n_returns, scan_angle, deviation)\n",
    "print(\"Total points:\", N, \"| features:\", X.shape)\n",
    "\n",
    "# normalize using train stats\n",
    "X = (X - Xmean) / (Xstd + 1e-6)\n",
    "\n",
    "# ---- make blocks with idx mapping (stable tiling by XY grid) ----\n",
    "# group by grid cell to keep spatial coherence\n",
    "minx, miny = float(xyz[:,0].min()), float(xyz[:,1].min())\n",
    "gx = np.floor((xyz[:,0] - minx) / CELL).astype(np.int32)\n",
    "gy = np.floor((xyz[:,1] - miny) / CELL).astype(np.int32)\n",
    "key = gx.astype(np.int64) * 1_000_000 + gy.astype(np.int64)\n",
    "\n",
    "order = np.argsort(key)\n",
    "order = order.astype(np.int64)\n",
    "\n",
    "# create blocks sequentially from sorted order\n",
    "blocks = []\n",
    "for s in range(0, N, NPTS):\n",
    "    idx = order[s:s+NPTS]\n",
    "    if len(idx) < NPTS:\n",
    "        break\n",
    "    blocks.append(idx)\n",
    "\n",
    "print(\"Blocks for prediction:\", len(blocks))\n",
    "\n",
    "# per-point voting (reduces “salt & pepper”)\n",
    "votes = np.zeros((N, NUM_CLASSES), dtype=np.uint16)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in tqdm(blocks, desc=\"Predict blocks\"):\n",
    "        xb = torch.from_numpy(X[idx]).float().unsqueeze(0).to(DEVICE)  # (1,N,10)\n",
    "        logits = model(xb)  # (1,N,C)\n",
    "        pred = logits.argmax(dim=-1).squeeze(0).detach().cpu().numpy()  # (N,)\n",
    "        # vote\n",
    "        votes[idx, pred] += 1\n",
    "\n",
    "final_idx = votes.argmax(axis=1)          # 0..C-1\n",
    "final_cls = np.array([idx_to_class[i] for i in final_idx], dtype=np.uint8)\n",
    "\n",
    "las.classification = final_cls\n",
    "\n",
    "os.makedirs(os.path.dirname(OUT_LAS), exist_ok=True)\n",
    "las.write(OUT_LAS)\n",
    "print(\"Saved LAS:\", OUT_LAS)\n",
    "\n",
    "try:\n",
    "    las.write(OUT_LAZ)\n",
    "    print(\"Saved LAZ:\", OUT_LAZ)\n",
    "except Exception as e:\n",
    "    print(\"LAZ write failed (ok):\", e)\n",
    "\n",
    "u, c = np.unique(final_cls, return_counts=True)\n",
    "print(\"Pred class counts:\", dict(zip(u.tolist(), c.tolist())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67d7d2a3-1e15-4adc-8e1e-0ea10a75682f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n",
      "Loaded model. Classes: [1, 2, 3, 6, 7, 12, 13]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PointNetSmall:\n\tMissing key(s) in state_dict: \"mlp1.0.weight\", \"mlp1.0.bias\", \"mlp1.2.weight\", \"mlp1.2.bias\", \"mlp1.4.weight\", \"mlp1.4.bias\", \"mlp2.0.weight\", \"mlp2.0.bias\", \"mlp2.2.weight\", \"mlp2.2.bias\", \"head.weight\", \"head.bias\". \n\tUnexpected key(s) in state_dict: \"model_state\", \"classes\", \"class_to_idx\", \"Xmean\", \"Xstd\", \"feats\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 101\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Build + load model\u001b[39;00m\n\u001b[0;32m    100\u001b[0m model \u001b[38;5;241m=\u001b[39m PointNetSmall(in_ch\u001b[38;5;241m=\u001b[39mIN_CH, num_classes\u001b[38;5;241m=\u001b[39mNUM_CLASSES)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m--> 101\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# HELPERS: robust dimension getter\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\lidar\\lib\\site-packages\\torch\\nn\\modules\\module.py:2635\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2627\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2628\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2629\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2630\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2631\u001b[0m             ),\n\u001b[0;32m   2632\u001b[0m         )\n\u001b[0;32m   2634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2635\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2636\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2637\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2638\u001b[0m         )\n\u001b[0;32m   2639\u001b[0m     )\n\u001b[0;32m   2640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PointNetSmall:\n\tMissing key(s) in state_dict: \"mlp1.0.weight\", \"mlp1.0.bias\", \"mlp1.2.weight\", \"mlp1.2.bias\", \"mlp1.4.weight\", \"mlp1.4.bias\", \"mlp2.0.weight\", \"mlp2.0.bias\", \"mlp2.2.weight\", \"mlp2.2.bias\", \"head.weight\", \"head.bias\". \n\tUnexpected key(s) in state_dict: \"model_state\", \"classes\", \"class_to_idx\", \"Xmean\", \"Xstd\", \"feats\". "
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# FULL WORKING CODE: Load PointNet checkpoint safely (PyTorch 2.6+),\n",
    "# run prediction on a LAS/LAZ, and save output LAS/LAZ for CloudCompare.\n",
    "#\n",
    "# Fixes your error:\n",
    "# UnpicklingError / weights_only load failed\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import laspy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# PATHS (EDIT)\n",
    "# ----------------------------\n",
    "MODEL_PT = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"     # your checkpoint\n",
    "IN_LAZ   = r\"D:/lidarrrrr/anbu/DX3035724 S.GIUSTO000001.laz\"  # input\n",
    "OUT_LAS  = r\"D:/lidarrrrr/anbu/New folder/dl_predicted.las\"\n",
    "OUT_LAZ  = r\"D:/lidarrrrr/anbu/New folder/dl_predicted.laz\"\n",
    "\n",
    "# ----------------------------\n",
    "# DEVICE\n",
    "# ----------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# ----------------------------\n",
    "# MODEL (must match training architecture)\n",
    "# Input: (B,N,F)\n",
    "# Output: (B,C,N)\n",
    "# ----------------------------\n",
    "class PointNetSmall(nn.Module):\n",
    "    def __init__(self, in_ch, num_classes):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(in_ch, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(256 + 256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.head = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,N,F)\n",
    "        f = self.mlp1(x)              # (B,N,256)\n",
    "        g = torch.max(f, dim=1)[0]    # (B,256)\n",
    "        g = g.unsqueeze(1).repeat(1, f.shape[1], 1)  # (B,N,256)\n",
    "        h = self.mlp2(torch.cat([f, g], dim=-1))     # (B,N,128)\n",
    "        out = self.head(h)            # (B,N,C)\n",
    "        return out.permute(0, 2, 1)   # (B,C,N)\n",
    "\n",
    "# ----------------------------\n",
    "# SAFE LOAD CHECKPOINT (PyTorch 2.6+)\n",
    "# Use weights_only=False ONLY if you trust your own file.\n",
    "# ----------------------------\n",
    "ckpt = torch.load(MODEL_PT, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "# Expected keys:\n",
    "# ckpt[\"state_dict\"], ckpt[\"classes\"], ckpt[\"Xmean\"], ckpt[\"Xstd\"]\n",
    "if \"state_dict\" in ckpt:\n",
    "    state = ckpt[\"state_dict\"]\n",
    "else:\n",
    "    # if you saved model.state_dict() directly\n",
    "    state = ckpt\n",
    "\n",
    "classes = ckpt.get(\"classes\", None)     # list of original LAS class ids\n",
    "Xmean   = ckpt.get(\"Xmean\", None)       # (F,)\n",
    "Xstd    = ckpt.get(\"Xstd\", None)        # (F,)\n",
    "\n",
    "if classes is None:\n",
    "    raise RuntimeError(\"Checkpoint missing 'classes'. Re-save your checkpoint with classes list.\")\n",
    "print(\"Loaded model. Classes:\", classes)\n",
    "\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "# Infer feature count from checkpoint if possible\n",
    "# (we assume Xmean exists; if not, set manually)\n",
    "if Xmean is not None:\n",
    "    IN_CH = int(len(Xmean))\n",
    "else:\n",
    "    # set manually if needed\n",
    "    IN_CH = 10\n",
    "    Xmean = np.zeros(IN_CH, dtype=np.float32)\n",
    "    Xstd  = np.ones(IN_CH, dtype=np.float32)\n",
    "\n",
    "# Build + load model\n",
    "model = PointNetSmall(in_ch=IN_CH, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------\n",
    "# HELPERS: robust dimension getter\n",
    "# ----------------------------\n",
    "def get_dim(las, name, fallback=0.0):\n",
    "    try:\n",
    "        return np.asarray(las[name])\n",
    "    except Exception:\n",
    "        # scan_angle might be stored as scan_angle_rank in some files\n",
    "        if name == \"scan_angle\":\n",
    "            try:\n",
    "                return np.asarray(las[\"scan_angle_rank\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return np.full(len(las.x), fallback, dtype=np.float32)\n",
    "\n",
    "def grid_min_z(x, y, z, cell=2.0):\n",
    "    \"\"\"Returns per-point zmin of its XY grid cell (approx ground surface proxy).\"\"\"\n",
    "    minx, miny = float(x.min()), float(y.min())\n",
    "    gx = np.floor((x - minx) / cell).astype(np.int32)\n",
    "    gy = np.floor((y - miny) / cell).astype(np.int32)\n",
    "    key = gx.astype(np.int64) * 1_000_000 + gy.astype(np.int64)\n",
    "\n",
    "    order = np.argsort(key)\n",
    "    key_s = key[order]\n",
    "    z_s   = z[order]\n",
    "\n",
    "    uniq, start = np.unique(key_s, return_index=True)\n",
    "    zmin = np.full(len(uniq), np.inf, dtype=np.float32)\n",
    "\n",
    "    for i in range(len(uniq)):\n",
    "        a = start[i]\n",
    "        b = start[i+1] if i+1 < len(uniq) else len(z_s)\n",
    "        zmin[i] = float(z_s[a:b].min())\n",
    "\n",
    "    pos = np.searchsorted(uniq, key)\n",
    "    pos = np.clip(pos, 0, len(uniq)-1)\n",
    "    return zmin[pos]\n",
    "\n",
    "def make_features_from_las(las, cell=2.0):\n",
    "    \"\"\"\n",
    "    IMPORTANT: this must match your training block feature design.\n",
    "    Below is a COMMON 10-feature setup:\n",
    "      0 z\n",
    "      1 hag (z - grid_min_z)\n",
    "      2 dx (x - mean_x)  [or centered x]\n",
    "      3 dy (y - mean_y)\n",
    "      4 intensity\n",
    "      5 return_number\n",
    "      6 number_of_returns\n",
    "      7 scan_angle (or 0)\n",
    "      8 deviation (or 0)\n",
    "      9 overlap flag (or 0)\n",
    "    If your training used a different definition, tell me and I’ll align it.\n",
    "    \"\"\"\n",
    "    x = np.asarray(las.x, dtype=np.float32)\n",
    "    y = np.asarray(las.y, dtype=np.float32)\n",
    "    z = np.asarray(las.z, dtype=np.float32)\n",
    "\n",
    "    intensity = get_dim(las, \"intensity\", 0.0).astype(np.float32)\n",
    "    ret_num   = get_dim(las, \"return_number\", 1.0).astype(np.float32)\n",
    "    nret      = get_dim(las, \"number_of_returns\", 1.0).astype(np.float32)\n",
    "\n",
    "    scan_angle = get_dim(las, \"scan_angle\", 0.0).astype(np.float32)\n",
    "    deviation  = get_dim(las, \"Deviation\", 0.0).astype(np.float32)\n",
    "    overlap    = get_dim(las, \"overlap\", 0.0).astype(np.float32)\n",
    "\n",
    "    # simple approximate HAG using grid min z\n",
    "    zmin = grid_min_z(x, y, z, cell=cell)\n",
    "    hag = (z - zmin).astype(np.float32)\n",
    "\n",
    "    # center XY locally to reduce huge coordinate magnitudes\n",
    "    cx, cy = x.mean(), y.mean()\n",
    "    dx = (x - cx).astype(np.float32)\n",
    "    dy = (y - cy).astype(np.float32)\n",
    "\n",
    "    X = np.stack([z, hag, dx, dy, intensity, ret_num, nret, scan_angle, deviation, overlap], axis=1).astype(np.float32)\n",
    "    return X\n",
    "\n",
    "# ----------------------------\n",
    "# PREDICT IN BLOCKS (to avoid OOM)\n",
    "# ----------------------------\n",
    "BATCH_POINTS = 4096  # must match training block size\n",
    "STRIDE = 4096        # no overlap; you can set 2048 for overlap voting later\n",
    "\n",
    "las = laspy.read(IN_LAZ)\n",
    "X = make_features_from_las(las, cell=2.0)\n",
    "print(\"Total points:\", len(X), \"| features:\", X.shape)\n",
    "\n",
    "# Normalize using training stats\n",
    "Xn = (X - Xmean.astype(np.float32)) / (Xstd.astype(np.float32) + 1e-6)\n",
    "\n",
    "# output arrays\n",
    "pred_idx = np.zeros(len(Xn), dtype=np.int64)  # predicted class index in [0..C-1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for start in tqdm(range(0, len(Xn), STRIDE), desc=\"Predicting blocks\"):\n",
    "        end = min(start + BATCH_POINTS, len(Xn))\n",
    "        block = Xn[start:end]\n",
    "\n",
    "        # pad to 4096 if last block smaller\n",
    "        if len(block) < BATCH_POINTS:\n",
    "            pad_n = BATCH_POINTS - len(block)\n",
    "            pad = np.repeat(block[-1:], pad_n, axis=0)\n",
    "            block = np.vstack([block, pad])\n",
    "\n",
    "        inp = torch.from_numpy(block).unsqueeze(0).to(DEVICE)  # (1,4096,F)\n",
    "\n",
    "        logits = model(inp)          # (1,C,4096)\n",
    "        p = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()  # (4096,)\n",
    "\n",
    "        p = p[: (end - start)]       # unpad\n",
    "        pred_idx[start:end] = p\n",
    "\n",
    "# map predicted indices back to LAS class codes\n",
    "pred_cls = np.array([classes[i] for i in pred_idx], dtype=np.uint8)\n",
    "\n",
    "# write output\n",
    "las.classification = pred_cls\n",
    "os.makedirs(os.path.dirname(OUT_LAS), exist_ok=True)\n",
    "\n",
    "las.write(OUT_LAS)\n",
    "print(\"Saved LAS:\", OUT_LAS)\n",
    "\n",
    "try:\n",
    "    las.write(OUT_LAZ)\n",
    "    print(\"Saved LAZ:\", OUT_LAZ)\n",
    "except Exception as e:\n",
    "    print(\"LAZ write failed (ok):\", e)\n",
    "\n",
    "u, c = np.unique(pred_cls, return_counts=True)\n",
    "print(\"Pred class counts:\", dict(zip(u.tolist(), c.tolist())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91bc9caf-4c96-485c-bffe-70d6f437374f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n",
      "Loaded model. Classes: [1, 2, 3, 6, 7, 12, 13]\n",
      "Input features: 10 | Num classes: 7\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PointNetSmall:\n\tMissing key(s) in state_dict: \"mlp1.2.weight\", \"mlp1.2.bias\", \"mlp2.2.weight\", \"mlp2.2.bias\", \"head.weight\", \"head.bias\". \n\tUnexpected key(s) in state_dict: \"mlp1.1.weight\", \"mlp1.1.bias\", \"mlp1.1.running_mean\", \"mlp1.1.running_var\", \"mlp1.1.num_batches_tracked\", \"mlp1.3.weight\", \"mlp1.3.bias\", \"mlp1.4.running_mean\", \"mlp1.4.running_var\", \"mlp1.4.num_batches_tracked\", \"mlp2.1.weight\", \"mlp2.1.bias\", \"mlp2.1.running_mean\", \"mlp2.1.running_var\", \"mlp2.1.num_batches_tracked\", \"head.0.weight\", \"head.0.bias\", \"head.1.weight\", \"head.1.bias\", \"head.1.running_mean\", \"head.1.running_var\", \"head.1.num_batches_tracked\", \"head.4.weight\", \"head.4.bias\". \n\tsize mismatch for mlp1.0.weight: copying a param with shape torch.Size([64, 10, 1]) from checkpoint, the shape in current model is torch.Size([64, 10]).\n\tsize mismatch for mlp1.4.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for mlp1.4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mlp2.0.weight: copying a param with shape torch.Size([256, 128, 1]) from checkpoint, the shape in current model is torch.Size([256, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# build model + load weights\u001b[39;00m\n\u001b[0;32m     92\u001b[0m model \u001b[38;5;241m=\u001b[39m PointNetSmall(in_ch\u001b[38;5;241m=\u001b[39mIN_CH, num_classes\u001b[38;5;241m=\u001b[39mNUM_CLASSES)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 93\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfixed_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Helpers\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\lidar\\lib\\site-packages\\torch\\nn\\modules\\module.py:2635\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2627\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2628\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2629\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2630\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2631\u001b[0m             ),\n\u001b[0;32m   2632\u001b[0m         )\n\u001b[0;32m   2634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2635\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2636\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2637\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2638\u001b[0m         )\n\u001b[0;32m   2639\u001b[0m     )\n\u001b[0;32m   2640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PointNetSmall:\n\tMissing key(s) in state_dict: \"mlp1.2.weight\", \"mlp1.2.bias\", \"mlp2.2.weight\", \"mlp2.2.bias\", \"head.weight\", \"head.bias\". \n\tUnexpected key(s) in state_dict: \"mlp1.1.weight\", \"mlp1.1.bias\", \"mlp1.1.running_mean\", \"mlp1.1.running_var\", \"mlp1.1.num_batches_tracked\", \"mlp1.3.weight\", \"mlp1.3.bias\", \"mlp1.4.running_mean\", \"mlp1.4.running_var\", \"mlp1.4.num_batches_tracked\", \"mlp2.1.weight\", \"mlp2.1.bias\", \"mlp2.1.running_mean\", \"mlp2.1.running_var\", \"mlp2.1.num_batches_tracked\", \"head.0.weight\", \"head.0.bias\", \"head.1.weight\", \"head.1.bias\", \"head.1.running_mean\", \"head.1.running_var\", \"head.1.num_batches_tracked\", \"head.4.weight\", \"head.4.bias\". \n\tsize mismatch for mlp1.0.weight: copying a param with shape torch.Size([64, 10, 1]) from checkpoint, the shape in current model is torch.Size([64, 10]).\n\tsize mismatch for mlp1.4.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for mlp1.4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for mlp2.0.weight: copying a param with shape torch.Size([256, 128, 1]) from checkpoint, the shape in current model is torch.Size([256, 512])."
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# FULL FIXED CODE: Load your checkpoint correctly\n",
    "# (because your .pt stores model_state + metadata)\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import laspy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# PATHS (EDIT)\n",
    "# ----------------------------\n",
    "MODEL_PT = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"\n",
    "IN_LAZ   = r\"D:/lidarrrrr/anbu/DX3035724 S.GIUSTO000001.laz\"\n",
    "OUT_LAS  = r\"D:/lidarrrrr/anbu/New folder/dl_predicted.las\"\n",
    "OUT_LAZ  = r\"D:/lidarrrrr/anbu/New folder/dl_predicted.laz\"\n",
    "\n",
    "# ----------------------------\n",
    "# DEVICE\n",
    "# ----------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# ----------------------------\n",
    "# MODEL (MUST MATCH TRAINING ARCH)\n",
    "# If your training code used a different PointNet class,\n",
    "# you must paste that model definition here.\n",
    "# ----------------------------\n",
    "class PointNetSmall(nn.Module):\n",
    "    def __init__(self, in_ch, num_classes):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(in_ch, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(256 + 256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.head = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,N,F)\n",
    "        f = self.mlp1(x)              # (B,N,256)\n",
    "        g = torch.max(f, dim=1)[0]    # (B,256)\n",
    "        g = g.unsqueeze(1).repeat(1, f.shape[1], 1)  # (B,N,256)\n",
    "        h = self.mlp2(torch.cat([f, g], dim=-1))     # (B,N,128)\n",
    "        out = self.head(h)            # (B,N,C)\n",
    "        return out.permute(0, 2, 1)   # (B,C,N)\n",
    "\n",
    "# ----------------------------\n",
    "# LOAD CHECKPOINT (PyTorch 2.6+ safe)\n",
    "# ----------------------------\n",
    "ckpt = torch.load(MODEL_PT, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "# IMPORTANT: your checkpoint stores weights under \"model_state\"\n",
    "if \"model_state\" not in ckpt:\n",
    "    raise RuntimeError(f\"Checkpoint keys are {list(ckpt.keys())}, but 'model_state' not found.\")\n",
    "\n",
    "state_dict = ckpt[\"model_state\"]\n",
    "\n",
    "# remove \"module.\" prefix if it exists (from DataParallel)\n",
    "fixed_state = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith(\"module.\"):\n",
    "        fixed_state[k[len(\"module.\"):]] = v\n",
    "    else:\n",
    "        fixed_state[k] = v\n",
    "\n",
    "classes = ckpt[\"classes\"]          # list of LAS class codes\n",
    "Xmean   = np.array(ckpt[\"Xmean\"], dtype=np.float32)\n",
    "Xstd    = np.array(ckpt[\"Xstd\"], dtype=np.float32)\n",
    "\n",
    "NUM_CLASSES = len(classes)\n",
    "IN_CH = int(len(Xmean))\n",
    "\n",
    "print(\"Loaded model. Classes:\", classes)\n",
    "print(\"Input features:\", IN_CH, \"| Num classes:\", NUM_CLASSES)\n",
    "\n",
    "# build model + load weights\n",
    "model = PointNetSmall(in_ch=IN_CH, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "model.load_state_dict(fixed_state, strict=True)\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def get_dim(las, name, fallback=0.0):\n",
    "    try:\n",
    "        return np.asarray(las[name])\n",
    "    except Exception:\n",
    "        if name == \"scan_angle\":\n",
    "            try:\n",
    "                return np.asarray(las[\"scan_angle_rank\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return np.full(len(las.x), fallback, dtype=np.float32)\n",
    "\n",
    "def grid_min_z(x, y, z, cell=2.0):\n",
    "    minx, miny = float(x.min()), float(y.min())\n",
    "    gx = np.floor((x - minx) / cell).astype(np.int32)\n",
    "    gy = np.floor((y - miny) / cell).astype(np.int32)\n",
    "    key = gx.astype(np.int64) * 1_000_000 + gy.astype(np.int64)\n",
    "\n",
    "    order = np.argsort(key)\n",
    "    key_s = key[order]\n",
    "    z_s   = z[order]\n",
    "\n",
    "    uniq, start = np.unique(key_s, return_index=True)\n",
    "    zmin = np.full(len(uniq), np.inf, dtype=np.float32)\n",
    "\n",
    "    for i in range(len(uniq)):\n",
    "        a = start[i]\n",
    "        b = start[i+1] if i+1 < len(uniq) else len(z_s)\n",
    "        zmin[i] = float(z_s[a:b].min())\n",
    "\n",
    "    pos = np.searchsorted(uniq, key)\n",
    "    pos = np.clip(pos, 0, len(uniq)-1)\n",
    "    return zmin[pos]\n",
    "\n",
    "def make_features_from_las(las, cell=2.0):\n",
    "    \"\"\"\n",
    "    WARNING: this MUST match your dataset block feature creation.\n",
    "    If you created blocks with keys ['X','y'] and X has 10 columns,\n",
    "    then this must generate the SAME 10 columns in SAME order.\n",
    "    \"\"\"\n",
    "    x = np.asarray(las.x, dtype=np.float32)\n",
    "    y = np.asarray(las.y, dtype=np.float32)\n",
    "    z = np.asarray(las.z, dtype=np.float32)\n",
    "\n",
    "    intensity = get_dim(las, \"intensity\", 0.0).astype(np.float32)\n",
    "    ret_num   = get_dim(las, \"return_number\", 1.0).astype(np.float32)\n",
    "    nret      = get_dim(las, \"number_of_returns\", 1.0).astype(np.float32)\n",
    "\n",
    "    scan_angle = get_dim(las, \"scan_angle\", 0.0).astype(np.float32)\n",
    "    deviation  = get_dim(las, \"Deviation\", 0.0).astype(np.float32)\n",
    "    overlap    = get_dim(las, \"overlap\", 0.0).astype(np.float32)\n",
    "\n",
    "    zmin = grid_min_z(x, y, z, cell=cell)\n",
    "    hag  = (z - zmin).astype(np.float32)\n",
    "\n",
    "    cx, cy = x.mean(), y.mean()\n",
    "    dx = (x - cx).astype(np.float32)\n",
    "    dy = (y - cy).astype(np.float32)\n",
    "\n",
    "    X = np.stack([z, hag, dx, dy, intensity, ret_num, nret, scan_angle, deviation, overlap], axis=1).astype(np.float32)\n",
    "    return X\n",
    "\n",
    "# ----------------------------\n",
    "# Predict (block-wise)\n",
    "# ----------------------------\n",
    "BATCH_POINTS = 4096\n",
    "STRIDE = 4096\n",
    "\n",
    "las = laspy.read(IN_LAZ)\n",
    "X = make_features_from_las(las, cell=2.0)\n",
    "\n",
    "print(\"Total points:\", len(X), \"| X shape:\", X.shape)\n",
    "\n",
    "# normalize using training stats\n",
    "Xn = (X - Xmean) / (Xstd + 1e-6)\n",
    "\n",
    "pred_idx = np.zeros(len(Xn), dtype=np.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for start in tqdm(range(0, len(Xn), STRIDE), desc=\"Predicting\"):\n",
    "        end = min(start + BATCH_POINTS, len(Xn))\n",
    "        block = Xn[start:end]\n",
    "\n",
    "        if len(block) < BATCH_POINTS:\n",
    "            pad_n = BATCH_POINTS - len(block)\n",
    "            pad = np.repeat(block[-1:], pad_n, axis=0)\n",
    "            block = np.vstack([block, pad])\n",
    "\n",
    "        inp = torch.from_numpy(block).unsqueeze(0).to(DEVICE)  # (1,4096,F)\n",
    "        logits = model(inp)                                    # (1,C,4096)\n",
    "        p = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()  # (4096,)\n",
    "\n",
    "        pred_idx[start:end] = p[: end-start]\n",
    "\n",
    "# map to LAS classes\n",
    "pred_cls = np.array([classes[i] for i in pred_idx], dtype=np.uint8)\n",
    "\n",
    "las.classification = pred_cls\n",
    "os.makedirs(os.path.dirname(OUT_LAS), exist_ok=True)\n",
    "\n",
    "las.write(OUT_LAS)\n",
    "print(\"Saved LAS:\", OUT_LAS)\n",
    "\n",
    "try:\n",
    "    las.write(OUT_LAZ)\n",
    "    print(\"Saved LAZ:\", OUT_LAZ)\n",
    "except Exception as e:\n",
    "    print(\"LAZ write failed (ok):\", e)\n",
    "\n",
    "u, c = np.unique(pred_cls, return_counts=True)\n",
    "print(\"Pred class counts:\", dict(zip(u.tolist(), c.tolist())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a21ddb9c-1529-4eca-bf2a-503aea919f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n",
      "Loaded model. Classes: [1, 2, 3, 6, 7, 12, 13]\n",
      "Input features: 10 | Num classes: 7\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PointNetConv:\n\tMissing key(s) in state_dict: \"mlp1.6.weight\", \"mlp1.6.bias\", \"mlp1.7.weight\", \"mlp1.7.bias\", \"mlp1.7.running_mean\", \"mlp1.7.running_var\". \n\tsize mismatch for mlp2.0.weight: copying a param with shape torch.Size([256, 128, 1]) from checkpoint, the shape in current model is torch.Size([256, 512, 1]).\n\tsize mismatch for head.0.weight: copying a param with shape torch.Size([256, 512, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1]).\n\tsize mismatch for head.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for head.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for head.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for head.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for head.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for head.4.weight: copying a param with shape torch.Size([7, 256, 1]) from checkpoint, the shape in current model is torch.Size([7, 128, 1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 106\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput features:\u001b[39m\u001b[38;5;124m\"\u001b[39m, IN_CH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| Num classes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, NUM_CLASSES)\n\u001b[0;32m    105\u001b[0m model \u001b[38;5;241m=\u001b[39m PointNetConv(in_ch\u001b[38;5;241m=\u001b[39mIN_CH, num_classes\u001b[38;5;241m=\u001b[39mNUM_CLASSES)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m--> 106\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfixed_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Helpers (MUST match training feature order)\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\lidar\\lib\\site-packages\\torch\\nn\\modules\\module.py:2635\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2627\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2628\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2629\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2630\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2631\u001b[0m             ),\n\u001b[0;32m   2632\u001b[0m         )\n\u001b[0;32m   2634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2635\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2636\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2637\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2638\u001b[0m         )\n\u001b[0;32m   2639\u001b[0m     )\n\u001b[0;32m   2640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PointNetConv:\n\tMissing key(s) in state_dict: \"mlp1.6.weight\", \"mlp1.6.bias\", \"mlp1.7.weight\", \"mlp1.7.bias\", \"mlp1.7.running_mean\", \"mlp1.7.running_var\". \n\tsize mismatch for mlp2.0.weight: copying a param with shape torch.Size([256, 128, 1]) from checkpoint, the shape in current model is torch.Size([256, 512, 1]).\n\tsize mismatch for head.0.weight: copying a param with shape torch.Size([256, 512, 1]) from checkpoint, the shape in current model is torch.Size([128, 256, 1]).\n\tsize mismatch for head.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for head.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for head.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for head.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for head.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for head.4.weight: copying a param with shape torch.Size([7, 256, 1]) from checkpoint, the shape in current model is torch.Size([7, 128, 1])."
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# FULL WORKING CODE (matches your checkpoint):\n",
    "# - Conv1d PointNet + BatchNorm\n",
    "# - Loads ckpt[\"model_state\"]\n",
    "# - Predicts classes for full LAS/LAZ\n",
    "# - Saves output LAS/LAZ for CloudCompare\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import laspy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# PATHS (EDIT)\n",
    "# ----------------------------\n",
    "MODEL_PT = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"\n",
    "IN_LAZ   = r\"D:/lidarrrrr/anbu/DX3035724 S.GIUSTO000001.laz\"\n",
    "OUT_LAS  = r\"D:/lidarrrrr/anbu/New folder/dl_predicted.las\"\n",
    "OUT_LAZ  = r\"D:/lidarrrrr/anbu/New folder/dl_predicted.laz\"\n",
    "\n",
    "# ----------------------------\n",
    "# DEVICE\n",
    "# ----------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# ----------------------------\n",
    "# Conv1d PointNet model (matches your checkpoint)\n",
    "# Input: (B, F, N)\n",
    "# Output: (B, C, N)\n",
    "# ----------------------------\n",
    "class PointNetConv(nn.Module):\n",
    "    def __init__(self, in_ch, num_classes):\n",
    "        super().__init__()\n",
    "        # per-point feature extractor\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 64, 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv1d(64, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv1d(128, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # fuse local+global\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Conv1d(256 + 256, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # head (Sequential in checkpoint)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(256, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Conv1d(128, num_classes, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, F, N)\n",
    "        f = self.mlp1(x)                 # (B,256,N)\n",
    "        g = torch.max(f, dim=2)[0]       # (B,256)\n",
    "        g = g.unsqueeze(2).repeat(1, 1, f.shape[2])  # (B,256,N)\n",
    "        h = self.mlp2(torch.cat([f, g], dim=1))      # (B,256,N)\n",
    "        out = self.head(h)               # (B,C,N)\n",
    "        return out\n",
    "\n",
    "# ----------------------------\n",
    "# LOAD CHECKPOINT properly\n",
    "# ----------------------------\n",
    "ckpt = torch.load(MODEL_PT, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "if \"model_state\" not in ckpt:\n",
    "    raise RuntimeError(f\"Checkpoint keys: {list(ckpt.keys())}. Expected 'model_state' key.\")\n",
    "\n",
    "state_dict = ckpt[\"model_state\"]\n",
    "\n",
    "# remove DataParallel \"module.\" prefix if exists\n",
    "fixed_state = {}\n",
    "for k, v in state_dict.items():\n",
    "    fixed_state[k.replace(\"module.\", \"\")] = v\n",
    "\n",
    "classes = ckpt[\"classes\"]\n",
    "Xmean   = np.array(ckpt[\"Xmean\"], dtype=np.float32)\n",
    "Xstd    = np.array(ckpt[\"Xstd\"], dtype=np.float32)\n",
    "\n",
    "NUM_CLASSES = len(classes)\n",
    "IN_CH = int(len(Xmean))\n",
    "\n",
    "print(\"Loaded model. Classes:\", classes)\n",
    "print(\"Input features:\", IN_CH, \"| Num classes:\", NUM_CLASSES)\n",
    "\n",
    "model = PointNetConv(in_ch=IN_CH, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "model.load_state_dict(fixed_state, strict=True)\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers (MUST match training feature order)\n",
    "# ----------------------------\n",
    "def get_dim(las, name, fallback=0.0):\n",
    "    try:\n",
    "        return np.asarray(las[name])\n",
    "    except Exception:\n",
    "        if name == \"scan_angle\":\n",
    "            try:\n",
    "                return np.asarray(las[\"scan_angle_rank\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return np.full(len(las.x), fallback, dtype=np.float32)\n",
    "\n",
    "def grid_min_z(x, y, z, cell=2.0):\n",
    "    minx, miny = float(x.min()), float(y.min())\n",
    "    gx = np.floor((x - minx) / cell).astype(np.int32)\n",
    "    gy = np.floor((y - miny) / cell).astype(np.int32)\n",
    "    key = gx.astype(np.int64) * 1_000_000 + gy.astype(np.int64)\n",
    "\n",
    "    order = np.argsort(key)\n",
    "    key_s = key[order]\n",
    "    z_s   = z[order]\n",
    "\n",
    "    uniq, start = np.unique(key_s, return_index=True)\n",
    "    zmin = np.full(len(uniq), np.inf, dtype=np.float32)\n",
    "\n",
    "    for i in range(len(uniq)):\n",
    "        a = start[i]\n",
    "        b = start[i+1] if i+1 < len(uniq) else len(z_s)\n",
    "        zmin[i] = float(z_s[a:b].min())\n",
    "\n",
    "    pos = np.searchsorted(uniq, key)\n",
    "    pos = np.clip(pos, 0, len(uniq)-1)\n",
    "    return zmin[pos]\n",
    "\n",
    "def make_features_from_las(las, cell=2.0):\n",
    "    \"\"\"\n",
    "    Generate SAME 10 features you trained on.\n",
    "    If your dataset builder used a different feature order, prediction will be bad.\n",
    "    \"\"\"\n",
    "    x = np.asarray(las.x, dtype=np.float32)\n",
    "    y = np.asarray(las.y, dtype=np.float32)\n",
    "    z = np.asarray(las.z, dtype=np.float32)\n",
    "\n",
    "    intensity = get_dim(las, \"intensity\", 0.0).astype(np.float32)\n",
    "    ret_num   = get_dim(las, \"return_number\", 1.0).astype(np.float32)\n",
    "    nret      = get_dim(las, \"number_of_returns\", 1.0).astype(np.float32)\n",
    "\n",
    "    scan_angle = get_dim(las, \"scan_angle\", 0.0).astype(np.float32)\n",
    "    deviation  = get_dim(las, \"Deviation\", 0.0).astype(np.float32)\n",
    "    overlap    = get_dim(las, \"overlap\", 0.0).astype(np.float32)\n",
    "\n",
    "    zmin = grid_min_z(x, y, z, cell=cell)\n",
    "    hag  = (z - zmin).astype(np.float32)\n",
    "\n",
    "    cx, cy = x.mean(), y.mean()\n",
    "    dx = (x - cx).astype(np.float32)\n",
    "    dy = (y - cy).astype(np.float32)\n",
    "\n",
    "    # SAME as earlier assumption: [z,hag,dx,dy,intensity,ret_num,nret,scan_angle,deviation,overlap]\n",
    "    X = np.stack([z, hag, dx, dy, intensity, ret_num, nret, scan_angle, deviation, overlap], axis=1).astype(np.float32)\n",
    "    return X\n",
    "\n",
    "# ----------------------------\n",
    "# Predict in blocks\n",
    "# ----------------------------\n",
    "BATCH_POINTS = 4096\n",
    "STRIDE = 4096\n",
    "\n",
    "las = laspy.read(IN_LAZ)\n",
    "X = make_features_from_las(las, cell=2.0)\n",
    "\n",
    "print(\"Total points:\", len(X), \"| X shape:\", X.shape)\n",
    "\n",
    "# normalize\n",
    "Xn = (X - Xmean) / (Xstd + 1e-6)\n",
    "\n",
    "pred_idx = np.zeros(len(Xn), dtype=np.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for start in tqdm(range(0, len(Xn), STRIDE), desc=\"Predicting\"):\n",
    "        end = min(start + BATCH_POINTS, len(Xn))\n",
    "        block = Xn[start:end]\n",
    "\n",
    "        if len(block) < BATCH_POINTS:\n",
    "            pad_n = BATCH_POINTS - len(block)\n",
    "            pad = np.repeat(block[-1:], pad_n, axis=0)\n",
    "            block = np.vstack([block, pad])\n",
    "\n",
    "        # Conv1d expects (B,F,N)\n",
    "        inp = torch.from_numpy(block).to(DEVICE)          # (N,F)\n",
    "        inp = inp.unsqueeze(0).permute(0, 2, 1)           # (1,F,N)\n",
    "\n",
    "        logits = model(inp)                                # (1,C,N)\n",
    "        p = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()  # (N,)\n",
    "\n",
    "        pred_idx[start:end] = p[: end-start]\n",
    "\n",
    "pred_cls = np.array([classes[i] for i in pred_idx], dtype=np.uint8)\n",
    "\n",
    "las.classification = pred_cls\n",
    "os.makedirs(os.path.dirname(OUT_LAS), exist_ok=True)\n",
    "\n",
    "las.write(OUT_LAS)\n",
    "print(\"Saved LAS:\", OUT_LAS)\n",
    "\n",
    "try:\n",
    "    las.write(OUT_LAZ)\n",
    "    print(\"Saved LAZ:\", OUT_LAZ)\n",
    "except Exception as e:\n",
    "    print(\"LAZ write failed (ok):\", e)\n",
    "\n",
    "u, c = np.unique(pred_cls, return_counts=True)\n",
    "print(\"Pred class counts:\", dict(zip(u.tolist(), c.tolist())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d72d8b5-6bff-4c0a-a673-4e04fee7dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n",
      "Loaded model. Classes: [1, 2, 3, 6, 7, 12, 13]\n",
      "Input features: 10 | Num classes: 7\n",
      "Checkpoint shapes summary:\n",
      " mlp1 last out: 128\n",
      " mlp2 in/out : 128 → 256\n",
      " head in/mid/out: 512 → 256 → 7\n",
      "✅ Model loaded successfully.\n",
      "Total points: 12374846 | X shape: (12374846, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 3022/3022 [00:05<00:00, 525.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LAS: D:/lidarrrrr/anbu/New folder/dl_predicted.las\n",
      "Saved LAZ: D:/lidarrrrr/anbu/New folder/dl_predicted.laz\n",
      "Pred class counts: {1: 5909693, 2: 6425564, 6: 36652, 12: 2937}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import laspy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_PT = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"\n",
    "IN_LAZ   = r\"D:/lidarrrrr/anbu/DX3035724 S.GIUSTO000001.laz\"\n",
    "OUT_LAS  = r\"D:/lidarrrrr/anbu/New folder/dl_predicted.las\"\n",
    "OUT_LAZ  = r\"D:/lidarrrrr/anbu/New folder/dl_predicted.laz\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# ----------------------------\n",
    "# Load checkpoint\n",
    "# ----------------------------\n",
    "ckpt = torch.load(MODEL_PT, map_location=DEVICE, weights_only=False)\n",
    "state = ckpt[\"model_state\"]\n",
    "state = {k.replace(\"module.\", \"\"): v for k, v in state.items()}\n",
    "\n",
    "classes = ckpt[\"classes\"]\n",
    "Xmean   = np.array(ckpt[\"Xmean\"], dtype=np.float32)\n",
    "Xstd    = np.array(ckpt[\"Xstd\"], dtype=np.float32)\n",
    "\n",
    "NUM_CLASSES = len(classes)\n",
    "IN_CH = int(len(Xmean))\n",
    "\n",
    "print(\"Loaded model. Classes:\", classes)\n",
    "print(\"Input features:\", IN_CH, \"| Num classes:\", NUM_CLASSES)\n",
    "\n",
    "# ----------------------------\n",
    "# Build model EXACTLY from checkpoint shapes\n",
    "# ----------------------------\n",
    "def conv1_out_ch(key):\n",
    "    # weight shape: (out_ch, in_ch, 1)\n",
    "    return int(state[key].shape[0])\n",
    "\n",
    "def conv1_in_ch(key):\n",
    "    return int(state[key].shape[1])\n",
    "\n",
    "# detect how many conv blocks in mlp1 by scanning keys\n",
    "mlp1_conv_keys = sorted([k for k in state.keys() if k.startswith(\"mlp1.\") and k.endswith(\".weight\") and state[k].ndim == 3])\n",
    "# keep only conv weights (BN weights are 1D, conv are 3D)\n",
    "mlp1_conv_keys = [k for k in mlp1_conv_keys if state[k].shape[-1] == 1]\n",
    "# Example conv keys look like: mlp1.0.weight, mlp1.3.weight, mlp1.6.weight, ...\n",
    "# We'll build them in order based on module index number\n",
    "def module_index(k): return int(k.split(\".\")[1])\n",
    "mlp1_conv_keys.sort(key=module_index)\n",
    "\n",
    "mlp2_conv_key = [k for k in state.keys() if k.startswith(\"mlp2.\") and k.endswith(\".weight\") and state[k].ndim == 3][0]\n",
    "head0_key     = [k for k in state.keys() if k.startswith(\"head.0\") and k.endswith(\".weight\")][0]\n",
    "head4_key     = [k for k in state.keys() if k.startswith(\"head.4\") and k.endswith(\".weight\")][0]\n",
    "\n",
    "mlp1_out = conv1_out_ch(mlp1_conv_keys[-1])\n",
    "mlp2_in  = conv1_in_ch(mlp2_conv_key)\n",
    "mlp2_out = conv1_out_ch(mlp2_conv_key)\n",
    "\n",
    "head_in  = conv1_in_ch(head0_key)\n",
    "head_mid = conv1_out_ch(head0_key)\n",
    "head_out = conv1_out_ch(head4_key)\n",
    "\n",
    "print(\"Checkpoint shapes summary:\")\n",
    "print(\" mlp1 last out:\", mlp1_out)\n",
    "print(\" mlp2 in/out :\", mlp2_in, \"→\", mlp2_out)\n",
    "print(\" head in/mid/out:\", head_in, \"→\", head_mid, \"→\", head_out)\n",
    "\n",
    "class PointNetFromCkpt(nn.Module):\n",
    "    def __init__(self, in_ch, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Build mlp1 blocks from conv keys list\n",
    "        layers = []\n",
    "        prev_in = in_ch\n",
    "\n",
    "        for conv_k in mlp1_conv_keys:\n",
    "            out_ch = conv1_out_ch(conv_k)\n",
    "            # Conv\n",
    "            layers.append(nn.Conv1d(prev_in, out_ch, 1))\n",
    "            # BN\n",
    "            layers.append(nn.BatchNorm1d(out_ch))\n",
    "            # ReLU\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            prev_in = out_ch\n",
    "\n",
    "        self.mlp1 = nn.Sequential(*layers)\n",
    "\n",
    "        # mlp2 is a single conv+bn+relu in your checkpoint\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Conv1d(mlp2_in, mlp2_out, 1),\n",
    "            nn.BatchNorm1d(mlp2_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # head is Conv(head_in→head_mid), BN, ReLU, Dropout, Conv(head_mid→num_classes)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(head_in, head_mid, 1),\n",
    "            nn.BatchNorm1d(head_mid),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Conv1d(head_mid, num_classes, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.mlp1(x)\n",
    "        g = torch.max(f, dim=2)[0]       \n",
    "        g = g.unsqueeze(2).repeat(1, 1, f.shape[2]) \n",
    "        local_for_mlp2 = f[:, :mlp2_in, :]           \n",
    "        h_local = self.mlp2(local_for_mlp2)          \n",
    "        feat = torch.cat([h_local, f, g], dim=1)    \n",
    "        out = self.head(feat)                        \n",
    "        return out\n",
    "\n",
    "\n",
    "model = PointNetFromCkpt(IN_CH, NUM_CLASSES).to(DEVICE)\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()\n",
    "print(\"✅ Model loaded successfully.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Feature builder (must match training order)\n",
    "# ----------------------------\n",
    "def get_dim(las, name, fallback=0.0):\n",
    "    try:\n",
    "        return np.asarray(las[name])\n",
    "    except Exception:\n",
    "        if name == \"scan_angle\":\n",
    "            try:\n",
    "                return np.asarray(las[\"scan_angle_rank\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "    return np.full(len(las.x), fallback, dtype=np.float32)\n",
    "\n",
    "def grid_min_z(x, y, z, cell=2.0):\n",
    "    minx, miny = float(x.min()), float(y.min())\n",
    "    gx = np.floor((x - minx) / cell).astype(np.int32)\n",
    "    gy = np.floor((y - miny) / cell).astype(np.int32)\n",
    "    key = gx.astype(np.int64) * 1_000_000 + gy.astype(np.int64)\n",
    "\n",
    "    order = np.argsort(key)\n",
    "    key_s = key[order]\n",
    "    z_s   = z[order]\n",
    "\n",
    "    uniq, start = np.unique(key_s, return_index=True)\n",
    "    zmin = np.full(len(uniq), np.inf, dtype=np.float32)\n",
    "\n",
    "    for i in range(len(uniq)):\n",
    "        a = start[i]\n",
    "        b = start[i+1] if i+1 < len(uniq) else len(z_s)\n",
    "        zmin[i] = float(z_s[a:b].min())\n",
    "\n",
    "    pos = np.searchsorted(uniq, key)\n",
    "    pos = np.clip(pos, 0, len(uniq)-1)\n",
    "    return zmin[pos]\n",
    "\n",
    "def make_features_from_las(las, cell=2.0):\n",
    "    x = np.asarray(las.x, dtype=np.float32)\n",
    "    y = np.asarray(las.y, dtype=np.float32)\n",
    "    z = np.asarray(las.z, dtype=np.float32)\n",
    "\n",
    "    intensity = get_dim(las, \"intensity\", 0.0).astype(np.float32)\n",
    "    ret_num   = get_dim(las, \"return_number\", 1.0).astype(np.float32)\n",
    "    nret      = get_dim(las, \"number_of_returns\", 1.0).astype(np.float32)\n",
    "    scan_angle = get_dim(las, \"scan_angle\", 0.0).astype(np.float32)\n",
    "    deviation  = get_dim(las, \"Deviation\", 0.0).astype(np.float32)\n",
    "    overlap    = get_dim(las, \"overlap\", 0.0).astype(np.float32)\n",
    "\n",
    "    zmin = grid_min_z(x, y, z, cell=cell)\n",
    "    hag  = (z - zmin).astype(np.float32)\n",
    "\n",
    "    cx, cy = x.mean(), y.mean()\n",
    "    dx = (x - cx).astype(np.float32)\n",
    "    dy = (y - cy).astype(np.float32)\n",
    "\n",
    "    # IMPORTANT: must match your training blocks!\n",
    "    X = np.stack([z, hag, dx, dy, intensity, ret_num, nret, scan_angle, deviation, overlap], axis=1).astype(np.float32)\n",
    "    return X\n",
    "\n",
    "# ----------------------------\n",
    "# Predict in blocks\n",
    "# ----------------------------\n",
    "BATCH_POINTS = 4096\n",
    "STRIDE = 4096\n",
    "\n",
    "las = laspy.read(IN_LAZ)\n",
    "X = make_features_from_las(las, cell=2.0)\n",
    "print(\"Total points:\", len(X), \"| X shape:\", X.shape)\n",
    "\n",
    "Xn = (X - Xmean) / (Xstd + 1e-6)\n",
    "\n",
    "pred_idx = np.zeros(len(Xn), dtype=np.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for start in tqdm(range(0, len(Xn), STRIDE), desc=\"Predicting\"):\n",
    "        end = min(start + BATCH_POINTS, len(Xn))\n",
    "        block = Xn[start:end]\n",
    "\n",
    "        if len(block) < BATCH_POINTS:\n",
    "            pad_n = BATCH_POINTS - len(block)\n",
    "            pad = np.repeat(block[-1:], pad_n, axis=0)\n",
    "            block = np.vstack([block, pad])\n",
    "\n",
    "        inp = torch.from_numpy(block).to(DEVICE)         # (N,F)\n",
    "        inp = inp.unsqueeze(0).permute(0, 2, 1)          # (1,F,N)\n",
    "\n",
    "        logits = model(inp)                              # (1,C,N)\n",
    "        p = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()\n",
    "        pred_idx[start:end] = p[: end-start]\n",
    "\n",
    "pred_cls = np.array([classes[i] for i in pred_idx], dtype=np.uint8)\n",
    "\n",
    "las.classification = pred_cls\n",
    "os.makedirs(os.path.dirname(OUT_LAS), exist_ok=True)\n",
    "\n",
    "las.write(OUT_LAS)\n",
    "print(\"Saved LAS:\", OUT_LAS)\n",
    "\n",
    "try:\n",
    "    las.write(OUT_LAZ)\n",
    "    print(\"Saved LAZ:\", OUT_LAZ)\n",
    "except Exception as e:\n",
    "    print(\"LAZ write failed (ok):\", e)\n",
    "\n",
    "u, c = np.unique(pred_cls, return_counts=True)\n",
    "print(\"Pred class counts:\", dict(zip(u.tolist(), c.tolist())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "148a617d-d27a-444e-a929-ba07264fbde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt keys: dict_keys(['model_state', 'classes', 'class_to_idx', 'Xmean', 'Xstd', 'feats'])\n",
      "feats: 10\n",
      "Xmean shape: (10,)\n",
      "Xstd  shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(MODEL_PT, map_location=\"cpu\", weights_only=False)\n",
    "print(\"ckpt keys:\", ckpt.keys())\n",
    "print(\"feats:\", ckpt.get(\"feats\", None))   # very important\n",
    "print(\"Xmean shape:\", np.array(ckpt.get(\"Xmean\")).shape)\n",
    "print(\"Xstd  shape:\", np.array(ckpt.get(\"Xstd\")).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fcd5403b-f995-4a58-baaf-8898a753c6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X[0]: [-7.4062500e+01 -1.0600000e+02 -5.1693678e-01  1.9999981e-02\n",
      "  3.9495000e+04  1.0000000e+00  1.0000000e+00  3.5000000e+01\n",
      "  0.0000000e+00  1.6000009e-01]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Xpred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain X[0]:\u001b[39m\u001b[38;5;124m\"\u001b[39m, d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# After you build prediction features matrix Xpred for LAS:\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPred  X[0]:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mXpred\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Xpred' is not defined"
     ]
    }
   ],
   "source": [
    "d = np.load(train_files[0])\n",
    "print(\"Train X[0]:\", d[\"X\"][0])\n",
    "\n",
    "# After you build prediction features matrix Xpred for LAS:\n",
    "print(\"Pred  X[0]:\", Xpred[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e26ebebe-10cd-4767-86d1-73c2fe2505d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train keys: ['X', 'y', 'idx']\n",
      "Train X shape: (4096, 10) y shape: (4096,)\n",
      "Train X[0]: [ 6.1937500e+01 -1.2750000e+02 -3.8357973e-02  5.9999943e-02\n",
      "  3.4995000e+04  1.0000000e+00  1.0000000e+00  2.0000000e+01\n",
      "  0.0000000e+00  1.0999999e+00]\n",
      "\n",
      "ckpt keys: dict_keys(['model_state', 'classes', 'class_to_idx', 'Xmean', 'Xstd', 'feats'])\n",
      "feats: 10\n",
      "Xmean shape: (10,) Xstd shape: (10,)\n",
      "\n",
      "Pred X shape: (12374846, 10)\n",
      "Pred X[0]: [-1.409375e+02 -5.755000e+02  1.500000e+00  0.000000e+00  0.000000e+00\n",
      "  0.000000e+00  3.460200e+04  1.000000e+00  1.000000e+00  0.000000e+00]\n",
      "\n",
      "Train norm[0]: [ 0.21162534 -0.72202593 -0.01150283 -0.26280332 -0.19306287 -0.200048\n",
      " -0.22842097  0.17862275 -0.18284726  0.12864566]\n",
      "Pred  norm[0]: [-4.8128140e-01 -2.6159768e+00  4.0655655e-01 -3.2531869e-01\n",
      " -5.3476200e+00 -3.9973624e+00  1.1301210e+05  1.5767007e-01\n",
      "  4.6956405e-02 -6.0108793e-01]\n",
      "\n",
      "Diff (abs) norm: [6.9290674e-01 1.8939509e+00 4.1805938e-01 6.2515378e-02 5.1545572e+00\n",
      " 3.7973144e+00 1.1301233e+05 2.0952687e-02 2.2980367e-01 7.2973359e-01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import laspy\n",
    "import torch\n",
    "\n",
    "# -------- paths --------\n",
    "TRAIN_NPZ = r\"D:/lidarrrrr/anbu/dl_dataset/blocks/block_0000000.npz\"\n",
    "MODEL_PT  = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"   # change if needed\n",
    "PRED_LAZ  = r\"D:/lidarrrrr/anbu/DX3035724 S.GIUSTO000001.laz\" # change if needed\n",
    "\n",
    "# -------- load training sample --------\n",
    "d = np.load(TRAIN_NPZ)\n",
    "print(\"Train keys:\", d.files)\n",
    "print(\"Train X shape:\", d[\"X\"].shape, \"y shape:\", d[\"y\"].shape)\n",
    "print(\"Train X[0]:\", d[\"X\"][0])\n",
    "\n",
    "# -------- load checkpoint stats --------\n",
    "ckpt = torch.load(MODEL_PT, map_location=\"cpu\", weights_only=False)\n",
    "Xmean = np.asarray(ckpt[\"Xmean\"], dtype=np.float32)\n",
    "Xstd  = np.asarray(ckpt[\"Xstd\"],  dtype=np.float32)\n",
    "feats = int(ckpt[\"feats\"])\n",
    "print(\"\\nckpt keys:\", ckpt.keys())\n",
    "print(\"feats:\", feats)\n",
    "print(\"Xmean shape:\", Xmean.shape, \"Xstd shape:\", Xstd.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# IMPORTANT: this MUST match your dataset maker!\n",
    "# If your dataset maker used local block coords,\n",
    "# you must do the same here.\n",
    "# -----------------------------\n",
    "def build_features_for_prediction(las):\n",
    "    xyz = np.vstack([las.x, las.y, las.z]).T.astype(np.float32)\n",
    "\n",
    "    # ---- FEATURE 1–2: local XY (to match Train X[0] negative values) ----\n",
    "    # If your dataset builder used centered coords, do this:\n",
    "    x = xyz[:,0] - np.mean(xyz[:,0])\n",
    "    y = xyz[:,1] - np.mean(xyz[:,1])\n",
    "\n",
    "    # ---- other features: edit to match exactly what you used in blocks ----\n",
    "    # Below is a COMMON 10-feature layout, but you MUST align with your dataset.\n",
    "    z = xyz[:,2].astype(np.float32)\n",
    "\n",
    "    intensity = np.asarray(getattr(las, \"intensity\", np.zeros(len(z))), dtype=np.float32)\n",
    "    rn = np.asarray(getattr(las, \"return_number\", np.ones(len(z))), dtype=np.float32)\n",
    "    nr = np.asarray(getattr(las, \"number_of_returns\", np.ones(len(z))), dtype=np.float32)\n",
    "\n",
    "    # If you had HAG/slope/local_range etc in X, you must compute them here too.\n",
    "    # For now, make placeholders (zeros) so code runs; replace with real computations.\n",
    "    hag = np.zeros_like(z, dtype=np.float32)\n",
    "    slope = np.zeros_like(z, dtype=np.float32)\n",
    "    local_range = np.zeros_like(z, dtype=np.float32)\n",
    "    zstd = np.zeros_like(z, dtype=np.float32)\n",
    "\n",
    "    # ---- choose an order and KEEP IT IDENTICAL to training ----\n",
    "    # Example 10 features:\n",
    "    Xpred = np.stack([\n",
    "        x, y, z, hag, local_range, zstd, intensity, rn, nr, slope\n",
    "    ], axis=1).astype(np.float32)\n",
    "\n",
    "    return Xpred\n",
    "\n",
    "# -------- create Xpred then print --------\n",
    "las = laspy.read(PRED_LAZ)\n",
    "Xpred = build_features_for_prediction(las)\n",
    "\n",
    "print(\"\\nPred X shape:\", Xpred.shape)\n",
    "print(\"Pred X[0]:\", Xpred[0])\n",
    "\n",
    "# -------- compare normalized features (what the model sees) --------\n",
    "train_x0 = d[\"X\"][0].astype(np.float32)\n",
    "pred_x0  = Xpred[0].astype(np.float32)\n",
    "\n",
    "train_norm = (train_x0 - Xmean) / (Xstd + 1e-6)\n",
    "pred_norm  = (pred_x0  - Xmean) / (Xstd + 1e-6)\n",
    "\n",
    "print(\"\\nTrain norm[0]:\", train_norm)\n",
    "print(\"Pred  norm[0]:\", pred_norm)\n",
    "\n",
    "print(\"\\nDiff (abs) norm:\", np.abs(train_norm - pred_norm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a96a84f4-553b-43cd-9ab2-0e036f446aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feats: 10\n",
      "feature list (feats): Not stored\n",
      "Train X mins: [-1.378125e+02 -2.095000e+02 -8.483579e-01  0.000000e+00  0.000000e+00\n",
      "  1.000000e+00  1.000000e+00 -5.900000e+01  0.000000e+00  3.999996e-02]\n",
      "Train X maxs: [1.4343750e+02 1.5750000e+02 1.0561642e+01 1.0889999e+01 4.4083000e+04\n",
      " 5.0000000e+00 5.0000000e+00 5.4000000e+01 0.0000000e+00 1.6670000e+01]\n"
     ]
    }
   ],
   "source": [
    "print(\"feats:\", ckpt[\"feats\"])\n",
    "print(\"feature list (feats):\", ckpt.get(\"feature_names\", \"Not stored\"))\n",
    "print(\"Train X mins:\", d[\"X\"].min(axis=0))\n",
    "print(\"Train X maxs:\", d[\"X\"].max(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd60a7e4-4766-47d7-bffb-e730e6f79b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training block X[0]: [ 6.1937500e+01 -1.2750000e+02 -3.8357973e-02  5.9999943e-02\n",
      "  3.4995000e+04  1.0000000e+00  1.0000000e+00  2.0000000e+01\n",
      "  0.0000000e+00  1.0999999e+00]\n",
      "Raw check 0:\n",
      "x,y,z,inten,rn,nr,scan: 609816.6 4847096.5 2.56 34995.0 1.0 1.0 20.0\n",
      "0 min/max -137.8125 143.4375 | corr(x,y,z,int) 1.0 -0.947 0.132 -0.194\n",
      "1 min/max -209.5 157.5 | corr(x,y,z,int) -0.947 1.0 -0.157 0.19\n",
      "2 min/max -0.8483579158782959 10.561641693115234 | corr(x,y,z,int) 0.132 -0.157 1.0 -0.177\n",
      "3 min/max 0.0 10.889999389648438 | corr(x,y,z,int) 0.02 -0.03 0.916 -0.152\n",
      "4 min/max 0.0 44083.0 | corr(x,y,z,int) -0.194 0.19 -0.177 1.0\n",
      "5 min/max 1.0 5.0 | corr(x,y,z,int) 0.024 -0.028 0.193 -0.419\n",
      "6 min/max 1.0 5.0 | corr(x,y,z,int) 0.013 -0.02 0.397 -0.473\n",
      "7 min/max -59.0 54.0 | corr(x,y,z,int) 0.097 -0.261 -0.013 0.026\n",
      "8 min/max 0.0 0.0 | corr(x,y,z,int) nan nan nan nan\n",
      "9 min/max 0.039999961853027344 16.670000076293945 | corr(x,y,z,int) 0.076 -0.12 0.486 -0.193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\lidar\\lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3045: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "D:\\anaconda\\envs\\lidar\\lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3046: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, laspy\n",
    "\n",
    "BLOCK_NPZ = r\"D:/lidarrrrr/anbu/dl_dataset/blocks/block_0000000.npz\"\n",
    "SOURCE_LAZ = r\"D:/lidarrrrr/anbu/training_labeled/DX3011148 ULMIANO000001.laz\"  # <-- the file that block came from\n",
    "\n",
    "d = np.load(BLOCK_NPZ)\n",
    "idx = d[\"idx\"].astype(np.int64)\n",
    "X_train = d[\"X\"].astype(np.float32)\n",
    "\n",
    "las = laspy.read(SOURCE_LAZ)\n",
    "\n",
    "# raw dims\n",
    "x = np.asarray(las.x, dtype=np.float32)[idx]\n",
    "y = np.asarray(las.y, dtype=np.float32)[idx]\n",
    "z = np.asarray(las.z, dtype=np.float32)[idx]\n",
    "inten = np.asarray(las.intensity, dtype=np.float32)[idx]\n",
    "rn = np.asarray(las.return_number, dtype=np.float32)[idx]\n",
    "nr = np.asarray(las.number_of_returns, dtype=np.float32)[idx]\n",
    "\n",
    "# optional dims (may not exist)\n",
    "def get_dim(las, name, default=0.0):\n",
    "    try:\n",
    "        return np.asarray(las[name], dtype=np.float32)[idx]\n",
    "    except Exception:\n",
    "        if name == \"scan_angle\":\n",
    "            try:\n",
    "                return np.asarray(las[\"scan_angle_rank\"], dtype=np.float32)[idx]\n",
    "            except Exception:\n",
    "                pass\n",
    "    return np.full(len(idx), default, dtype=np.float32)\n",
    "\n",
    "scan_angle = get_dim(las, \"scan_angle\", 0.0)\n",
    "\n",
    "# now compare against training columns\n",
    "print(\"Training block X[0]:\", X_train[0])\n",
    "print(\"Raw check 0:\")\n",
    "print(\"x,y,z,inten,rn,nr,scan:\", x[0], y[0], z[0], inten[0], rn[0], nr[0], scan_angle[0])\n",
    "\n",
    "# Correlation guess: which X column matches intensity etc.\n",
    "for j in range(10):\n",
    "    col = X_train[:, j]\n",
    "    # print rough min/max and a correlation with intensity\n",
    "    corr_int = np.corrcoef(col, inten)[0,1]\n",
    "    corr_z = np.corrcoef(col, z)[0,1]\n",
    "    corr_x = np.corrcoef(col, x)[0,1]\n",
    "    corr_y = np.corrcoef(col, y)[0,1]\n",
    "    print(j, \"min/max\", float(col.min()), float(col.max()),\n",
    "          \"| corr(x,y,z,int)\", round(corr_x,3), round(corr_y,3), round(corr_z,3), round(corr_int,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4e722cf-0959-40dd-a07e-6cd363b9af28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loaded model. Classes: [1, 2, 3, 6, 7, 12, 13]\n",
      "Input features: 10 | Num classes: 7\n",
      "Total tiles: 118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting tiles: 100%|██████████| 8/8 [00:01<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LAS: D:/lidarrrrr/anbu/New folder/dl_predicted_fixed.las\n",
      "Saved LAZ: D:/lidarrrrr/anbu/New folder/dl_predicted_fixed.laz\n",
      "Pred class counts: {1: 12087409, 2: 232242, 3: 54453, 12: 742}\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# DL PREDICT (PointNet ckpt) — FULL WORKING SCRIPT\n",
    "# Matches your discovered 10-feature order\n",
    "# ===========================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import laspy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# PATHS (edit)\n",
    "# ----------------------------\n",
    "IN_LAZ   = r\"D:/lidarrrrr/anbu/DX3035724 S.GIUSTO000001.laz\"\n",
    "MODEL_PT = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"   # your ckpt\n",
    "OUT_LAS  = r\"D:/lidarrrrr/anbu/New folder/dl_predicted_fixed.las\"\n",
    "OUT_LAZ  = r\"D:/lidarrrrr/anbu/New folder/dl_predicted_fixed.laz\"\n",
    "\n",
    "# ----------------------------\n",
    "# BLOCKING (must match training)\n",
    "# ----------------------------\n",
    "TILE_SIZE = 40.0      # <-- if your dataset maker used another value, change it\n",
    "NPTS      = 4096\n",
    "BATCH_BLK = 16        # blocks per GPU batch (adjust if OOM)\n",
    "\n",
    "# ----------------------------\n",
    "# FEATURE GRID for HAG / local variability\n",
    "# ----------------------------\n",
    "CELL = 2.0            # grid size for zmin/zmax (keep stable)\n",
    "EPS  = 1e-6\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ----------------------------\n",
    "# Robust dimension getter\n",
    "# ----------------------------\n",
    "def get_dim(las, name, default=0.0, dtype=np.float32):\n",
    "    # Works with standard dims + ExtraBytes\n",
    "    try:\n",
    "        return np.asarray(las[name], dtype=dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # common alternate name\n",
    "    if name == \"scan_angle\":\n",
    "        for alt in [\"scan_angle_rank\", \"scan_angle_rank_degrees\"]:\n",
    "            try:\n",
    "                return np.asarray(las[alt], dtype=dtype)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return np.full(len(las.x), default, dtype=dtype)\n",
    "\n",
    "# ----------------------------\n",
    "# Grid stats: zmin/zmax per XY cell\n",
    "# ----------------------------\n",
    "def grid_zmin_zmax(x, y, z, cell=CELL):\n",
    "    minx, miny = float(x.min()), float(y.min())\n",
    "    gx = np.floor((x - minx) / cell).astype(np.int32)\n",
    "    gy = np.floor((y - miny) / cell).astype(np.int32)\n",
    "    key = gx.astype(np.int64) * 1_000_000 + gy.astype(np.int64)\n",
    "\n",
    "    order = np.argsort(key)\n",
    "    key_s = key[order]\n",
    "    z_s   = z[order]\n",
    "\n",
    "    uniq, start = np.unique(key_s, return_index=True)\n",
    "    zmin = np.empty(len(uniq), dtype=np.float32)\n",
    "    zmax = np.empty(len(uniq), dtype=np.float32)\n",
    "\n",
    "    for i in range(len(uniq)):\n",
    "        a = start[i]\n",
    "        b = start[i+1] if i+1 < len(uniq) else len(key_s)\n",
    "        zs = z_s[a:b]\n",
    "        zmin[i] = float(zs.min())\n",
    "        zmax[i] = float(zs.max())\n",
    "\n",
    "    # map back\n",
    "    pos = np.searchsorted(uniq, key)\n",
    "    pos = np.clip(pos, 0, len(uniq)-1)\n",
    "    return zmin[pos], zmax[pos], minx, miny\n",
    "\n",
    "# ----------------------------\n",
    "# Build 10 features EXACT ORDER\n",
    "# 0 dx, 1 dy, 2 hag_raw, 3 hag_clip, 4 intensity, 5 rn, 6 nr, 7 scan, 8 zero, 9 local_range\n",
    "# ----------------------------\n",
    "def make_X_for_points(x, y, z, intensity, rn, nr, scan, zmin, zmax, cx, cy):\n",
    "    dx = (x - cx).astype(np.float32)\n",
    "    dy = (y - cy).astype(np.float32)\n",
    "\n",
    "    hag_raw  = (z - zmin).astype(np.float32)                  # can be negative if grid min isn't true ground\n",
    "    hag_clip = np.maximum(hag_raw, 0.0).astype(np.float32)\n",
    "\n",
    "    local_range = (zmax - zmin).astype(np.float32)            # matches your feat9 magnitude (0..~16)\n",
    "\n",
    "    X = np.stack([\n",
    "        dx,\n",
    "        dy,\n",
    "        hag_raw,\n",
    "        hag_clip,\n",
    "        intensity.astype(np.float32),\n",
    "        rn.astype(np.float32),\n",
    "        nr.astype(np.float32),\n",
    "        scan.astype(np.float32),\n",
    "        np.zeros_like(dx, dtype=np.float32),                  # feat8 = constant 0 in your training\n",
    "        local_range\n",
    "    ], axis=1).astype(np.float32)\n",
    "\n",
    "    return X\n",
    "\n",
    "# ----------------------------\n",
    "# PointNet (must match checkpoint architecture)\n",
    "# Your ckpt summary:\n",
    "#  - mlp1 last out: 128\n",
    "#  - mlp2: 128 -> 256\n",
    "#  - head: 512 -> 256 -> C\n",
    "# ----------------------------\n",
    "class PointNetFromCkpt(nn.Module):\n",
    "    def __init__(self, in_ch, num_classes):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 64, 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(64, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        # ✅ match checkpoint: head.0, head.1, head.4\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(512, 256, 1),    # head.0\n",
    "            nn.BatchNorm1d(256),       # head.1\n",
    "            nn.ReLU(True),             # head.2\n",
    "            nn.Dropout(p=0.3),         # head.3  (this was missing)\n",
    "            nn.Conv1d(256, num_classes, 1)  # head.4\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: (B,F,N)\n",
    "        h = self.mlp1(x)                      # (B,128,N)\n",
    "        h_local = self.mlp2(h)                # (B,256,N)\n",
    "        g = torch.max(h_local, dim=2, keepdim=True).values  # (B,256,1)\n",
    "        g = g.repeat(1, 1, h_local.size(2))                 # (B,256,N)\n",
    "        feat = torch.cat([h_local, g], dim=1)               # (B,512,N)\n",
    "        out = self.head(feat)                               # (B,C,N)\n",
    "        return out\n",
    "\n",
    "# ----------------------------\n",
    "# Load checkpoint safely (PyTorch 2.6+ default changed)\n",
    "# ----------------------------\n",
    "ckpt = torch.load(MODEL_PT, map_location=DEVICE, weights_only=False)\n",
    "state = ckpt[\"model_state\"]\n",
    "classes = ckpt[\"classes\"]                 # original class labels like [1,2,3,6,7,12,13]\n",
    "class_to_idx = ckpt[\"class_to_idx\"]\n",
    "Xmean = ckpt[\"Xmean\"].astype(np.float32)\n",
    "Xstd  = ckpt[\"Xstd\"].astype(np.float32)\n",
    "IN_CH = int(ckpt.get(\"feats\", 10))\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"Loaded model. Classes:\", classes)\n",
    "print(\"Input features:\", IN_CH, \"| Num classes:\", NUM_CLASSES)\n",
    "\n",
    "model = PointNetFromCkpt(in_ch=IN_CH, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------\n",
    "# Read LAZ\n",
    "# ----------------------------\n",
    "las = laspy.read(IN_LAZ)\n",
    "x = np.asarray(las.x, dtype=np.float32)\n",
    "y = np.asarray(las.y, dtype=np.float32)\n",
    "z = np.asarray(las.z, dtype=np.float32)\n",
    "\n",
    "intensity = get_dim(las, \"intensity\", 0.0)\n",
    "rn        = get_dim(las, \"return_number\", 1.0)\n",
    "nr        = get_dim(las, \"number_of_returns\", 1.0)\n",
    "scan      = get_dim(las, \"scan_angle\", 0.0)\n",
    "\n",
    "zmin, zmax, minx, miny = grid_zmin_zmax(x, y, z, cell=CELL)\n",
    "\n",
    "# ----------------------------\n",
    "# Build tile -> indices list\n",
    "# ----------------------------\n",
    "gx = np.floor((x - minx) / TILE_SIZE).astype(np.int32)\n",
    "gy = np.floor((y - miny) / TILE_SIZE).astype(np.int32)\n",
    "tile_key = gx.astype(np.int64) * 1_000_000 + gy.astype(np.int64)\n",
    "\n",
    "# group points by tile\n",
    "order = np.argsort(tile_key)\n",
    "tile_s = tile_key[order]\n",
    "\n",
    "uniq_tiles, start = np.unique(tile_s, return_index=True)\n",
    "\n",
    "tile_slices = []\n",
    "for i in range(len(uniq_tiles)):\n",
    "    a = start[i]\n",
    "    b = start[i+1] if i+1 < len(uniq_tiles) else len(order)\n",
    "    idxs = order[a:b]\n",
    "    if len(idxs) < 200:   # skip tiny tiles\n",
    "        continue\n",
    "    tile_slices.append(idxs)\n",
    "\n",
    "print(\"Total tiles:\", len(tile_slices))\n",
    "\n",
    "# ----------------------------\n",
    "# Predict per tile in batches\n",
    "# ----------------------------\n",
    "pred_idx = np.zeros(len(x), dtype=np.int64)  # stores class index 0..C-1\n",
    "\n",
    "def norm_X(X):\n",
    "    return (X - Xmean[None, :]) / (Xstd[None, :] + 1e-12)\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(range(0, len(tile_slices), BATCH_BLK), desc=\"Predicting tiles\")\n",
    "    for t0 in pbar:\n",
    "        batch_tiles = tile_slices[t0:t0 + BATCH_BLK]\n",
    "\n",
    "        X_batch = []\n",
    "        pick_map = []   # (point_indices_original, used_count)\n",
    "        for idxs in batch_tiles:\n",
    "            # sample NPTS points from this tile\n",
    "            if len(idxs) >= NPTS:\n",
    "                pick = rng.choice(idxs, NPTS, replace=False)\n",
    "            else:\n",
    "                pick = rng.choice(idxs, NPTS, replace=True)\n",
    "\n",
    "            # tile center for dx/dy (must match training)\n",
    "            cx = float(x[pick].mean())\n",
    "            cy = float(y[pick].mean())\n",
    "\n",
    "            Xb = make_X_for_points(\n",
    "                x[pick], y[pick], z[pick],\n",
    "                intensity[pick], rn[pick], nr[pick], scan[pick],\n",
    "                zmin[pick], zmax[pick],\n",
    "                cx, cy\n",
    "            )\n",
    "            Xb = norm_X(Xb)\n",
    "\n",
    "            X_batch.append(Xb)\n",
    "            pick_map.append(pick)\n",
    "\n",
    "        X_batch = np.stack(X_batch, axis=0)                   # (B,N,F)\n",
    "        inp = torch.from_numpy(X_batch).to(DEVICE)            # (B,N,F)\n",
    "        inp = inp.permute(0, 2, 1).contiguous()               # (B,F,N)\n",
    "\n",
    "        logits = model(inp)                                   # (B,C,N)\n",
    "        p = torch.argmax(logits, dim=1).cpu().numpy()         # (B,N) class-index\n",
    "\n",
    "        # write back to original points (only for sampled points)\n",
    "        for b in range(len(batch_tiles)):\n",
    "            pred_idx[pick_map[b]] = p[b]\n",
    "\n",
    "# map class index -> real class label (LAS classification)\n",
    "pred_class = np.array([classes[i] for i in pred_idx], dtype=np.uint8)\n",
    "\n",
    "las.classification = pred_class\n",
    "las.write(OUT_LAS)\n",
    "print(\"Saved LAS:\", OUT_LAS)\n",
    "\n",
    "try:\n",
    "    las.write(OUT_LAZ)\n",
    "    print(\"Saved LAZ:\", OUT_LAZ)\n",
    "except Exception as e:\n",
    "    print(\"LAZ write failed (ok):\", e)\n",
    "\n",
    "u, c = np.unique(pred_class, return_counts=True)\n",
    "print(\"Pred class counts:\", dict(zip(u.tolist(), c.tolist())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "876c93fc-ed6e-4bda-b730-0d5934a22b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PointNetFromCkpt(in_ch=IN_CH, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "model.load_state_dict(state, strict=True)   # ✅ should load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d56535ca-db03-4987-b60a-9c675957eb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['head.0.weight', 'head.0.bias', 'head.1.weight', 'head.1.bias', 'head.1.running_mean', 'head.1.running_var', 'head.1.num_batches_tracked', 'head.4.weight', 'head.4.bias']\n"
     ]
    }
   ],
   "source": [
    "print([k for k in state.keys() if k.startswith(\"head.\")][:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11fcf372-7395-49a1-a6fd-dccf3c8a22d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m d \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[43mtrain_files\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain col mins:\u001b[39m\u001b[38;5;124m\"\u001b[39m, d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmin(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain col maxs:\u001b[39m\u001b[38;5;124m\"\u001b[39m, d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_files' is not defined"
     ]
    }
   ],
   "source": [
    "d = np.load(train_files[0])\n",
    "print(\"Train col mins:\", d[\"X\"].min(axis=0))\n",
    "print(\"Train col maxs:\", d[\"X\"].max(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4264fba3-ac8d-442c-90a7-61c07c1a17e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_block' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPred col mins:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mX_block\u001b[49m\u001b[38;5;241m.\u001b[39mmin(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPred col maxs:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_block\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_block' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Pred col mins:\", X_block.min(axis=0))\n",
    "print(\"Pred col maxs:\", X_block.max(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e898783-75cc-4bb4-a0a1-5b9118da1120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total blocks: 27697\n",
      "Example file: D:/lidarrrrr/anbu/dl_dataset/blocks\\block_0000000.npz\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "BLOCK_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "\n",
    "train_files = sorted(glob.glob(BLOCK_DIR + \"/*.npz\"))\n",
    "\n",
    "print(\"Total blocks:\", len(train_files))\n",
    "print(\"Example file:\", train_files[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fb76e9d-60f9-4a3d-99f1-9d8be8767c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['X', 'y', 'idx']\n",
      "Train col mins: [-1.378125e+02 -2.095000e+02 -8.483579e-01  0.000000e+00  0.000000e+00\n",
      "  1.000000e+00  1.000000e+00 -5.900000e+01  0.000000e+00  3.999996e-02]\n",
      "Train col maxs: [1.4343750e+02 1.5750000e+02 1.0561642e+01 1.0889999e+01 4.4083000e+04\n",
      " 5.0000000e+00 5.0000000e+00 5.4000000e+01 0.0000000e+00 1.6670000e+01]\n",
      "Train col means: [-2.2071991e+00 -1.8538818e+00 -3.7940481e-04  2.1671166e-01\n",
      "  3.5122293e+04  1.0463867e+00  1.0629883e+00  5.8276367e-01\n",
      "  0.0000000e+00  8.9575773e-01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "d = np.load(train_files[0])\n",
    "\n",
    "print(\"Keys:\", d.files)\n",
    "print(\"Train col mins:\", d[\"X\"].min(axis=0))\n",
    "print(\"Train col maxs:\", d[\"X\"].max(axis=0))\n",
    "print(\"Train col means:\", d[\"X\"].mean(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87227d9f-6eae-44e3-bcb2-280a763e16f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_block' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPred col mins:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mX_block\u001b[49m\u001b[38;5;241m.\u001b[39mmin(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPred col maxs:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_block\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_block' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Pred col mins:\", X_block.min(axis=0))\n",
    "print(\"Pred col maxs:\", X_block.max(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "941c64b4-e23e-4a9a-a9a6-cb6eed93dfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n",
      "\n",
      "Train keys: ['X', 'y', 'idx']\n",
      "Train X shape: (4096, 10) y shape: (4096,)\n",
      "Train X mins: [-1.378125e+02 -2.095000e+02 -8.483579e-01  0.000000e+00  0.000000e+00\n",
      "  1.000000e+00  1.000000e+00 -5.900000e+01  0.000000e+00  3.999996e-02]\n",
      "Train X maxs: [1.4343750e+02 1.5750000e+02 1.0561642e+01 1.0889999e+01 4.4083000e+04\n",
      " 5.0000000e+00 5.0000000e+00 5.4000000e+01 0.0000000e+00 1.6670000e+01]\n",
      "\n",
      "Loaded model. Classes: [1, 2, 3, 6, 7, 12, 13]\n",
      "Input features: 10 | Num classes: 7\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PointNetFromCkpt:\n\tMissing key(s) in state_dict: \"mlp1.6.weight\", \"mlp1.6.bias\", \"mlp1.7.weight\", \"mlp1.7.bias\", \"mlp1.7.running_mean\", \"mlp1.7.running_var\", \"head.3.weight\", \"head.3.bias\". \n\tUnexpected key(s) in state_dict: \"head.4.weight\", \"head.4.bias\". \n\tsize mismatch for mlp1.3.weight: copying a param with shape torch.Size([128, 64, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1]).\n\tsize mismatch for mlp1.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp1.4.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp1.4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp1.4.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp1.4.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 206\u001b[0m\n\u001b[0;32m    203\u001b[0m state \u001b[38;5;241m=\u001b[39m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    205\u001b[0m model \u001b[38;5;241m=\u001b[39m PointNetFromCkpt(in_ch\u001b[38;5;241m=\u001b[39mIN_CH, num_classes\u001b[38;5;241m=\u001b[39mNUM_CLASSES)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m--> 206\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Model loaded successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\lidar\\lib\\site-packages\\torch\\nn\\modules\\module.py:2635\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2627\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2628\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2629\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2630\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2631\u001b[0m             ),\n\u001b[0;32m   2632\u001b[0m         )\n\u001b[0;32m   2634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2635\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2636\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2637\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2638\u001b[0m         )\n\u001b[0;32m   2639\u001b[0m     )\n\u001b[0;32m   2640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PointNetFromCkpt:\n\tMissing key(s) in state_dict: \"mlp1.6.weight\", \"mlp1.6.bias\", \"mlp1.7.weight\", \"mlp1.7.bias\", \"mlp1.7.running_mean\", \"mlp1.7.running_var\", \"head.3.weight\", \"head.3.bias\". \n\tUnexpected key(s) in state_dict: \"head.4.weight\", \"head.4.bias\". \n\tsize mismatch for mlp1.3.weight: copying a param with shape torch.Size([128, 64, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1]).\n\tsize mismatch for mlp1.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp1.4.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp1.4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp1.4.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp1.4.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64])."
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import laspy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# =========================================================\n",
    "# EDIT PATHS\n",
    "# =========================================================\n",
    "TRAIN_BLOCK_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"     # has .npz blocks with keys X,y,idx\n",
    "IN_LAS_OR_LAZ   = r\"D:/lidarrrrr/anbu/New folder/stage1_outputs/DX3035724_stage1_ground_v2.las\"\n",
    "MODEL_PT        = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"   # your saved ckpt\n",
    "OUT_LAS         = r\"D:/lidarrrrr/anbu/New folder/dl_predicted_fixed2.las\"\n",
    "OUT_LAZ         = r\"D:/lidarrrrr/anbu/New folder/dl_predicted_fixed2.laz\"\n",
    "\n",
    "# =========================================================\n",
    "# SETTINGS (must match training)\n",
    "# =========================================================\n",
    "CELL = 3.0          # same grid cell you used for HAG\n",
    "BLOCK = 4096        # same points per block used in training blocks\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================================================\n",
    "# 1) LOAD 1 TRAINING BLOCK AND PRINT FEATURE STATS\n",
    "# =========================================================\n",
    "train_files = sorted(glob.glob(os.path.join(TRAIN_BLOCK_DIR, \"*.npz\")))\n",
    "if not train_files:\n",
    "    raise RuntimeError(\"No training .npz found in TRAIN_BLOCK_DIR\")\n",
    "\n",
    "d = np.load(train_files[0])\n",
    "Xtr = d[\"X\"].astype(np.float32)\n",
    "ytr = d[\"y\"].astype(np.int64)\n",
    "print(\"\\nTrain keys:\", list(d.keys()))\n",
    "print(\"Train X shape:\", Xtr.shape, \"y shape:\", ytr.shape)\n",
    "print(\"Train X mins:\", Xtr.min(axis=0))\n",
    "print(\"Train X maxs:\", Xtr.max(axis=0))\n",
    "\n",
    "# =========================================================\n",
    "# 2) HELPERS: robust LAS dimension getter\n",
    "# =========================================================\n",
    "def get_dim(las, name, fallback=0.0, dtype=np.float32):\n",
    "    # Try normal dimension name\n",
    "    try:\n",
    "        return np.asarray(las[name]).astype(dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Special cases\n",
    "    if name == \"scan_angle\":\n",
    "        for alt in [\"scan_angle\", \"scan_angle_rank\"]:\n",
    "            try:\n",
    "                return np.asarray(las[alt]).astype(dtype)\n",
    "            except Exception:\n",
    "                pass\n",
    "    # RGB fields may not exist\n",
    "    if name in [\"red\", \"green\", \"blue\"]:\n",
    "        return np.full(len(las.x), fallback, dtype=dtype)\n",
    "    # Extra bytes may not exist\n",
    "    return np.full(len(las.x), fallback, dtype=dtype)\n",
    "\n",
    "# =========================================================\n",
    "# 3) BUILD PRED FEATURES (10 features)\n",
    "#    IMPORTANT: This must match training feature order.\n",
    "#    We use this order:\n",
    "#    [x, y, z, hag, intensity, ret_num, n_returns, scan_angle, deviation, slope]\n",
    "# =========================================================\n",
    "def compute_hag_and_slope(xyz, cls, cell=3.0, ground_class=2):\n",
    "    x, y, z = xyz[:,0], xyz[:,1], xyz[:,2]\n",
    "    minx, miny = x.min(), y.min()\n",
    "    gx = np.floor((x - minx)/cell).astype(np.int32)\n",
    "    gy = np.floor((y - miny)/cell).astype(np.int32)\n",
    "\n",
    "    # ground surface per cell from ground points (class 2)\n",
    "    cell_min = {}\n",
    "    g_idx = np.where(cls == ground_class)[0]\n",
    "    for i in g_idx:\n",
    "        k = (gx[i], gy[i])\n",
    "        zi = float(z[i])\n",
    "        if (k not in cell_min) or (zi < cell_min[k]):\n",
    "            cell_min[k] = zi\n",
    "\n",
    "    hag = np.zeros(len(z), dtype=np.float32)\n",
    "    has_ground = np.zeros(len(z), dtype=bool)\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        if k in cell_min:\n",
    "            hag[i] = z[i] - cell_min[k]\n",
    "            has_ground[i] = True\n",
    "        else:\n",
    "            hag[i] = 0.0\n",
    "\n",
    "    # local range proxy (zmax-zmin) for slope\n",
    "    # compute zmin/zmax per cell quickly using dict\n",
    "    cell_zmin, cell_zmax = {}, {}\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        zi = float(z[i])\n",
    "        if k not in cell_zmin:\n",
    "            cell_zmin[k] = zi\n",
    "            cell_zmax[k] = zi\n",
    "        else:\n",
    "            if zi < cell_zmin[k]: cell_zmin[k] = zi\n",
    "            if zi > cell_zmax[k]: cell_zmax[k] = zi\n",
    "\n",
    "    local_range = np.zeros(len(z), dtype=np.float32)\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        local_range[i] = float(cell_zmax[k] - cell_zmin[k])\n",
    "\n",
    "    slope = (hag / (local_range + 1e-6)).astype(np.float32)\n",
    "    return hag, slope, has_ground\n",
    "\n",
    "def make_Xpred(las, cell=3.0):\n",
    "    xyz = np.vstack([las.x, las.y, las.z]).T.astype(np.float32)\n",
    "    cls = np.asarray(las.classification, dtype=np.int32)\n",
    "\n",
    "    intensity = get_dim(las, \"intensity\", fallback=0.0, dtype=np.float32)\n",
    "    ret_num   = get_dim(las, \"return_number\", fallback=1.0, dtype=np.float32)\n",
    "    n_returns = get_dim(las, \"number_of_returns\", fallback=1.0, dtype=np.float32)\n",
    "    scan_ang  = get_dim(las, \"scan_angle\", fallback=0.0, dtype=np.float32)\n",
    "    deviation = get_dim(las, \"Deviation\", fallback=0.0, dtype=np.float32)   # extra bytes\n",
    "\n",
    "    hag, slope, has_ground = compute_hag_and_slope(xyz, cls, cell=cell)\n",
    "\n",
    "    X = np.stack([\n",
    "        xyz[:,0], xyz[:,1], xyz[:,2],\n",
    "        hag,\n",
    "        intensity,\n",
    "        ret_num,\n",
    "        n_returns,\n",
    "        scan_ang,\n",
    "        deviation,\n",
    "        slope\n",
    "    ], axis=1).astype(np.float32)\n",
    "\n",
    "    return X, xyz, cls, has_ground\n",
    "\n",
    "# =========================================================\n",
    "# 4) LOAD CKPT + BUILD MODEL THAT MATCHES CKPT SHAPES\n",
    "#    Your ckpt keys: model_state, classes, class_to_idx, Xmean, Xstd, feats\n",
    "# =========================================================\n",
    "class PointNetFromCkpt(nn.Module):\n",
    "    \"\"\"\n",
    "    Matches a common 'PointNet-small' used in your training:\n",
    "    - mlp1: (in -> 64 -> 64 -> 128) conv1d + BN + ReLU\n",
    "    - mlp2: (128 -> 256) conv1d + BN + ReLU\n",
    "    - global max pool -> 256\n",
    "    - concat local(256) + global(256) = 512\n",
    "    - head: (512 -> 256 -> C) conv1d\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=10, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 64, 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(64, 64, 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(64, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(512, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(256, num_classes, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, F, N)\n",
    "        h = self.mlp1(x)           # (B,128,N)\n",
    "        h = self.mlp2(h)           # (B,256,N)\n",
    "        g = torch.max(h, dim=2, keepdim=True)[0]   # (B,256,1)\n",
    "        g = g.repeat(1, 1, h.shape[2])             # (B,256,N)\n",
    "        feat = torch.cat([h, g], dim=1)            # (B,512,N)\n",
    "        out = self.head(feat)                      # (B,C,N)\n",
    "        return out\n",
    "\n",
    "# Load ckpt safely (PyTorch 2.6+)\n",
    "ckpt = torch.load(MODEL_PT, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "classes = ckpt[\"classes\"]\n",
    "class_to_idx = ckpt[\"class_to_idx\"]\n",
    "Xmean = ckpt[\"Xmean\"].astype(np.float32)\n",
    "Xstd  = ckpt[\"Xstd\"].astype(np.float32)\n",
    "IN_CH = int(ckpt[\"feats\"])\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "print(\"\\nLoaded model. Classes:\", classes)\n",
    "print(\"Input features:\", IN_CH, \"| Num classes:\", NUM_CLASSES)\n",
    "\n",
    "state = ckpt[\"model_state\"]\n",
    "\n",
    "model = PointNetFromCkpt(in_ch=IN_CH, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()\n",
    "print(\"✅ Model loaded successfully.\")\n",
    "\n",
    "# =========================================================\n",
    "# 5) BUILD PRED FEATURES + PRINT STATS\n",
    "# =========================================================\n",
    "las = laspy.read(IN_LAS_OR_LAZ)\n",
    "Xpred, xyz, cls_in, has_ground = make_Xpred(las, cell=CELL)\n",
    "\n",
    "print(\"\\nPred X shape:\", Xpred.shape)\n",
    "print(\"Pred X mins:\", Xpred.min(axis=0))\n",
    "print(\"Pred X maxs:\", Xpred.max(axis=0))\n",
    "\n",
    "# normalize EXACTLY like training\n",
    "Xpred_n = (Xpred - Xmean[None, :]) / (Xstd[None, :] + 1e-6)\n",
    "\n",
    "# show first-row sanity\n",
    "print(\"\\nTrain X[0]:\", Xtr[0])\n",
    "print(\"Pred  X[0]:\", Xpred[0])\n",
    "print(\"Train norm[0]:\", (Xtr[0] - Xmean) / (Xstd + 1e-6))\n",
    "print(\"Pred  norm[0]:\", (Xpred[0] - Xmean) / (Xstd + 1e-6))\n",
    "\n",
    "# =========================================================\n",
    "# 6) PREDICT IN BLOCKS + SAVE LAS/LAZ\n",
    "# =========================================================\n",
    "pred_idx = np.zeros((len(Xpred_n),), dtype=np.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(range(0, len(Xpred_n), BLOCK), desc=\"Predicting blocks\")\n",
    "    for start in pbar:\n",
    "        end = min(start + BLOCK, len(Xpred_n))\n",
    "        X_block = Xpred_n[start:end]  # <-- defined here\n",
    "\n",
    "        # print once (fixes your NameError problem)\n",
    "        if start == 0:\n",
    "            print(\"\\n[DEBUG] X_block mins:\", X_block.min(axis=0))\n",
    "            print(\"[DEBUG] X_block maxs:\", X_block.max(axis=0))\n",
    "\n",
    "        inp = torch.from_numpy(X_block).to(DEVICE)          # (N,F)\n",
    "        inp = inp.unsqueeze(0).permute(0, 2, 1)             # (1,F,N)\n",
    "\n",
    "        logits = model(inp)                                  # (1,C,N)\n",
    "        p = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()\n",
    "        pred_idx[start:end] = p[:end-start]\n",
    "\n",
    "# map back to LAS class codes\n",
    "idx_to_class = {i:c for c,i in class_to_idx.items()}\n",
    "pred_class = np.vectorize(idx_to_class.get)(pred_idx).astype(np.uint8)\n",
    "\n",
    "las.classification = pred_class\n",
    "las.write(OUT_LAS)\n",
    "print(\"\\nSaved LAS:\", OUT_LAS)\n",
    "try:\n",
    "    las.write(OUT_LAZ)\n",
    "    print(\"Saved LAZ:\", OUT_LAZ)\n",
    "except Exception as e:\n",
    "    print(\"LAZ write failed (ok):\", e)\n",
    "\n",
    "u, c = np.unique(pred_class, return_counts=True)\n",
    "print(\"Pred class counts:\", dict(zip(u.tolist(), c.tolist())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4acd74f-2f94-4ca1-9b4b-d923a71ca38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PointNetFromCkptV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Matches checkpoint structure:\n",
    "    mlp1: Conv(in->64) + BN + ReLU\n",
    "          Conv(64->128) + BN + ReLU\n",
    "          Conv(128->128) + BN + ReLU\n",
    "    mlp2: Conv(128->256) + BN + ReLU\n",
    "    head: Conv(512->256) + BN + ReLU + Dropout + Conv(256->C)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=10, num_classes=7, drop=0.3):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 64, 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv1d(64, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv1d(128, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(512, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=drop),              # <-- shifts final conv to head.4.*\n",
    "            nn.Conv1d(256, num_classes, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,F,N)\n",
    "        h = self.mlp1(x)                            # (B,128,N)\n",
    "        h = self.mlp2(h)                            # (B,256,N)\n",
    "        g = torch.max(h, dim=2, keepdim=True)[0]     # (B,256,1)\n",
    "        g = g.repeat(1, 1, h.shape[2])               # (B,256,N)\n",
    "        feat = torch.cat([h, g], dim=1)              # (B,512,N)\n",
    "        out = self.head(feat)                        # (B,C,N)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33ccc030-496b-426b-acf8-fdfb9b8c6933",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PointNetFromCkptV2:\n\tMissing key(s) in state_dict: \"mlp1.6.weight\", \"mlp1.6.bias\", \"mlp1.7.weight\", \"mlp1.7.bias\", \"mlp1.7.running_mean\", \"mlp1.7.running_var\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m classes \u001b[38;5;241m=\u001b[39m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m PointNetFromCkptV2(in_ch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeats\u001b[39m\u001b[38;5;124m\"\u001b[39m]), num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(classes))\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# ✅ should succeed now\u001b[39;00m\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Model loaded OK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\lidar\\lib\\site-packages\\torch\\nn\\modules\\module.py:2635\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2627\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2628\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2629\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2630\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2631\u001b[0m             ),\n\u001b[0;32m   2632\u001b[0m         )\n\u001b[0;32m   2634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2635\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2636\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2637\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2638\u001b[0m         )\n\u001b[0;32m   2639\u001b[0m     )\n\u001b[0;32m   2640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PointNetFromCkptV2:\n\tMissing key(s) in state_dict: \"mlp1.6.weight\", \"mlp1.6.bias\", \"mlp1.7.weight\", \"mlp1.7.bias\", \"mlp1.7.running_mean\", \"mlp1.7.running_var\". "
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "ckpt = torch.load(MODEL_PT, map_location=DEVICE, weights_only=False)\n",
    "state = ckpt[\"model_state\"]\n",
    "classes = ckpt[\"classes\"]\n",
    "\n",
    "model = PointNetFromCkptV2(in_ch=int(ckpt[\"feats\"]), num_classes=len(classes)).to(DEVICE)\n",
    "model.load_state_dict(state, strict=True)   # ✅ should succeed now\n",
    "model.eval()\n",
    "print(\"✅ Model loaded OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "004e88cf-ab14-44c0-83fa-1aefd1e2fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PointNetFromCkptV3(nn.Module):\n",
    "    \"\"\"\n",
    "    Matches checkpoint:\n",
    "    mlp1: Conv(in->64) + BN + ReLU\n",
    "          Conv(64->128) + BN + ReLU\n",
    "    mlp2: Conv(128->256) + BN + ReLU\n",
    "    head: Conv(512->256) + BN + ReLU + Dropout + Conv(256->C)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=10, num_classes=7, drop=0.3):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 64, 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv1d(64, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(512, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=drop),          # keeps final conv as head.4.*\n",
    "            nn.Conv1d(256, num_classes, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,F,N)\n",
    "        h = self.mlp1(x)                            # (B,128,N)\n",
    "        h = self.mlp2(h)                            # (B,256,N)\n",
    "        g = torch.max(h, dim=2, keepdim=True)[0]     # (B,256,1)\n",
    "        g = g.repeat(1, 1, h.shape[2])               # (B,256,N)\n",
    "        feat = torch.cat([h, g], dim=1)              # (B,512,N)\n",
    "        return self.head(feat)                       # (B,C,N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fdf1049-9518-42c4-ac5a-e05c83826028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded OK | feats: 10 | classes: [1, 2, 3, 6, 7, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "ckpt = torch.load(MODEL_PT, map_location=DEVICE, weights_only=False)\n",
    "state = ckpt[\"model_state\"]\n",
    "classes = ckpt[\"classes\"]\n",
    "\n",
    "model = PointNetFromCkptV3(in_ch=int(ckpt[\"feats\"]), num_classes=len(classes)).to(DEVICE)\n",
    "model.load_state_dict(state, strict=True)   # ✅ should load now\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ Loaded OK | feats:\", ckpt[\"feats\"], \"| classes:\", classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a2fa054-1495-47e2-a58e-b63ac6e30d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n",
      "\n",
      "[TRAIN BLOCK]\n",
      "keys: ['X', 'y', 'idx']\n",
      "X shape: (4096, 10)\n",
      "col mins: [-1.378125e+02 -2.095000e+02 -8.483579e-01  0.000000e+00  0.000000e+00\n",
      "  1.000000e+00  1.000000e+00 -5.900000e+01  0.000000e+00  3.999996e-02]\n",
      "col maxs: [1.4343750e+02 1.5750000e+02 1.0561642e+01 1.0889999e+01 4.4083000e+04\n",
      " 5.0000000e+00 5.0000000e+00 5.4000000e+01 0.0000000e+00 1.6670000e+01]\n",
      "\n",
      "[CKPT]\n",
      "classes: [1, 2, 3, 6, 7, 12, 13]\n",
      "feats: 10 | num_classes: 7\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PointNetFromCkpt:\n\tMissing key(s) in state_dict: \"mlp1.6.weight\", \"mlp1.6.bias\", \"mlp1.7.weight\", \"mlp1.7.bias\", \"mlp1.7.running_mean\", \"mlp1.7.running_var\", \"head.3.weight\", \"head.3.bias\". \n\tUnexpected key(s) in state_dict: \"head.4.weight\", \"head.4.bias\". \n\tsize mismatch for mlp1.3.weight: copying a param with shape torch.Size([128, 64, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1]).\n\tsize mismatch for mlp1.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp1.4.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp1.4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp1.4.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp1.4.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 121\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeats:\u001b[39m\u001b[38;5;124m\"\u001b[39m, IN_CH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| num_classes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, NUM_CLASSES)\n\u001b[0;32m    120\u001b[0m model \u001b[38;5;241m=\u001b[39m PointNetFromCkpt(in_ch\u001b[38;5;241m=\u001b[39mIN_CH, num_classes\u001b[38;5;241m=\u001b[39mNUM_CLASSES)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m--> 121\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Model loaded OK.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\lidar\\lib\\site-packages\\torch\\nn\\modules\\module.py:2635\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2627\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2628\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2629\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2630\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2631\u001b[0m             ),\n\u001b[0;32m   2632\u001b[0m         )\n\u001b[0;32m   2634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2635\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2636\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2637\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2638\u001b[0m         )\n\u001b[0;32m   2639\u001b[0m     )\n\u001b[0;32m   2640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PointNetFromCkpt:\n\tMissing key(s) in state_dict: \"mlp1.6.weight\", \"mlp1.6.bias\", \"mlp1.7.weight\", \"mlp1.7.bias\", \"mlp1.7.running_mean\", \"mlp1.7.running_var\", \"head.3.weight\", \"head.3.bias\". \n\tUnexpected key(s) in state_dict: \"head.4.weight\", \"head.4.bias\". \n\tsize mismatch for mlp1.3.weight: copying a param with shape torch.Size([128, 64, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1]).\n\tsize mismatch for mlp1.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp1.4.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp1.4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp1.4.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for mlp1.4.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64])."
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import laspy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# =========================================================\n",
    "# EDIT PATHS\n",
    "# =========================================================\n",
    "TRAIN_BLOCK_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"     # .npz blocks with keys X,y,idx\n",
    "IN_LAS_OR_LAZ   = r\"D:/lidarrrrr/anbu/New folder/stage1_outputs/DX3035724_stage1_ground_v2.las\"\n",
    "MODEL_PT        = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"\n",
    "OUT_LAS         = r\"D:/lidarrrrr/anbu/New folder/dl_predicted_fixed2.las\"\n",
    "OUT_LAZ         = r\"D:/lidarrrrr/anbu/New folder/dl_predicted_fixed2.laz\"\n",
    "\n",
    "# =========================================================\n",
    "# MUST MATCH TRAINING\n",
    "# =========================================================\n",
    "GROUND_CLASS = 2\n",
    "CELL = 3.0          # HAG grid cell\n",
    "BLOCK = 4096        # points per forward pass\n",
    "TILE_SIZE = 300.0   # IMPORTANT: local XY centering. This is what makes x/y ~ [-150..150]\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================================================\n",
    "# 1) Load 1 training block to see expected feature ranges\n",
    "# =========================================================\n",
    "train_files = sorted(glob.glob(os.path.join(TRAIN_BLOCK_DIR, \"*.npz\")))\n",
    "if not train_files:\n",
    "    raise RuntimeError(\"No .npz found in TRAIN_BLOCK_DIR\")\n",
    "\n",
    "d0 = np.load(train_files[0])\n",
    "Xtr = d0[\"X\"].astype(np.float32)\n",
    "print(\"\\n[TRAIN BLOCK]\")\n",
    "print(\"keys:\", list(d0.keys()))\n",
    "print(\"X shape:\", Xtr.shape)\n",
    "print(\"col mins:\", Xtr.min(axis=0))\n",
    "print(\"col maxs:\", Xtr.max(axis=0))\n",
    "\n",
    "# =========================================================\n",
    "# 2) Robust LAS dimension getter\n",
    "# =========================================================\n",
    "def get_dim(las, name, fallback=0.0, dtype=np.float32):\n",
    "    try:\n",
    "        return np.asarray(las[name]).astype(dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if name == \"scan_angle\":\n",
    "        for alt in [\"scan_angle\", \"scan_angle_rank\"]:\n",
    "            try:\n",
    "                return np.asarray(las[alt]).astype(dtype)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # RGB / extra bytes might not exist\n",
    "    return np.full(len(las.x), fallback, dtype=dtype)\n",
    "\n",
    "# =========================================================\n",
    "# 3) PointNet model (MUST match your checkpoint)\n",
    "# =========================================================\n",
    "class PointNetFromCkpt(nn.Module):\n",
    "    def __init__(self, in_ch=10, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 64, 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv1d(64, 64, 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv1d(64, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(512, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(256, num_classes, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: (B,F,N)\n",
    "        h = self.mlp1(x)                 # (B,128,N)\n",
    "        h = self.mlp2(h)                 # (B,256,N)\n",
    "        g = torch.max(h, 2, keepdim=True)[0]  # (B,256,1)\n",
    "        g = g.repeat(1, 1, h.shape[2])        # (B,256,N)\n",
    "        feat = torch.cat([h, g], dim=1)       # (B,512,N)\n",
    "        out = self.head(feat)                # (B,C,N)\n",
    "        return out\n",
    "\n",
    "# =========================================================\n",
    "# 4) Load checkpoint\n",
    "# =========================================================\n",
    "ckpt = torch.load(MODEL_PT, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "classes     = ckpt[\"classes\"]\n",
    "class_to_idx= ckpt[\"class_to_idx\"]\n",
    "Xmean       = ckpt[\"Xmean\"].astype(np.float32)\n",
    "Xstd        = ckpt[\"Xstd\"].astype(np.float32)\n",
    "IN_CH       = int(ckpt[\"feats\"])\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "print(\"\\n[CKPT]\")\n",
    "print(\"classes:\", classes)\n",
    "print(\"feats:\", IN_CH, \"| num_classes:\", NUM_CLASSES)\n",
    "\n",
    "model = PointNetFromCkpt(in_ch=IN_CH, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state\"], strict=True)\n",
    "model.eval()\n",
    "print(\"✅ Model loaded OK.\")\n",
    "\n",
    "idx_to_class = {i:c for c,i in class_to_idx.items()}\n",
    "\n",
    "# =========================================================\n",
    "# 5) HAG + slope inside a tile\n",
    "# =========================================================\n",
    "def hag_slope_for_tile(xyz_tile, cls_tile, cell=3.0, ground_class=2):\n",
    "    x, y, z = xyz_tile[:,0], xyz_tile[:,1], xyz_tile[:,2]\n",
    "    minx, miny = x.min(), y.min()\n",
    "    gx = np.floor((x - minx)/cell).astype(np.int32)\n",
    "    gy = np.floor((y - miny)/cell).astype(np.int32)\n",
    "\n",
    "    # ground surface (per cell) from ground points only\n",
    "    cell_min = {}\n",
    "    g_idx = np.where(cls_tile == ground_class)[0]\n",
    "    for i in g_idx:\n",
    "        k = (gx[i], gy[i])\n",
    "        zi = float(z[i])\n",
    "        if (k not in cell_min) or (zi < cell_min[k]):\n",
    "            cell_min[k] = zi\n",
    "\n",
    "    # fallback if tile has no ground (rare)\n",
    "    if len(cell_min) == 0:\n",
    "        hag = np.zeros(len(z), dtype=np.float32)\n",
    "        slope = np.zeros(len(z), dtype=np.float32)\n",
    "        has_ground = np.zeros(len(z), dtype=bool)\n",
    "        tile_ground_ref = float(np.percentile(z, 1))\n",
    "        return hag, slope, has_ground, tile_ground_ref\n",
    "\n",
    "    # HAG\n",
    "    hag = np.zeros(len(z), dtype=np.float32)\n",
    "    has_ground = np.zeros(len(z), dtype=bool)\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        if k in cell_min:\n",
    "            hag[i] = z[i] - cell_min[k]\n",
    "            has_ground[i] = True\n",
    "        else:\n",
    "            hag[i] = 0.0\n",
    "\n",
    "    # local range for slope proxy (per cell zmax-zmin)\n",
    "    cell_zmin, cell_zmax = {}, {}\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        zi = float(z[i])\n",
    "        if k not in cell_zmin:\n",
    "            cell_zmin[k] = zi\n",
    "            cell_zmax[k] = zi\n",
    "        else:\n",
    "            if zi < cell_zmin[k]: cell_zmin[k] = zi\n",
    "            if zi > cell_zmax[k]: cell_zmax[k] = zi\n",
    "\n",
    "    local_range = np.zeros(len(z), dtype=np.float32)\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        local_range[i] = float(cell_zmax[k] - cell_zmin[k])\n",
    "\n",
    "    slope = (hag / (local_range + 1e-6)).astype(np.float32)\n",
    "\n",
    "    # tile ground reference for z_local\n",
    "    tile_ground_ref = float(np.percentile(z[g_idx], 1))  # stable, similar to what your blocks show\n",
    "    return hag, slope, has_ground, tile_ground_ref\n",
    "\n",
    "# =========================================================\n",
    "# 6) Build prediction features tile-by-tile (MATCH TRAINING)\n",
    "#    Feature order:\n",
    "#    [x_local, y_local, z_local, hag, intensity, rn, nr, scan, deviation, slope]\n",
    "# =========================================================\n",
    "def predict_on_file(in_path, out_las, out_laz):\n",
    "    las = laspy.read(in_path)\n",
    "\n",
    "    xyz = np.vstack([las.x, las.y, las.z]).T.astype(np.float32)\n",
    "    cls_in = np.asarray(las.classification, dtype=np.int32)\n",
    "\n",
    "    intensity = get_dim(las, \"intensity\", fallback=0.0)\n",
    "    rn        = get_dim(las, \"return_number\", fallback=1.0)\n",
    "    nr        = get_dim(las, \"number_of_returns\", fallback=1.0)\n",
    "    scan      = get_dim(las, \"scan_angle\", fallback=0.0)\n",
    "    deviation = get_dim(las, \"Deviation\", fallback=0.0)\n",
    "\n",
    "    x = xyz[:,0]; y = xyz[:,1]\n",
    "    minx = float(x.min()); miny = float(y.min())\n",
    "\n",
    "    tx = np.floor((x - minx)/TILE_SIZE).astype(np.int32)\n",
    "    ty = np.floor((y - miny)/TILE_SIZE).astype(np.int32)\n",
    "    tile_key = tx.astype(np.int64) * 1_000_000 + ty.astype(np.int64)\n",
    "\n",
    "    order = np.argsort(tile_key)\n",
    "    tile_key_s = tile_key[order]\n",
    "    unique_tiles, start_idx = np.unique(tile_key_s, return_index=True)\n",
    "\n",
    "    pred_idx_all = np.zeros(len(xyz), dtype=np.int64)\n",
    "\n",
    "    print(\"\\nTotal tiles:\", len(unique_tiles))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in tqdm(range(len(unique_tiles)), desc=\"Predicting tiles\"):\n",
    "            a = start_idx[t]\n",
    "            b = start_idx[t+1] if t+1 < len(unique_tiles) else len(order)\n",
    "            idx = order[a:b]  # indices for this tile in original arrays\n",
    "\n",
    "            xyz_tile = xyz[idx]\n",
    "            cls_tile = cls_in[idx]\n",
    "\n",
    "            # local XY centered in tile -> range about [-150..150]\n",
    "            tile_minx = float(xyz_tile[:,0].min())\n",
    "            tile_miny = float(xyz_tile[:,1].min())\n",
    "            x_local = xyz_tile[:,0] - (tile_minx + TILE_SIZE/2.0)\n",
    "            y_local = xyz_tile[:,1] - (tile_miny + TILE_SIZE/2.0)\n",
    "\n",
    "            hag, slope, has_ground, tile_ground_ref = hag_slope_for_tile(\n",
    "                xyz_tile, cls_tile, cell=CELL, ground_class=GROUND_CLASS\n",
    "            )\n",
    "            z_local = (xyz_tile[:,2] - tile_ground_ref).astype(np.float32)\n",
    "\n",
    "            X_tile = np.stack([\n",
    "                x_local.astype(np.float32),\n",
    "                y_local.astype(np.float32),\n",
    "                z_local,\n",
    "                hag.astype(np.float32),\n",
    "                intensity[idx].astype(np.float32),\n",
    "                rn[idx].astype(np.float32),\n",
    "                nr[idx].astype(np.float32),\n",
    "                scan[idx].astype(np.float32),\n",
    "                deviation[idx].astype(np.float32),\n",
    "                slope.astype(np.float32),\n",
    "            ], axis=1).astype(np.float32)\n",
    "\n",
    "            # normalize like training\n",
    "            X_tile_n = (X_tile - Xmean[None, :]) / (Xstd[None, :] + 1e-6)\n",
    "\n",
    "            # debug once: compare prediction ranges to training ranges\n",
    "            if t == 0:\n",
    "                print(\"\\n[PRED TILE 0]\")\n",
    "                print(\"col mins:\", X_tile.min(axis=0))\n",
    "                print(\"col maxs:\", X_tile.max(axis=0))\n",
    "                print(\"norm mins:\", X_tile_n.min(axis=0))\n",
    "                print(\"norm maxs:\", X_tile_n.max(axis=0))\n",
    "\n",
    "            # predict in blocks\n",
    "            pred_idx_tile = np.zeros(len(idx), dtype=np.int64)\n",
    "\n",
    "            for s in range(0, len(idx), BLOCK):\n",
    "                e = min(s + BLOCK, len(idx))\n",
    "                block = X_tile_n[s:e]\n",
    "                inp = torch.from_numpy(block).to(DEVICE).unsqueeze(0).permute(0,2,1)  # (1,F,N)\n",
    "                logits = model(inp)  # (1,C,N)\n",
    "                p = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()\n",
    "                pred_idx_tile[s:e] = p[:e-s]\n",
    "\n",
    "            pred_idx_all[idx] = pred_idx_tile\n",
    "\n",
    "    # map to LAS class codes\n",
    "    pred_class = np.vectorize(idx_to_class.get)(pred_idx_all).astype(np.uint8)\n",
    "    las.classification = pred_class\n",
    "\n",
    "    las.write(out_las)\n",
    "    print(\"Saved LAS:\", out_las)\n",
    "    try:\n",
    "        las.write(out_laz)\n",
    "        print(\"Saved LAZ:\", out_laz)\n",
    "    except Exception as e:\n",
    "        print(\"LAZ write failed (ok):\", e)\n",
    "\n",
    "    u, c = np.unique(pred_class, return_counts=True)\n",
    "    print(\"Pred class counts:\", dict(zip(u.tolist(), c.tolist())))\n",
    "\n",
    "# =========================================================\n",
    "# RUN\n",
    "# =========================================================\n",
    "predict_on_file(IN_LAS_OR_LAZ, OUT_LAS, OUT_LAZ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40c10b27-b953-4643-ae06-48dfe9d1071d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CKPT classes: [1, 2, 3, 6, 7, 12, 13]\n",
      "✅ feats: 10 num_classes: 7\n",
      "✅ Model loaded successfully (STRICT).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "MODEL_PT = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"\n",
    "ckpt = torch.load(MODEL_PT, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "classes = ckpt[\"classes\"]\n",
    "IN_CH = int(ckpt[\"feats\"])\n",
    "NUM_CLASSES = len(classes)\n",
    "state = ckpt[\"model_state\"]\n",
    "\n",
    "print(\"✅ CKPT classes:\", classes)\n",
    "print(\"✅ feats:\", IN_CH, \"num_classes:\", NUM_CLASSES)\n",
    "\n",
    "class PointNetFromCkptFixed(nn.Module):\n",
    "    \"\"\"\n",
    "    Matches ckpt:\n",
    "    mlp1: Conv(in->64), BN, ReLU, Conv(64->128), BN, ReLU\n",
    "    mlp2: Conv(128->256), BN, ReLU\n",
    "    head: Conv(512->256), BN, ReLU, Dropout, Conv(256->C)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=10, num_classes=7, p_drop=0.3):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 64, 1),        # mlp1.0.*\n",
    "            nn.BatchNorm1d(64),             # mlp1.1.*\n",
    "            nn.ReLU(inplace=True),          # mlp1.2\n",
    "            nn.Conv1d(64, 128, 1),          # mlp1.3.*  (THIS is [128,64,1])\n",
    "            nn.BatchNorm1d(128),            # mlp1.4.*\n",
    "            nn.ReLU(inplace=True),          # mlp1.5\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 1),         # mlp2.0.*\n",
    "            nn.BatchNorm1d(256),            # mlp2.1.*\n",
    "            nn.ReLU(inplace=True),          # mlp2.2\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(512, 256, 1),         # head.0.*\n",
    "            nn.BatchNorm1d(256),            # head.1.*\n",
    "            nn.ReLU(inplace=True),          # head.2\n",
    "            nn.Dropout(p_drop),             # head.3  (no weights)\n",
    "            nn.Conv1d(256, num_classes, 1)  # head.4.*  (matches your ckpt)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, F, N)\n",
    "        h = self.mlp1(x)                    # (B,128,N)\n",
    "        h = self.mlp2(h)                    # (B,256,N)\n",
    "        g = torch.max(h, dim=2, keepdim=True)[0]  # (B,256,1)\n",
    "        g = g.repeat(1, 1, h.shape[2])            # (B,256,N)\n",
    "        feat = torch.cat([h, g], dim=1)           # (B,512,N)\n",
    "        out = self.head(feat)                      # (B,C,N)\n",
    "        return out\n",
    "\n",
    "model = PointNetFromCkptFixed(in_ch=IN_CH, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()\n",
    "print(\"✅ Model loaded successfully (STRICT).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "557a0481-bf28-432b-a9d0-f8e10963b91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mlp1.0.weight', 'mlp1.0.bias', 'mlp1.1.weight', 'mlp1.1.bias', 'mlp1.1.running_mean', 'mlp1.1.running_var', 'mlp1.1.num_batches_tracked', 'mlp1.3.weight', 'mlp1.3.bias', 'mlp1.4.weight', 'mlp1.4.bias', 'mlp1.4.running_mean', 'mlp1.4.running_var', 'mlp1.4.num_batches_tracked']\n",
      "['head.0.weight', 'head.0.bias', 'head.1.weight', 'head.1.bias', 'head.1.running_mean', 'head.1.running_var', 'head.1.num_batches_tracked', 'head.4.weight', 'head.4.bias']\n",
      "torch.Size([128, 64, 1])\n"
     ]
    }
   ],
   "source": [
    "print([k for k in state.keys() if k.startswith(\"mlp1\")][:20])\n",
    "print([k for k in state.keys() if k.startswith(\"head\")][:20])\n",
    "print(state[\"mlp1.3.weight\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31576b37-72d2-46e6-899c-1d302de2a423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n",
      "\n",
      "Loaded CKPT classes: [1, 2, 3, 6, 7, 12, 13]\n",
      "Input feats: 10 Num classes: 7\n",
      "✅ Model loaded (strict).\n",
      "\n",
      "Xpred: (12374846, 10) min/max: -577.5 65535.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac6369bb40b4633b850c6aea06688e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting blocks:   0%|          | 0/3022 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved LAS: D:/lidarrrrr/anbu/New folder/dl_predicted_fixed_final.las\n",
      "Saved LAZ: D:/lidarrrrr/anbu/New folder/dl_predicted_fixed_final.laz\n",
      "Pred class counts: {1: 4259309, 2: 6828365, 3: 1287172}\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import laspy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ==========================\n",
    "# PATHS (EDIT)\n",
    "# ==========================\n",
    "IN_LAS_OR_LAZ = r\"D:/lidarrrrr/anbu/New folder/stage1_outputs/DX3035724_stage1_ground_v2.las\"\n",
    "MODEL_PT      = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"\n",
    "OUT_LAS       = r\"D:/lidarrrrr/anbu/New folder/dl_predicted_fixed_final.las\"\n",
    "OUT_LAZ       = r\"D:/lidarrrrr/anbu/New folder/dl_predicted_fixed_final.laz\"\n",
    "\n",
    "# ==========================\n",
    "# SETTINGS (must match training)\n",
    "# ==========================\n",
    "CELL  = 3.0\n",
    "BLOCK = 4096\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# ==========================\n",
    "# Helpers\n",
    "# ==========================\n",
    "def get_dim(las, name, fallback=0.0, dtype=np.float32):\n",
    "    \"\"\"Robust getter for LAS dimensions (works even if some fields missing).\"\"\"\n",
    "    try:\n",
    "        return np.asarray(las[name]).astype(dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if name == \"scan_angle\":\n",
    "        for alt in [\"scan_angle\", \"scan_angle_rank\"]:\n",
    "            try:\n",
    "                return np.asarray(las[alt]).astype(dtype)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # RGB may not exist\n",
    "    if name in [\"red\", \"green\", \"blue\"]:\n",
    "        return np.full(len(las.x), fallback, dtype=dtype)\n",
    "\n",
    "    # ExtraBytes may not exist\n",
    "    return np.full(len(las.x), fallback, dtype=dtype)\n",
    "\n",
    "def compute_hag_and_slope(xyz, cls, cell=3.0, ground_class=2):\n",
    "    \"\"\"HAG and slope computed from ground (class 2) surface per grid cell.\"\"\"\n",
    "    x, y, z = xyz[:,0], xyz[:,1], xyz[:,2]\n",
    "    minx, miny = x.min(), y.min()\n",
    "    gx = np.floor((x - minx)/cell).astype(np.int32)\n",
    "    gy = np.floor((y - miny)/cell).astype(np.int32)\n",
    "\n",
    "    # ground surface per cell from class=2\n",
    "    cell_min = {}\n",
    "    g_idx = np.where(cls == ground_class)[0]\n",
    "    for i in g_idx:\n",
    "        k = (gx[i], gy[i])\n",
    "        zi = float(z[i])\n",
    "        if (k not in cell_min) or (zi < cell_min[k]):\n",
    "            cell_min[k] = zi\n",
    "\n",
    "    hag = np.zeros(len(z), dtype=np.float32)\n",
    "    has_ground = np.zeros(len(z), dtype=bool)\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        if k in cell_min:\n",
    "            hag[i] = z[i] - cell_min[k]\n",
    "            has_ground[i] = True\n",
    "        else:\n",
    "            hag[i] = 0.0\n",
    "\n",
    "    # local range per cell for slope proxy\n",
    "    cell_zmin, cell_zmax = {}, {}\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        zi = float(z[i])\n",
    "        if k not in cell_zmin:\n",
    "            cell_zmin[k] = zi\n",
    "            cell_zmax[k] = zi\n",
    "        else:\n",
    "            if zi < cell_zmin[k]: cell_zmin[k] = zi\n",
    "            if zi > cell_zmax[k]: cell_zmax[k] = zi\n",
    "\n",
    "    local_range = np.zeros(len(z), dtype=np.float32)\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        local_range[i] = float(cell_zmax[k] - cell_zmin[k])\n",
    "\n",
    "    slope = (hag / (local_range + 1e-6)).astype(np.float32)\n",
    "    return hag, slope, has_ground\n",
    "\n",
    "def make_Xpred(las, cell=3.0):\n",
    "    \"\"\"\n",
    "    IMPORTANT: feature order MUST match training.\n",
    "    We'll use this 10-feature order:\n",
    "\n",
    "    [0] x_rel\n",
    "    [1] y_rel\n",
    "    [2] z\n",
    "    [3] hag\n",
    "    [4] intensity\n",
    "    [5] return_number\n",
    "    [6] number_of_returns\n",
    "    [7] scan_angle\n",
    "    [8] deviation (ExtraBytes 'Deviation', else 0)\n",
    "    [9] slope\n",
    "    \"\"\"\n",
    "    xyz = np.vstack([las.x, las.y, las.z]).T.astype(np.float32)\n",
    "    cls = np.asarray(las.classification, dtype=np.int32)\n",
    "\n",
    "    # shift x/y like training blocks did (relative coords improve generalization)\n",
    "    x_rel = (xyz[:,0] - xyz[:,0].mean()).astype(np.float32)\n",
    "    y_rel = (xyz[:,1] - xyz[:,1].mean()).astype(np.float32)\n",
    "    z     = xyz[:,2].astype(np.float32)\n",
    "\n",
    "    intensity = get_dim(las, \"intensity\", fallback=0.0, dtype=np.float32)\n",
    "    rn        = get_dim(las, \"return_number\", fallback=1.0, dtype=np.float32)\n",
    "    nr        = get_dim(las, \"number_of_returns\", fallback=1.0, dtype=np.float32)\n",
    "    scan_ang  = get_dim(las, \"scan_angle\", fallback=0.0, dtype=np.float32)\n",
    "    deviation = get_dim(las, \"Deviation\", fallback=0.0, dtype=np.float32)\n",
    "\n",
    "    hag, slope, has_ground = compute_hag_and_slope(xyz, cls, cell=cell)\n",
    "\n",
    "    X = np.stack([\n",
    "        x_rel, y_rel, z,\n",
    "        hag,\n",
    "        intensity,\n",
    "        rn,\n",
    "        nr,\n",
    "        scan_ang,\n",
    "        deviation,\n",
    "        slope\n",
    "    ], axis=1).astype(np.float32)\n",
    "\n",
    "    return X, xyz, cls, has_ground\n",
    "\n",
    "# ==========================\n",
    "# Model: matches your ckpt exactly\n",
    "# ==========================\n",
    "class PointNetFromCkptFixed(nn.Module):\n",
    "    \"\"\"\n",
    "    mlp1: Conv(in->64), BN, ReLU, Conv(64->128), BN, ReLU\n",
    "    mlp2: Conv(128->256), BN, ReLU\n",
    "    head: Conv(512->256), BN, ReLU, Dropout, Conv(256->C)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=10, num_classes=7, p_drop=0.3):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 64, 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(64, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(512, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Conv1d(256, num_classes, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.mlp1(x)                               # (B,128,N)\n",
    "        h = self.mlp2(h)                               # (B,256,N)\n",
    "        g = torch.max(h, dim=2, keepdim=True)[0]        # (B,256,1)\n",
    "        g = g.repeat(1, 1, h.shape[2])                  # (B,256,N)\n",
    "        feat = torch.cat([h, g], dim=1)                 # (B,512,N)\n",
    "        out = self.head(feat)                           # (B,C,N)\n",
    "        return out\n",
    "\n",
    "# ==========================\n",
    "# Load checkpoint\n",
    "# ==========================\n",
    "ckpt = torch.load(MODEL_PT, map_location=DEVICE, weights_only=False)\n",
    "classes     = ckpt[\"classes\"]\n",
    "class_to_idx= ckpt[\"class_to_idx\"]\n",
    "Xmean       = ckpt[\"Xmean\"].astype(np.float32)\n",
    "Xstd        = ckpt[\"Xstd\"].astype(np.float32)\n",
    "IN_CH       = int(ckpt[\"feats\"])\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "print(\"\\nLoaded CKPT classes:\", classes)\n",
    "print(\"Input feats:\", IN_CH, \"Num classes:\", NUM_CLASSES)\n",
    "\n",
    "model = PointNetFromCkptFixed(in_ch=IN_CH, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state\"], strict=True)\n",
    "model.eval()\n",
    "print(\"✅ Model loaded (strict).\")\n",
    "\n",
    "idx_to_class = {i:c for c,i in class_to_idx.items()}\n",
    "\n",
    "# ==========================\n",
    "# Build features for input LAS/LAZ\n",
    "# ==========================\n",
    "las = laspy.read(IN_LAS_OR_LAZ)\n",
    "Xpred, xyz, cls_in, has_ground = make_Xpred(las, cell=CELL)\n",
    "print(\"\\nXpred:\", Xpred.shape, \"min/max:\", float(Xpred.min()), float(Xpred.max()))\n",
    "\n",
    "# Normalize exactly like training\n",
    "Xpred_n = (Xpred - Xmean[None, :]) / (Xstd[None, :] + 1e-6)\n",
    "\n",
    "# ==========================\n",
    "# Predict in blocks\n",
    "# ==========================\n",
    "pred_idx = np.zeros((len(Xpred_n),), dtype=np.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for start in tqdm(range(0, len(Xpred_n), BLOCK), desc=\"Predicting blocks\"):\n",
    "        end = min(start + BLOCK, len(Xpred_n))\n",
    "        xb = Xpred_n[start:end]\n",
    "\n",
    "        inp = torch.from_numpy(xb).to(DEVICE)          # (N,F)\n",
    "        inp = inp.unsqueeze(0).permute(0, 2, 1)        # (1,F,N)\n",
    "\n",
    "        logits = model(inp)                            # (1,C,N)\n",
    "        p = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()\n",
    "        pred_idx[start:end] = p[:end-start]\n",
    "\n",
    "pred_class = np.vectorize(idx_to_class.get)(pred_idx).astype(np.uint8)\n",
    "\n",
    "# Optional safety: if no ground surface in that cell, force DEFAULT class 1\n",
    "pred_class[~has_ground] = 1\n",
    "\n",
    "las.classification = pred_class\n",
    "\n",
    "# ==========================\n",
    "# Save\n",
    "# ==========================\n",
    "os.makedirs(os.path.dirname(OUT_LAS), exist_ok=True)\n",
    "las.write(OUT_LAS)\n",
    "print(\"\\nSaved LAS:\", OUT_LAS)\n",
    "try:\n",
    "    las.write(OUT_LAZ)\n",
    "    print(\"Saved LAZ:\", OUT_LAZ)\n",
    "except Exception as e:\n",
    "    print(\"LAZ write failed (ok):\", e)\n",
    "\n",
    "u, c = np.unique(pred_class, return_counts=True)\n",
    "print(\"Pred class counts:\", dict(zip(u.tolist(), c.tolist())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4f351da-c0d0-41cf-8125-4db0c5048f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TRAIN] normalized stats per column:\n",
      "  mean: [-0.007 -0.191 -0.001 -0.1   -0.174 -0.024 -0.023  0.157 -0.183 -0.007]\n",
      "  std : [0.212 0.403 0.126 0.449 0.788 1.012 1.015 0.028 0.    0.621]\n",
      "  min : [-0.471 -1.069 -0.232 -0.325 -5.348 -0.2   -0.228  0.092 -0.183 -0.575]\n",
      "  max : [ 0.49   0.483  2.869 11.021  1.146 14.989 12.836  0.216 -0.183 10.458]\n",
      "\n",
      "[PRED (first 200k)] normalized stats per column:\n",
      "  mean: [-0.521 -2.518  0.488 -0.218  0.419 -0.197 -0.225  0.144 -0.183 -0.35 ]\n",
      "  std : [0.033 0.052 0.045 0.133 0.186 0.091 0.105 0.02  0.    0.123]\n",
      "  min : [-0.617 -2.62   0.371 -0.325 -5.348 -0.2   -0.228  0.114 -0.183 -0.601]\n",
      "  max : [-0.474 -2.377  1.999  5.999  1.348  3.597  3.038  0.204 -0.183  0.062]\n"
     ]
    }
   ],
   "source": [
    "# After you have Xtr (from one training .npz) and Xpred (from LAS) and ckpt Xmean/Xstd:\n",
    "\n",
    "def norm_stats(X, Xmean, Xstd, name):\n",
    "    Xn = (X - Xmean[None,:]) / (Xstd[None,:] + 1e-6)\n",
    "    print(f\"\\n[{name}] normalized stats per column:\")\n",
    "    print(\"  mean:\", np.round(Xn.mean(axis=0), 3))\n",
    "    print(\"  std :\", np.round(Xn.std(axis=0), 3))\n",
    "    print(\"  min :\", np.round(Xn.min(axis=0), 3))\n",
    "    print(\"  max :\", np.round(Xn.max(axis=0), 3))\n",
    "\n",
    "norm_stats(Xtr, Xmean, Xstd, \"TRAIN\")\n",
    "norm_stats(Xpred[:200000], Xmean, Xstd, \"PRED (first 200k)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df7382a0-57b9-4453-b201-526ad4be7cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feats per col mean: [-2.2071991e+00 -1.8538818e+00 -3.7940481e-04  2.1671166e-01\n",
      "  3.5122293e+04  1.0463867e+00  1.0629883e+00  5.8276367e-01\n",
      "  0.0000000e+00  8.9575773e-01]\n",
      "feats per col std : [6.2053955e+01 9.5265366e+01 4.6353060e-01 4.3087605e-01 5.3501665e+03\n",
      " 2.6663089e-01 3.1083465e-01 2.5165798e+01 0.0000000e+00 9.3632799e-01]\n",
      "first row: [ 6.1937500e+01 -1.2750000e+02 -3.8357973e-02  5.9999943e-02\n",
      "  3.4995000e+04  1.0000000e+00  1.0000000e+00  2.0000000e+01\n",
      "  0.0000000e+00  1.0999999e+00]\n"
     ]
    }
   ],
   "source": [
    "d = np.load(train_files[0])\n",
    "print(\"feats per col mean:\", d[\"X\"].mean(axis=0))\n",
    "print(\"feats per col std :\", d[\"X\"].std(axis=0))\n",
    "print(\"first row:\", d[\"X\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "208ee7c8-fcb2-49d7-bc66-efa0ae3808f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n",
      "\n",
      "[TRAIN BLOCK]\n",
      "keys: ['X', 'y', 'idx']\n",
      "X shape: (4096, 10)\n",
      "col means: [-2.2071991e+00 -1.8538818e+00 -3.7940481e-04  2.1671166e-01\n",
      "  3.5122293e+04  1.0463867e+00  1.0629883e+00  5.8276367e-01\n",
      "  0.0000000e+00  8.9575773e-01]\n",
      "col stds : [6.2053955e+01 9.5265366e+01 4.6353060e-01 4.3087605e-01 5.3501665e+03\n",
      " 2.6663089e-01 3.1083465e-01 2.5165798e+01 0.0000000e+00 9.3632799e-01]\n",
      "\n",
      "[CKPT]\n",
      "classes: [1, 2, 3, 6, 7, 12, 13]\n",
      "feats: 10 | num_classes: 7\n",
      "✅ Model loaded (strict).\n",
      "\n",
      "Total points: 12374846 | tiles: 120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea745aa284f84a77b6e84f18913dd64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting tiles:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved LAS: D:/lidarrrrr/anbu/New folder/dl_predicted_matched.las\n",
      "Saved LAZ: D:/lidarrrrr/anbu/New folder/dl_predicted_matched.laz\n",
      "Pred class counts: {1: 2471478, 2: 9592233, 3: 8518, 6: 302368, 12: 249}\n",
      "\n",
      "[PRED normalized quick stats first 200k]\n",
      "mean: [ 0.    -0.183 -0.001 -0.218  0.419 -0.197 -0.225  0.144 -0.183 -0.35 ]\n",
      "std : [0.015 0.036 0.041 0.133 0.186 0.091 0.105 0.02  0.    0.123]\n",
      "min : [-0.046 -0.268 -0.104 -0.325 -5.348 -0.2   -0.228  0.114 -0.183 -0.601]\n",
      "max : [ 0.049 -0.113  1.523  5.999  1.348  3.597  3.038  0.204 -0.183  0.062]\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import laspy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# =========================================================\n",
    "# EDIT PATHS\n",
    "# =========================================================\n",
    "TRAIN_BLOCK_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"     # has .npz with keys X,y,idx\n",
    "IN_LAS_OR_LAZ   = r\"D:/lidarrrrr/anbu/New folder/stage1_outputs/DX3035724_stage1_ground_v2.las\"\n",
    "MODEL_PT        = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"\n",
    "OUT_LAS         = r\"D:/lidarrrrr/anbu/New folder/dl_predicted_matched.las\"\n",
    "OUT_LAZ         = r\"D:/lidarrrrr/anbu/New folder/dl_predicted_matched.laz\"\n",
    "\n",
    "# =========================================================\n",
    "# SETTINGS (must match training)\n",
    "# =========================================================\n",
    "POINTS_PER_BLOCK = 4096         # N\n",
    "TILE_SIZE_M      = 40.0         # IMPORTANT: set to the SAME tile size used while creating blocks\n",
    "CELL_M           = 3.0          # HAG grid cell (same as before)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================================================\n",
    "# 1) LOAD ONE TRAIN BLOCK (just for sanity prints)\n",
    "# =========================================================\n",
    "train_files = sorted(glob.glob(os.path.join(TRAIN_BLOCK_DIR, \"*.npz\")))\n",
    "if not train_files:\n",
    "    raise RuntimeError(\"No training .npz found in TRAIN_BLOCK_DIR\")\n",
    "\n",
    "d = np.load(train_files[0])\n",
    "Xtr = d[\"X\"].astype(np.float32)\n",
    "print(\"\\n[TRAIN BLOCK]\")\n",
    "print(\"keys:\", list(d.keys()))\n",
    "print(\"X shape:\", Xtr.shape)\n",
    "print(\"col means:\", Xtr.mean(axis=0))\n",
    "print(\"col stds :\", Xtr.std(axis=0))\n",
    "\n",
    "# =========================================================\n",
    "# 2) Model definition that matches your checkpoint keys\n",
    "#    Your keys show:\n",
    "#    mlp1.0 Conv(in->64), mlp1.1 BN64, mlp1.2 ReLU,\n",
    "#    mlp1.3 Conv(64->128), mlp1.4 BN128, mlp1.5 ReLU\n",
    "#    mlp2.0 Conv(128->256), mlp2.1 BN256, mlp2.2 ReLU\n",
    "#    head.0 Conv(512->256), head.1 BN256, head.2 ReLU, head.3 (maybe Dropout), head.4 Conv(256->C)\n",
    "# =========================================================\n",
    "class PointNetFromCkpt(nn.Module):\n",
    "    def __init__(self, in_ch=10, num_classes=7, p_drop=0.0):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 64, 1),     # 0\n",
    "            nn.BatchNorm1d(64),          # 1\n",
    "            nn.ReLU(inplace=True),       # 2\n",
    "            nn.Conv1d(64, 128, 1),       # 3\n",
    "            nn.BatchNorm1d(128),         # 4\n",
    "            nn.ReLU(inplace=True),       # 5\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 1),      # 0\n",
    "            nn.BatchNorm1d(256),         # 1\n",
    "            nn.ReLU(inplace=True),       # 2\n",
    "        )\n",
    "        # Keep index 3 so that \"head.4.*\" matches\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(512, 256, 1),      # 0\n",
    "            nn.BatchNorm1d(256),         # 1\n",
    "            nn.ReLU(inplace=True),       # 2\n",
    "            nn.Dropout(p_drop),          # 3 (exists even if p_drop=0)\n",
    "            nn.Conv1d(256, num_classes, 1),  # 4\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,F,N)\n",
    "        h = self.mlp1(x)                         # (B,128,N)\n",
    "        h = self.mlp2(h)                         # (B,256,N)\n",
    "        g = torch.max(h, dim=2, keepdim=True)[0] # (B,256,1)\n",
    "        g = g.repeat(1, 1, h.shape[2])           # (B,256,N)\n",
    "        feat = torch.cat([h, g], dim=1)          # (B,512,N)\n",
    "        out = self.head(feat)                    # (B,C,N)\n",
    "        return out\n",
    "\n",
    "# =========================================================\n",
    "# 3) Load checkpoint (PyTorch 2.6+ safe)\n",
    "# =========================================================\n",
    "ckpt = torch.load(MODEL_PT, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "classes     = ckpt[\"classes\"]          # e.g. [1,2,3,6,7,12,13]\n",
    "class_to_idx= ckpt[\"class_to_idx\"]\n",
    "Xmean       = ckpt[\"Xmean\"].astype(np.float32)\n",
    "Xstd        = ckpt[\"Xstd\"].astype(np.float32)\n",
    "IN_CH       = int(ckpt[\"feats\"])\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "print(\"\\n[CKPT]\")\n",
    "print(\"classes:\", classes)\n",
    "print(\"feats:\", IN_CH, \"| num_classes:\", NUM_CLASSES)\n",
    "\n",
    "model = PointNetFromCkpt(in_ch=IN_CH, num_classes=NUM_CLASSES, p_drop=0.0).to(DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state\"], strict=True)\n",
    "model.eval()\n",
    "print(\"✅ Model loaded (strict).\")\n",
    "\n",
    "idx_to_class = {i:c for c,i in class_to_idx.items()}\n",
    "\n",
    "# =========================================================\n",
    "# 4) Robust LAS dimension getter\n",
    "# =========================================================\n",
    "def get_dim(las, name, fallback=0.0, dtype=np.float32):\n",
    "    try:\n",
    "        return np.asarray(las[name]).astype(dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if name == \"scan_angle\":\n",
    "        for alt in [\"scan_angle\", \"scan_angle_rank\"]:\n",
    "            try:\n",
    "                return np.asarray(las[alt]).astype(dtype)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return np.full(len(las.x), fallback, dtype=dtype)\n",
    "\n",
    "# =========================================================\n",
    "# 5) HAG + slope (same idea as before)\n",
    "# =========================================================\n",
    "def compute_hag_and_slope(x, y, z, cls, cell=CELL_M, ground_class=2):\n",
    "    minx, miny = x.min(), y.min()\n",
    "    gx = np.floor((x - minx) / cell).astype(np.int32)\n",
    "    gy = np.floor((y - miny) / cell).astype(np.int32)\n",
    "\n",
    "    # ground surface per cell\n",
    "    cell_min = {}\n",
    "    g_idx = np.where(cls == ground_class)[0]\n",
    "    for i in g_idx:\n",
    "        k = (gx[i], gy[i])\n",
    "        zi = float(z[i])\n",
    "        if (k not in cell_min) or (zi < cell_min[k]):\n",
    "            cell_min[k] = zi\n",
    "\n",
    "    hag = np.zeros(len(z), dtype=np.float32)\n",
    "    has_ground = np.zeros(len(z), dtype=bool)\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        if k in cell_min:\n",
    "            hag[i] = z[i] - cell_min[k]\n",
    "            has_ground[i] = True\n",
    "        else:\n",
    "            hag[i] = 0.0\n",
    "\n",
    "    # local z-range proxy per cell\n",
    "    cell_zmin, cell_zmax = {}, {}\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        zi = float(z[i])\n",
    "        if k not in cell_zmin:\n",
    "            cell_zmin[k] = zi\n",
    "            cell_zmax[k] = zi\n",
    "        else:\n",
    "            if zi < cell_zmin[k]: cell_zmin[k] = zi\n",
    "            if zi > cell_zmax[k]: cell_zmax[k] = zi\n",
    "\n",
    "    local_range = np.zeros(len(z), dtype=np.float32)\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        local_range[i] = float(cell_zmax[k] - cell_zmin[k])\n",
    "\n",
    "    slope = (hag / (local_range + 1e-6)).astype(np.float32)\n",
    "    return hag, slope, has_ground\n",
    "\n",
    "# =========================================================\n",
    "# 6) Build tile index (to create LOCAL x,y,z like training)\n",
    "# =========================================================\n",
    "def build_tile_ids(x, y, tile_size=TILE_SIZE_M):\n",
    "    minx, miny = x.min(), y.min()\n",
    "    tx = np.floor((x - minx) / tile_size).astype(np.int32)\n",
    "    ty = np.floor((y - miny) / tile_size).astype(np.int32)\n",
    "    # combine into one id\n",
    "    # (use large multiplier to avoid collisions)\n",
    "    tile_id = tx.astype(np.int64) * 1_000_000 + ty.astype(np.int64)\n",
    "    return tile_id\n",
    "\n",
    "# =========================================================\n",
    "# 7) Predict full LAS by processing each TILE in chunks of 4096\n",
    "#     Features order MUST match training:\n",
    "#     [x_local, y_local, z_local, hag, intensity, ret_num, n_returns, scan_angle, 0, slope]\n",
    "# =========================================================\n",
    "las = laspy.read(IN_LAS_OR_LAZ)\n",
    "\n",
    "x = np.asarray(las.x, dtype=np.float32)\n",
    "y = np.asarray(las.y, dtype=np.float32)\n",
    "z = np.asarray(las.z, dtype=np.float32)\n",
    "cls_in = np.asarray(las.classification, dtype=np.int32)\n",
    "\n",
    "intensity = get_dim(las, \"intensity\", fallback=0.0, dtype=np.float32)\n",
    "ret_num   = get_dim(las, \"return_number\", fallback=1.0, dtype=np.float32)\n",
    "n_returns = get_dim(las, \"number_of_returns\", fallback=1.0, dtype=np.float32)\n",
    "scan_ang  = get_dim(las, \"scan_angle\", fallback=0.0, dtype=np.float32)\n",
    "\n",
    "# col8 is always 0 in training\n",
    "zeros_col = np.zeros_like(x, dtype=np.float32)\n",
    "\n",
    "hag, slope, has_ground = compute_hag_and_slope(x, y, z, cls_in, cell=CELL_M, ground_class=2)\n",
    "\n",
    "tile_id = build_tile_ids(x, y, tile_size=TILE_SIZE_M)\n",
    "unique_tiles = np.unique(tile_id)\n",
    "\n",
    "pred_idx_all = np.zeros((len(x),), dtype=np.int64)\n",
    "\n",
    "print(\"\\nTotal points:\", len(x), \"| tiles:\", len(unique_tiles))\n",
    "\n",
    "with torch.no_grad():\n",
    "    pbar_tiles = tqdm(unique_tiles, desc=\"Predicting tiles\")\n",
    "    for tid in pbar_tiles:\n",
    "        inds = np.where(tile_id == tid)[0]\n",
    "        if len(inds) == 0:\n",
    "            continue\n",
    "\n",
    "        # TILE center (local coords)\n",
    "        xm = float(x[inds].mean())\n",
    "        ym = float(y[inds].mean())\n",
    "        zm = float(z[inds].mean())\n",
    "\n",
    "        # process this tile in chunks of 4096\n",
    "        for start in range(0, len(inds), POINTS_PER_BLOCK):\n",
    "            part = inds[start:start + POINTS_PER_BLOCK]\n",
    "\n",
    "            # pad to 4096 if needed\n",
    "            if len(part) < POINTS_PER_BLOCK:\n",
    "                pad = np.random.choice(part, size=(POINTS_PER_BLOCK - len(part)), replace=True)\n",
    "                part_full = np.concatenate([part, pad])\n",
    "                valid_len = len(part)\n",
    "            else:\n",
    "                part_full = part\n",
    "                valid_len = len(part)\n",
    "\n",
    "            # ====== build X block in TRAIN ORDER ======\n",
    "            x_local = x[part_full] - xm\n",
    "            y_local = y[part_full] - ym\n",
    "            z_local = z[part_full] - zm\n",
    "\n",
    "            Xblk = np.stack([\n",
    "                x_local,                 # col0\n",
    "                y_local,                 # col1\n",
    "                z_local,                 # col2\n",
    "                hag[part_full],          # col3\n",
    "                intensity[part_full],    # col4\n",
    "                ret_num[part_full],      # col5\n",
    "                n_returns[part_full],    # col6\n",
    "                scan_ang[part_full],     # col7\n",
    "                zeros_col[part_full],    # col8\n",
    "                slope[part_full],        # col9\n",
    "            ], axis=1).astype(np.float32)\n",
    "\n",
    "            # normalize EXACTLY like training\n",
    "            Xblk_n = (Xblk - Xmean[None, :]) / (Xstd[None, :] + 1e-6)\n",
    "\n",
    "            inp = torch.from_numpy(Xblk_n).to(DEVICE).unsqueeze(0).permute(0, 2, 1)  # (1,F,N)\n",
    "            logits = model(inp)                       # (1,C,N)\n",
    "            p = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()  # (N,)\n",
    "\n",
    "            pred_idx_all[part] = p[:valid_len]\n",
    "\n",
    "# map back to LAS class codes\n",
    "pred_class = np.vectorize(idx_to_class.get)(pred_idx_all).astype(np.uint8)\n",
    "\n",
    "las.classification = pred_class\n",
    "las.write(OUT_LAS)\n",
    "print(\"\\nSaved LAS:\", OUT_LAS)\n",
    "try:\n",
    "    las.write(OUT_LAZ)\n",
    "    print(\"Saved LAZ:\", OUT_LAZ)\n",
    "except Exception as e:\n",
    "    print(\"LAZ write failed (ok):\", e)\n",
    "\n",
    "u, c = np.unique(pred_class, return_counts=True)\n",
    "print(\"Pred class counts:\", dict(zip(u.tolist(), c.tolist())))\n",
    "\n",
    "# sanity check: compare normalized stats for first 200k points\n",
    "Ncheck = min(200_000, len(x))\n",
    "# rebuild exactly same X for those points (tile-local)\n",
    "inds0 = np.arange(Ncheck)\n",
    "# compute local per-point using its tile mean (vectorized)\n",
    "# (fast approx: use per-tile mean via dict)\n",
    "means = {}\n",
    "for tid in np.unique(tile_id[inds0]):\n",
    "    ii = inds0[tile_id[inds0] == tid]\n",
    "    means[tid] = (float(x[ii].mean()), float(y[ii].mean()), float(z[ii].mean()))\n",
    "xm0 = np.array([means[t][0] for t in tile_id[inds0]], dtype=np.float32)\n",
    "ym0 = np.array([means[t][1] for t in tile_id[inds0]], dtype=np.float32)\n",
    "zm0 = np.array([means[t][2] for t in tile_id[inds0]], dtype=np.float32)\n",
    "\n",
    "X0 = np.stack([\n",
    "    x[inds0]-xm0, y[inds0]-ym0, z[inds0]-zm0, hag[inds0], intensity[inds0],\n",
    "    ret_num[inds0], n_returns[inds0], scan_ang[inds0], zeros_col[inds0], slope[inds0]\n",
    "], axis=1).astype(np.float32)\n",
    "X0n = (X0 - Xmean[None,:]) / (Xstd[None,:] + 1e-6)\n",
    "\n",
    "print(\"\\n[PRED normalized quick stats first 200k]\")\n",
    "print(\"mean:\", np.round(X0n.mean(axis=0), 3))\n",
    "print(\"std :\", np.round(X0n.std(axis=0), 3))\n",
    "print(\"min :\", np.round(X0n.min(axis=0), 3))\n",
    "print(\"max :\", np.round(X0n.max(axis=0), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27ae84-928c-4ac9-abcd-a816fe43c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.load(train_files[0])\n",
    "print(\"X first row:\", d[\"X\"][0])\n",
    "print(\"X mean:\", d[\"X\"].mean(axis=0))\n",
    "print(\"X std :\", d[\"X\"].std(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12052164-e7e9-42f4-82a2-0d65a8bd381a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xmean: [ 2.4770452e-01  4.4691383e+01 -1.3224035e-05  3.2253304e-01\n",
      "  3.6415633e+04  1.0537578e+00  1.0711324e+00 -1.5879369e+02\n",
      "  8.8634253e-01  9.2550790e-01]\n",
      "Xstd : [2.9587668e+02 2.3407706e+02 3.8826628e+00 1.0009346e+00 6.9030566e+03\n",
      " 2.6688504e-01 3.0669588e-01 9.5657166e+02 4.5917196e+00 1.5607965e+00]\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "\n",
    "BLOCK_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "files = sorted(glob.glob(os.path.join(BLOCK_DIR, \"*.npz\")))\n",
    "assert files, \"No blocks found\"\n",
    "\n",
    "# Welford streaming mean/std\n",
    "n = 0\n",
    "mean = None\n",
    "M2 = None\n",
    "\n",
    "for fp in files:\n",
    "    d = np.load(fp)\n",
    "    X = d[\"X\"].astype(np.float64)  # (4096,10)\n",
    "    if mean is None:\n",
    "        mean = np.zeros(X.shape[1], dtype=np.float64)\n",
    "        M2   = np.zeros(X.shape[1], dtype=np.float64)\n",
    "\n",
    "    n_batch = X.shape[0]\n",
    "    n_new = n + n_batch\n",
    "\n",
    "    batch_mean = X.mean(axis=0)\n",
    "    batch_var  = X.var(axis=0)\n",
    "\n",
    "    delta = batch_mean - mean\n",
    "    mean = mean + delta * (n_batch / n_new)\n",
    "\n",
    "    # combine variances\n",
    "    M2 = M2 + batch_var * n_batch + (delta**2) * (n * n_batch / n_new)\n",
    "\n",
    "    n = n_new\n",
    "\n",
    "std = np.sqrt(M2 / max(n - 1, 1))\n",
    "\n",
    "mean = mean.astype(np.float32)\n",
    "std  = std.astype(np.float32)\n",
    "\n",
    "print(\"Xmean:\", mean)\n",
    "print(\"Xstd :\", std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ad0600-2499-4c40-8d1d-e64c6c776f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 208\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# ==============================\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# Load checkpoint\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# ==============================\u001b[39;00m\n\u001b[0;32m    207\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(MODEL_PT, map_location\u001b[38;5;241m=\u001b[39mDEVICE, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 208\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[43mckpt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclasses\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    209\u001b[0m class_to_idx \u001b[38;5;241m=\u001b[39m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_to_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    210\u001b[0m idx_to_class \u001b[38;5;241m=\u001b[39m {i:c \u001b[38;5;28;01mfor\u001b[39;00m c,i \u001b[38;5;129;01min\u001b[39;00m class_to_idx\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[1;31mKeyError\u001b[0m: 'classes'"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import laspy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ==============================\n",
    "# PATHS\n",
    "# ==============================\n",
    "IN_LAS_OR_LAZ = r\"D:/lidarrrrr/anbu/New folder/stage1_outputs/DX3035724_stage1_ground_v2.las\"\n",
    "MODEL_PT      = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"\n",
    "OUT_LAS       = r\"D:/lidarrrrr/anbu/New folder/dl_pred_pointnet7.las\"\n",
    "OUT_LAZ       = r\"D:/lidarrrrr/anbu/New folder/dl_pred_pointnet7.laz\"\n",
    "\n",
    "# ==============================\n",
    "# SETTINGS (IMPORTANT)\n",
    "# ==============================\n",
    "CELL      = 3.0      # same HAG grid cell\n",
    "TILE_SIZE = 40.0     # MUST match dataset tiling used in block creation\n",
    "DEVICE    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# block size used in training (points per block)\n",
    "BLOCK_N = 4096\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# ==============================\n",
    "# USE YOUR DATASET NORMALIZATION\n",
    "# ==============================\n",
    "Xmean = np.array([ 2.4770452e-01,  4.4691383e+01, -1.3224035e-05,  3.2253304e-01,\n",
    "                   3.6415633e+04,  1.0537578e+00,  1.0711324e+00, -1.5879369e+02,\n",
    "                   8.8634253e-01,  9.2550790e-01], dtype=np.float32)\n",
    "\n",
    "Xstd  = np.array([2.9587668e+02, 2.3407706e+02, 3.8826628e+00, 1.0009346e+00,\n",
    "                  6.9030566e+03, 2.6688504e-01, 3.0669588e-01, 9.5657166e+02,\n",
    "                  4.5917196e+00, 1.5607965e+00], dtype=np.float32)\n",
    "\n",
    "# ==============================\n",
    "# LAS dim getter\n",
    "# ==============================\n",
    "def get_dim(las, name, fallback=0.0, dtype=np.float32):\n",
    "    try:\n",
    "        return np.asarray(las[name]).astype(dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if name == \"scan_angle\":\n",
    "        for alt in [\"scan_angle\", \"scan_angle_rank\"]:\n",
    "            try:\n",
    "                return np.asarray(las[alt]).astype(dtype)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # common alt names\n",
    "    if name == \"return_number\":\n",
    "        for alt in [\"return_number\", \"return_num\"]:\n",
    "            try:\n",
    "                return np.asarray(las[alt]).astype(dtype)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    if name == \"number_of_returns\":\n",
    "        for alt in [\"number_of_returns\", \"num_returns\"]:\n",
    "            try:\n",
    "                return np.asarray(las[alt]).astype(dtype)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # RGB often missing\n",
    "    return np.full(len(las.x), fallback, dtype=dtype)\n",
    "\n",
    "# ==============================\n",
    "# HAG + slope (grid-based)\n",
    "# ==============================\n",
    "def compute_hag_and_slope(xyz, cls, cell=3.0, ground_class=2):\n",
    "    x, y, z = xyz[:,0], xyz[:,1], xyz[:,2]\n",
    "    minx, miny = x.min(), y.min()\n",
    "    gx = np.floor((x - minx)/cell).astype(np.int32)\n",
    "    gy = np.floor((y - miny)/cell).astype(np.int32)\n",
    "\n",
    "    # ground zmin per cell\n",
    "    cell_gmin = {}\n",
    "    g_idx = np.where(cls == ground_class)[0]\n",
    "    for i in g_idx:\n",
    "        k = (gx[i], gy[i])\n",
    "        zi = float(z[i])\n",
    "        if (k not in cell_gmin) or (zi < cell_gmin[k]):\n",
    "            cell_gmin[k] = zi\n",
    "\n",
    "    hag = np.zeros(len(z), dtype=np.float32)\n",
    "    has_ground = np.zeros(len(z), dtype=bool)\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        if k in cell_gmin:\n",
    "            hag[i] = z[i] - cell_gmin[k]\n",
    "            has_ground[i] = True\n",
    "        else:\n",
    "            hag[i] = 0.0\n",
    "\n",
    "    # zmin/zmax per cell for local range\n",
    "    cell_zmin, cell_zmax = {}, {}\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        zi = float(z[i])\n",
    "        if k not in cell_zmin:\n",
    "            cell_zmin[k] = zi\n",
    "            cell_zmax[k] = zi\n",
    "        else:\n",
    "            if zi < cell_zmin[k]: cell_zmin[k] = zi\n",
    "            if zi > cell_zmax[k]: cell_zmax[k] = zi\n",
    "\n",
    "    local_range = np.zeros(len(z), dtype=np.float32)\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        local_range[i] = float(cell_zmax[k] - cell_zmin[k])\n",
    "\n",
    "    slope = (hag / (local_range + 1e-6)).astype(np.float32)\n",
    "    return hag, slope, has_ground\n",
    "\n",
    "# ==============================\n",
    "# Build X for a tile (tile-wise XY centering)\n",
    "# Feature order MUST match training:\n",
    "# [x,y,z,hag,inten,rn,nr,scan,deviation,slope]\n",
    "# ==============================\n",
    "def build_features_for_indices(las, idx, cell=3.0):\n",
    "    x = np.asarray(las.x, dtype=np.float32)[idx]\n",
    "    y = np.asarray(las.y, dtype=np.float32)[idx]\n",
    "    z = np.asarray(las.z, dtype=np.float32)[idx]\n",
    "    xyz = np.stack([x,y,z], axis=1)\n",
    "\n",
    "    cls = np.asarray(las.classification, dtype=np.int32)[idx]\n",
    "\n",
    "    inten = get_dim(las, \"intensity\", 0.0)[idx]\n",
    "    rn    = get_dim(las, \"return_number\", 1.0)[idx]\n",
    "    nr    = get_dim(las, \"number_of_returns\", 1.0)[idx]\n",
    "    scan  = get_dim(las, \"scan_angle\", 0.0)[idx]\n",
    "    dev   = get_dim(las, \"Deviation\", 0.0)[idx]\n",
    "\n",
    "    # tile-wise centering for XY (THIS is usually what blocks used)\n",
    "    x0 = x.mean()\n",
    "    y0 = y.mean()\n",
    "    x_local = x - x0\n",
    "    y_local = y - y0\n",
    "\n",
    "    # HAG/slope computed using *tile xyz + tile cls*\n",
    "    hag, slope, has_ground = compute_hag_and_slope(xyz, cls, cell=cell)\n",
    "\n",
    "    X = np.stack([\n",
    "        x_local, y_local, z,\n",
    "        hag,\n",
    "        inten,\n",
    "        rn,\n",
    "        nr,\n",
    "        scan,\n",
    "        dev,\n",
    "        slope\n",
    "    ], axis=1).astype(np.float32)\n",
    "\n",
    "    return X, has_ground\n",
    "\n",
    "# ==============================\n",
    "# Model (must match ckpt)\n",
    "# ==============================\n",
    "class PointNetFromCkpt(nn.Module):\n",
    "    def __init__(self, in_ch=10, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 64, 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv1d(64, 64, 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv1d(64, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(512, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(256, num_classes, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.mlp1(x)                       # (B,128,N)\n",
    "        h = self.mlp2(h)                       # (B,256,N)\n",
    "        g = torch.max(h, dim=2, keepdim=True)[0]  # (B,256,1)\n",
    "        g = g.repeat(1, 1, h.shape[2])         # (B,256,N)\n",
    "        feat = torch.cat([h, g], dim=1)        # (B,512,N)\n",
    "        out = self.head(feat)                  # (B,C,N)\n",
    "        return out\n",
    "\n",
    "# ==============================\n",
    "# Load checkpoint\n",
    "# ==============================\n",
    "ckpt = torch.load(MODEL_PT, map_location=DEVICE, weights_only=False)\n",
    "classes = ckpt[\"classes\"]\n",
    "class_to_idx = ckpt[\"class_to_idx\"]\n",
    "idx_to_class = {i:c for c,i in class_to_idx.items()}\n",
    "\n",
    "IN_CH = int(ckpt[\"feats\"])\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "print(\"Loaded CKPT classes:\", classes)\n",
    "print(\"Input feats:\", IN_CH, \"Num classes:\", NUM_CLASSES)\n",
    "\n",
    "model = PointNetFromCkpt(in_ch=IN_CH, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state\"], strict=True)\n",
    "model.eval()\n",
    "print(\"✅ Model loaded (strict).\")\n",
    "\n",
    "# ==============================\n",
    "# Predict by tiles\n",
    "# ==============================\n",
    "las = laspy.read(IN_LAS_OR_LAZ)\n",
    "N = len(las.x)\n",
    "xyz_all = np.vstack([las.x, las.y, las.z]).T.astype(np.float32)\n",
    "\n",
    "# tile indexing on XY\n",
    "x_all = xyz_all[:,0]\n",
    "y_all = xyz_all[:,1]\n",
    "minx, miny = x_all.min(), y_all.min()\n",
    "tx = np.floor((x_all - minx) / TILE_SIZE).astype(np.int32)\n",
    "ty = np.floor((y_all - miny) / TILE_SIZE).astype(np.int32)\n",
    "tile_key = tx.astype(np.int64) * 1_000_000 + ty.astype(np.int64)\n",
    "\n",
    "# group indices per tile\n",
    "order = np.argsort(tile_key)\n",
    "tile_key_s = tile_key[order]\n",
    "uniq, start = np.unique(tile_key_s, return_index=True)\n",
    "\n",
    "pred_out = np.zeros(N, dtype=np.uint8)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(uniq)), desc=\"Predicting tiles\"):\n",
    "        a = start[i]\n",
    "        b = start[i+1] if i+1 < len(uniq) else len(order)\n",
    "        idx = order[a:b]\n",
    "\n",
    "        # build tile features\n",
    "        X, has_ground = build_features_for_indices(las, idx, cell=CELL)\n",
    "\n",
    "        # normalize\n",
    "        Xn = (X - Xmean[None, :]) / (Xstd[None, :] + 1e-6)\n",
    "\n",
    "        # predict in chunks of BLOCK_N (4096)\n",
    "        out_idx = np.zeros(len(idx), dtype=np.int64)\n",
    "        for s in range(0, len(idx), BLOCK_N):\n",
    "            e = min(s + BLOCK_N, len(idx))\n",
    "            xb = Xn[s:e]\n",
    "\n",
    "            inp = torch.from_numpy(xb).to(DEVICE)          # (n,f)\n",
    "            inp = inp.unsqueeze(0).permute(0, 2, 1)        # (1,f,n)\n",
    "\n",
    "            logits = model(inp)                             # (1,C,n)\n",
    "            p = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()\n",
    "            out_idx[s:e] = p\n",
    "\n",
    "        # map to LAS classes\n",
    "        pred_cls = np.vectorize(idx_to_class.get)(out_idx).astype(np.uint8)\n",
    "        pred_out[idx] = pred_cls\n",
    "\n",
    "# write output\n",
    "las.classification = pred_out\n",
    "las.write(OUT_LAS)\n",
    "print(\"Saved LAS:\", OUT_LAS)\n",
    "try:\n",
    "    las.write(OUT_LAZ)\n",
    "    print(\"Saved LAZ:\", OUT_LAZ)\n",
    "except Exception as e:\n",
    "    print(\"LAZ write failed (ok):\", e)\n",
    "\n",
    "u, c = np.unique(pred_out, return_counts=True)\n",
    "print(\"Pred class counts:\", dict(zip(u.tolist(), c.tolist())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4d7660f-f770-4888-8cfa-18632acb3b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'dict'>\n",
      "Top-level keys: ['model_state', 'num_classes', 'class_weights']\n",
      "Sample keys: ['model_state', 'num_classes', 'class_weights']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "MODEL_PT = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"\n",
    "ckpt = torch.load(MODEL_PT, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "print(\"Type:\", type(ckpt))\n",
    "\n",
    "if isinstance(ckpt, dict):\n",
    "    print(\"Top-level keys:\", list(ckpt.keys())[:50])\n",
    "    # if it looks like a state_dict, print a few parameter keys\n",
    "    # (state_dict keys usually contain '.weight' / '.bias')\n",
    "    sample_keys = list(ckpt.keys())[:20]\n",
    "    print(\"Sample keys:\", sample_keys)\n",
    "else:\n",
    "    print(\"Not a dict. Example repr:\", repr(ckpt)[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ff757a-60e5-454e-81e8-d9dfee7068e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PointNetFromCkpt:\n\tMissing key(s) in state_dict: \"mlp1.0.weight\", \"mlp1.0.bias\", \"mlp1.1.weight\", \"mlp1.1.bias\", \"mlp1.1.running_mean\", \"mlp1.1.running_var\", \"mlp1.3.weight\", \"mlp1.3.bias\", \"mlp1.4.weight\", \"mlp1.4.bias\", \"mlp1.4.running_mean\", \"mlp1.4.running_var\", \"mlp1.6.weight\", \"mlp1.6.bias\", \"mlp1.7.weight\", \"mlp1.7.bias\", \"mlp1.7.running_mean\", \"mlp1.7.running_var\", \"mlp2.0.weight\", \"mlp2.0.bias\", \"mlp2.1.weight\", \"mlp2.1.bias\", \"mlp2.1.running_mean\", \"mlp2.1.running_var\", \"head.0.weight\", \"head.0.bias\", \"head.1.weight\", \"head.1.bias\", \"head.1.running_mean\", \"head.1.running_var\", \"head.3.weight\", \"head.3.bias\". \n\tUnexpected key(s) in state_dict: \"model_state\", \"num_classes\", \"class_weights\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 139\u001b[0m\n\u001b[0;32m    137\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(MODEL_PT, map_location\u001b[38;5;241m=\u001b[39mDEVICE, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    138\u001b[0m model \u001b[38;5;241m=\u001b[39m PointNetFromCkpt(in_ch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39mNUM_CLASSES)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m--> 139\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Loaded weights (state_dict).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\lidar\\lib\\site-packages\\torch\\nn\\modules\\module.py:2635\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2627\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2628\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2629\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2630\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2631\u001b[0m             ),\n\u001b[0;32m   2632\u001b[0m         )\n\u001b[0;32m   2634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2635\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2636\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2637\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2638\u001b[0m         )\n\u001b[0;32m   2639\u001b[0m     )\n\u001b[0;32m   2640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PointNetFromCkpt:\n\tMissing key(s) in state_dict: \"mlp1.0.weight\", \"mlp1.0.bias\", \"mlp1.1.weight\", \"mlp1.1.bias\", \"mlp1.1.running_mean\", \"mlp1.1.running_var\", \"mlp1.3.weight\", \"mlp1.3.bias\", \"mlp1.4.weight\", \"mlp1.4.bias\", \"mlp1.4.running_mean\", \"mlp1.4.running_var\", \"mlp1.6.weight\", \"mlp1.6.bias\", \"mlp1.7.weight\", \"mlp1.7.bias\", \"mlp1.7.running_mean\", \"mlp1.7.running_var\", \"mlp2.0.weight\", \"mlp2.0.bias\", \"mlp2.1.weight\", \"mlp2.1.bias\", \"mlp2.1.running_mean\", \"mlp2.1.running_var\", \"head.0.weight\", \"head.0.bias\", \"head.1.weight\", \"head.1.bias\", \"head.1.running_mean\", \"head.1.running_var\", \"head.3.weight\", \"head.3.bias\". \n\tUnexpected key(s) in state_dict: \"model_state\", \"num_classes\", \"class_weights\". "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import laspy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "IN_LAS_OR_LAZ = r\"D:/lidarrrrr/anbu/New folder/stage1_outputs/DX3035724_stage1_ground_v2.las\"\n",
    "MODEL_PT      = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"\n",
    "OUT_LAS       = r\"D:/lidarrrrr/anbu/New folder/dl_pred_pointnet7.las\"\n",
    "OUT_LAZ       = r\"D:/lidarrrrr/anbu/New folder/dl_pred_pointnet7.laz\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# ===== Your 7-class label set (must match training) =====\n",
    "CLASSES = [1, 2, 3, 6, 7, 12, 13]  # order must match training\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "idx_to_class = {i: c for i, c in enumerate(CLASSES)}\n",
    "\n",
    "# ===== Your normalization (the one you posted) =====\n",
    "Xmean = np.array([ 2.4770452e-01,  4.4691383e+01, -1.3224035e-05,  3.2253304e-01,\n",
    "                   3.6415633e+04,  1.0537578e+00,  1.0711324e+00, -1.5879369e+02,\n",
    "                   8.8634253e-01,  9.2550790e-01], dtype=np.float32)\n",
    "\n",
    "Xstd  = np.array([2.9587668e+02, 2.3407706e+02, 3.8826628e+00, 1.0009346e+00,\n",
    "                  6.9030566e+03, 2.6688504e-01, 3.0669588e-01, 9.5657166e+02,\n",
    "                  4.5917196e+00, 1.5607965e+00], dtype=np.float32)\n",
    "\n",
    "# ===== Feature build settings =====\n",
    "CELL      = 3.0\n",
    "TILE_SIZE = 40.0\n",
    "BLOCK_N   = 4096\n",
    "\n",
    "def get_dim(las, name, fallback=0.0, dtype=np.float32):\n",
    "    try:\n",
    "        return np.asarray(las[name]).astype(dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if name == \"scan_angle\":\n",
    "        for alt in [\"scan_angle\", \"scan_angle_rank\"]:\n",
    "            try:\n",
    "                return np.asarray(las[alt]).astype(dtype)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return np.full(len(las.x), fallback, dtype=dtype)\n",
    "\n",
    "def compute_hag_and_slope(xyz, cls, cell=3.0, ground_class=2):\n",
    "    x, y, z = xyz[:,0], xyz[:,1], xyz[:,2]\n",
    "    minx, miny = x.min(), y.min()\n",
    "    gx = np.floor((x - minx)/cell).astype(np.int32)\n",
    "    gy = np.floor((y - miny)/cell).astype(np.int32)\n",
    "\n",
    "    cell_gmin = {}\n",
    "    g_idx = np.where(cls == ground_class)[0]\n",
    "    for i in g_idx:\n",
    "        k = (gx[i], gy[i])\n",
    "        zi = float(z[i])\n",
    "        if (k not in cell_gmin) or (zi < cell_gmin[k]):\n",
    "            cell_gmin[k] = zi\n",
    "\n",
    "    hag = np.zeros(len(z), dtype=np.float32)\n",
    "    has_ground = np.zeros(len(z), dtype=bool)\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        if k in cell_gmin:\n",
    "            hag[i] = z[i] - cell_gmin[k]\n",
    "            has_ground[i] = True\n",
    "\n",
    "    cell_zmin, cell_zmax = {}, {}\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        zi = float(z[i])\n",
    "        if k not in cell_zmin:\n",
    "            cell_zmin[k] = zi\n",
    "            cell_zmax[k] = zi\n",
    "        else:\n",
    "            if zi < cell_zmin[k]: cell_zmin[k] = zi\n",
    "            if zi > cell_zmax[k]: cell_zmax[k] = zi\n",
    "\n",
    "    local_range = np.zeros(len(z), dtype=np.float32)\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        local_range[i] = float(cell_zmax[k] - cell_zmin[k])\n",
    "\n",
    "    slope = (hag / (local_range + 1e-6)).astype(np.float32)\n",
    "    return hag, slope, has_ground\n",
    "\n",
    "def build_features_for_indices(las, idx):\n",
    "    x = np.asarray(las.x, dtype=np.float32)[idx]\n",
    "    y = np.asarray(las.y, dtype=np.float32)[idx]\n",
    "    z = np.asarray(las.z, dtype=np.float32)[idx]\n",
    "    xyz = np.stack([x,y,z], axis=1)\n",
    "    cls = np.asarray(las.classification, dtype=np.int32)[idx]\n",
    "\n",
    "    inten = get_dim(las, \"intensity\", 0.0)[idx]\n",
    "    rn    = get_dim(las, \"return_number\", 1.0)[idx]\n",
    "    nr    = get_dim(las, \"number_of_returns\", 1.0)[idx]\n",
    "    scan  = get_dim(las, \"scan_angle\", 0.0)[idx]\n",
    "    dev   = get_dim(las, \"Deviation\", 0.0)[idx]\n",
    "\n",
    "    # tile-wise centering\n",
    "    x_local = x - x.mean()\n",
    "    y_local = y - y.mean()\n",
    "\n",
    "    hag, slope, has_ground = compute_hag_and_slope(xyz, cls, cell=CELL)\n",
    "\n",
    "    X = np.stack([x_local, y_local, z, hag, inten, rn, nr, scan, dev, slope], axis=1).astype(np.float32)\n",
    "    return X\n",
    "\n",
    "class PointNetFromCkpt(nn.Module):\n",
    "    def __init__(self, in_ch=10, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 64, 1), nn.BatchNorm1d(64), nn.ReLU(True),\n",
    "            nn.Conv1d(64, 64, 1),    nn.BatchNorm1d(64), nn.ReLU(True),\n",
    "            nn.Conv1d(64, 128, 1),   nn.BatchNorm1d(128), nn.ReLU(True),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 1), nn.BatchNorm1d(256), nn.ReLU(True),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(512, 256, 1), nn.BatchNorm1d(256), nn.ReLU(True),\n",
    "            nn.Conv1d(256, num_classes, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.mlp1(x)                      # (B,128,N)\n",
    "        h = self.mlp2(h)                      # (B,256,N)\n",
    "        g = torch.max(h, dim=2, keepdim=True)[0].repeat(1,1,h.shape[2])\n",
    "        feat = torch.cat([h, g], dim=1)       # (B,512,N)\n",
    "        return self.head(feat)                # (B,C,N)\n",
    "\n",
    "# ---- load state_dict only ----\n",
    "state = torch.load(MODEL_PT, map_location=DEVICE, weights_only=False)\n",
    "model = PointNetFromCkpt(in_ch=10, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()\n",
    "print(\"✅ Loaded weights (state_dict).\")\n",
    "\n",
    "# ---- predict tile-wise ----\n",
    "las = laspy.read(IN_LAS_OR_LAZ)\n",
    "N = len(las.x)\n",
    "\n",
    "xy = np.vstack([las.x, las.y]).T.astype(np.float32)\n",
    "minx, miny = xy[:,0].min(), xy[:,1].min()\n",
    "tx = np.floor((xy[:,0] - minx)/TILE_SIZE).astype(np.int32)\n",
    "ty = np.floor((xy[:,1] - miny)/TILE_SIZE).astype(np.int32)\n",
    "tile_key = tx.astype(np.int64)*1_000_000 + ty.astype(np.int64)\n",
    "\n",
    "order = np.argsort(tile_key)\n",
    "tile_key_s = tile_key[order]\n",
    "uniq, start = np.unique(tile_key_s, return_index=True)\n",
    "\n",
    "pred_cls_all = np.zeros(N, dtype=np.uint8)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(uniq)), desc=\"Predicting tiles\"):\n",
    "        a = start[i]\n",
    "        b = start[i+1] if i+1 < len(uniq) else len(order)\n",
    "        idx = order[a:b]\n",
    "\n",
    "        X = build_features_for_indices(las, idx)\n",
    "        Xn = (X - Xmean[None,:]) / (Xstd[None,:] + 1e-6)\n",
    "\n",
    "        out_idx = np.zeros(len(idx), dtype=np.int64)\n",
    "        for s in range(0, len(idx), BLOCK_N):\n",
    "            e = min(s+BLOCK_N, len(idx))\n",
    "            xb = Xn[s:e]\n",
    "            inp = torch.from_numpy(xb).to(DEVICE).unsqueeze(0).permute(0,2,1)\n",
    "            logits = model(inp)\n",
    "            p = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()\n",
    "            out_idx[s:e] = p\n",
    "\n",
    "        pred_cls_all[idx] = np.vectorize(idx_to_class.get)(out_idx).astype(np.uint8)\n",
    "\n",
    "las.classification = pred_cls_all\n",
    "las.write(OUT_LAS)\n",
    "print(\"Saved LAS:\", OUT_LAS)\n",
    "try:\n",
    "    las.write(OUT_LAZ)\n",
    "    print(\"Saved LAZ:\", OUT_LAZ)\n",
    "except Exception as e:\n",
    "    print(\"LAZ write failed:\", e)\n",
    "\n",
    "u, c = np.unique(pred_cls_all, return_counts=True)\n",
    "print(\"Pred class counts:\", dict(zip(u.tolist(), c.tolist())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a84ca2a0-f70d-42e8-833b-2f85a18ace22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n",
      "CKPT keys: ['model_state', 'num_classes', 'class_weights']\n",
      "⚠️ num_classes in ckpt = 5 but CLASSES length = 7\n",
      "   If this mismatch is real, tell me your training class list.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PointNetFromCkptV3:\n\tMissing key(s) in state_dict: \"mlp1.0.weight\", \"mlp1.0.bias\", \"mlp1.1.weight\", \"mlp1.1.bias\", \"mlp1.1.running_mean\", \"mlp1.1.running_var\", \"mlp1.3.weight\", \"mlp1.3.bias\", \"mlp1.4.weight\", \"mlp1.4.bias\", \"mlp1.4.running_mean\", \"mlp1.4.running_var\", \"mlp1.6.weight\", \"mlp1.6.bias\", \"mlp1.7.weight\", \"mlp1.7.bias\", \"mlp1.7.running_mean\", \"mlp1.7.running_var\", \"mlp2.0.weight\", \"mlp2.0.bias\", \"mlp2.1.weight\", \"mlp2.1.bias\", \"mlp2.1.running_mean\", \"mlp2.1.running_var\", \"head.0.weight\", \"head.0.bias\", \"head.1.weight\", \"head.1.bias\", \"head.1.running_mean\", \"head.1.running_var\", \"head.3.weight\", \"head.3.bias\". \n\tUnexpected key(s) in state_dict: \"mlp3.weight\", \"mlp3.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"mlp1.weight\", \"mlp1.bias\", \"mlp2.weight\", \"mlp2.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 172\u001b[0m\n\u001b[0;32m    167\u001b[0m Xstd  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m2.9587668e+02\u001b[39m, \u001b[38;5;241m2.3407706e+02\u001b[39m, \u001b[38;5;241m3.8826628e+00\u001b[39m, \u001b[38;5;241m1.0009346e+00\u001b[39m,\n\u001b[0;32m    168\u001b[0m                   \u001b[38;5;241m6.9030566e+03\u001b[39m, \u001b[38;5;241m2.6688504e-01\u001b[39m, \u001b[38;5;241m3.0669588e-01\u001b[39m, \u001b[38;5;241m9.5657166e+02\u001b[39m,\n\u001b[0;32m    169\u001b[0m                   \u001b[38;5;241m4.5917196e+00\u001b[39m, \u001b[38;5;241m1.5607965e+00\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    171\u001b[0m model \u001b[38;5;241m=\u001b[39m PointNetFromCkptV3(in_ch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(CLASSES))\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m--> 172\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Model loaded (strict).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\anaconda\\envs\\lidar\\lib\\site-packages\\torch\\nn\\modules\\module.py:2635\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2627\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2628\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2629\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2630\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2631\u001b[0m             ),\n\u001b[0;32m   2632\u001b[0m         )\n\u001b[0;32m   2634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2635\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2636\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2637\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2638\u001b[0m         )\n\u001b[0;32m   2639\u001b[0m     )\n\u001b[0;32m   2640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PointNetFromCkptV3:\n\tMissing key(s) in state_dict: \"mlp1.0.weight\", \"mlp1.0.bias\", \"mlp1.1.weight\", \"mlp1.1.bias\", \"mlp1.1.running_mean\", \"mlp1.1.running_var\", \"mlp1.3.weight\", \"mlp1.3.bias\", \"mlp1.4.weight\", \"mlp1.4.bias\", \"mlp1.4.running_mean\", \"mlp1.4.running_var\", \"mlp1.6.weight\", \"mlp1.6.bias\", \"mlp1.7.weight\", \"mlp1.7.bias\", \"mlp1.7.running_mean\", \"mlp1.7.running_var\", \"mlp2.0.weight\", \"mlp2.0.bias\", \"mlp2.1.weight\", \"mlp2.1.bias\", \"mlp2.1.running_mean\", \"mlp2.1.running_var\", \"head.0.weight\", \"head.0.bias\", \"head.1.weight\", \"head.1.bias\", \"head.1.running_mean\", \"head.1.running_var\", \"head.3.weight\", \"head.3.bias\". \n\tUnexpected key(s) in state_dict: \"mlp3.weight\", \"mlp3.bias\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\", \"mlp1.weight\", \"mlp1.bias\", \"mlp2.weight\", \"mlp2.bias\". "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import laspy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# =========================================================\n",
    "# PATHS\n",
    "# =========================================================\n",
    "IN_LAS_OR_LAZ = r\"D:/lidarrrrr/anbu/New folder/stage1_outputs/DX3035724_stage1_ground_v2.las\"\n",
    "MODEL_PT      = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"\n",
    "OUT_LAS       = r\"D:/lidarrrrr/anbu/New folder/dl_pred_pointnet7_fixed.las\"\n",
    "OUT_LAZ       = r\"D:/lidarrrrr/anbu/New folder/dl_pred_pointnet7_fixed.laz\"\n",
    "\n",
    "# =========================================================\n",
    "# SETTINGS\n",
    "# =========================================================\n",
    "DEVICE    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CELL      = 3.0\n",
    "TILE_SIZE = 40.0\n",
    "BLOCK_N   = 4096\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# =========================================================\n",
    "# LAS helpers\n",
    "# =========================================================\n",
    "def get_dim(las, name, fallback=0.0, dtype=np.float32):\n",
    "    try:\n",
    "        return np.asarray(las[name]).astype(dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "    if name == \"scan_angle\":\n",
    "        for alt in [\"scan_angle\", \"scan_angle_rank\"]:\n",
    "            try:\n",
    "                return np.asarray(las[alt]).astype(dtype)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return np.full(len(las.x), fallback, dtype=dtype)\n",
    "\n",
    "def compute_hag_and_slope(xyz, cls, cell=3.0, ground_class=2):\n",
    "    x, y, z = xyz[:,0], xyz[:,1], xyz[:,2]\n",
    "    minx, miny = x.min(), y.min()\n",
    "    gx = np.floor((x - minx)/cell).astype(np.int32)\n",
    "    gy = np.floor((y - miny)/cell).astype(np.int32)\n",
    "\n",
    "    # ground min per cell\n",
    "    cell_gmin = {}\n",
    "    g_idx = np.where(cls == ground_class)[0]\n",
    "    for i in g_idx:\n",
    "        k = (gx[i], gy[i])\n",
    "        zi = float(z[i])\n",
    "        if (k not in cell_gmin) or (zi < cell_gmin[k]):\n",
    "            cell_gmin[k] = zi\n",
    "\n",
    "    hag = np.zeros(len(z), dtype=np.float32)\n",
    "    has_ground = np.zeros(len(z), dtype=bool)\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        if k in cell_gmin:\n",
    "            hag[i] = z[i] - cell_gmin[k]\n",
    "            has_ground[i] = True\n",
    "\n",
    "    # z range per cell\n",
    "    cell_zmin, cell_zmax = {}, {}\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        zi = float(z[i])\n",
    "        if k not in cell_zmin:\n",
    "            cell_zmin[k] = zi\n",
    "            cell_zmax[k] = zi\n",
    "        else:\n",
    "            if zi < cell_zmin[k]: cell_zmin[k] = zi\n",
    "            if zi > cell_zmax[k]: cell_zmax[k] = zi\n",
    "\n",
    "    local_range = np.zeros(len(z), dtype=np.float32)\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        local_range[i] = float(cell_zmax[k] - cell_zmin[k])\n",
    "\n",
    "    slope = (hag / (local_range + 1e-6)).astype(np.float32)\n",
    "    return hag, slope\n",
    "\n",
    "def build_features_for_indices(las, idx):\n",
    "    x = np.asarray(las.x, dtype=np.float32)[idx]\n",
    "    y = np.asarray(las.y, dtype=np.float32)[idx]\n",
    "    z = np.asarray(las.z, dtype=np.float32)[idx]\n",
    "    xyz = np.stack([x,y,z], axis=1)\n",
    "    cls = np.asarray(las.classification, dtype=np.int32)[idx]\n",
    "\n",
    "    inten = get_dim(las, \"intensity\", 0.0)[idx]\n",
    "    rn    = get_dim(las, \"return_number\", 1.0)[idx]\n",
    "    nr    = get_dim(las, \"number_of_returns\", 1.0)[idx]\n",
    "    scan  = get_dim(las, \"scan_angle\", 0.0)[idx]\n",
    "    dev   = get_dim(las, \"Deviation\", 0.0)[idx]\n",
    "\n",
    "    # IMPORTANT: tile-local centering for x,y (matches your later fix)\n",
    "    x_local = x - x.mean()\n",
    "    y_local = y - y.mean()\n",
    "\n",
    "    hag, slope = compute_hag_and_slope(xyz, cls, cell=CELL)\n",
    "\n",
    "    # 10 features (your training order)\n",
    "    X = np.stack([x_local, y_local, z, hag, inten, rn, nr, scan, dev, slope], axis=1).astype(np.float32)\n",
    "    return X\n",
    "\n",
    "# =========================================================\n",
    "# MODEL: match your checkpoint keys\n",
    "# Needed keys from your error:\n",
    "#  mlp1.0/1/3/4/6/7  (conv+bn pairs)\n",
    "#  mlp2.0/1\n",
    "#  head.0/1/3\n",
    "# =========================================================\n",
    "class PointNetFromCkptV3(nn.Module):\n",
    "    def __init__(self, in_ch=10, num_classes=7):\n",
    "        super().__init__()\n",
    "        # mlp1: 3 conv blocks (64->64->128) but with 6/7 present => actually 4 convs:\n",
    "        # conv64, conv64, conv128, conv128 (based on typical patterns + your keys)\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 64, 1), nn.BatchNorm1d(64), nn.ReLU(True),   # 0,1\n",
    "            nn.Conv1d(64, 64, 1),    nn.BatchNorm1d(64), nn.ReLU(True),   # 3,4\n",
    "            nn.Conv1d(64, 128, 1),   nn.BatchNorm1d(128), nn.ReLU(True),  # 6,7\n",
    "        )\n",
    "        # mlp2\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 1), nn.BatchNorm1d(256), nn.ReLU(True)    # 0,1\n",
    "        )\n",
    "        # head (512 -> 256 -> C) with head.3 being final conv\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(512, 256, 1), nn.BatchNorm1d(256), nn.ReLU(True),   # 0,1\n",
    "            nn.Conv1d(256, num_classes, 1),                               # 3\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,F,N)\n",
    "        h = self.mlp1(x)                                # (B,128,N)\n",
    "        h = self.mlp2(h)                                # (B,256,N)\n",
    "        g = torch.max(h, dim=2, keepdim=True)[0]         # (B,256,1)\n",
    "        g = g.repeat(1, 1, h.shape[2])                   # (B,256,N)\n",
    "        feat = torch.cat([h, g], dim=1)                  # (B,512,N)\n",
    "        return self.head(feat)                           # (B,C,N)\n",
    "\n",
    "# =========================================================\n",
    "# LOAD CHECKPOINT PROPERLY\n",
    "# =========================================================\n",
    "ckpt = torch.load(MODEL_PT, map_location=DEVICE, weights_only=False)\n",
    "print(\"CKPT keys:\", list(ckpt.keys()))\n",
    "\n",
    "state = ckpt[\"model_state\"]\n",
    "NUM_CLASSES = int(ckpt.get(\"num_classes\", 7))\n",
    "\n",
    "# IMPORTANT: If you trained on these 7 LiDAR classes, keep this mapping\n",
    "CLASSES = [1, 2, 3, 6, 7, 12, 13]\n",
    "if NUM_CLASSES != len(CLASSES):\n",
    "    print(\"⚠️ num_classes in ckpt =\", NUM_CLASSES, \"but CLASSES length =\", len(CLASSES))\n",
    "    print(\"   If this mismatch is real, tell me your training class list.\")\n",
    "\n",
    "idx_to_class = {i: c for i, c in enumerate(CLASSES)}\n",
    "\n",
    "# Normalization (use the values you already printed)\n",
    "Xmean = np.array([ 2.4770452e-01,  4.4691383e+01, -1.3224035e-05,  3.2253304e-01,\n",
    "                   3.6415633e+04,  1.0537578e+00,  1.0711324e+00, -1.5879369e+02,\n",
    "                   8.8634253e-01,  9.2550790e-01], dtype=np.float32)\n",
    "Xstd  = np.array([2.9587668e+02, 2.3407706e+02, 3.8826628e+00, 1.0009346e+00,\n",
    "                  6.9030566e+03, 2.6688504e-01, 3.0669588e-01, 9.5657166e+02,\n",
    "                  4.5917196e+00, 1.5607965e+00], dtype=np.float32)\n",
    "\n",
    "model = PointNetFromCkptV3(in_ch=10, num_classes=len(CLASSES)).to(DEVICE)\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()\n",
    "print(\"✅ Model loaded (strict).\")\n",
    "\n",
    "# =========================================================\n",
    "# PREDICT (tile-wise)\n",
    "# =========================================================\n",
    "las = laspy.read(IN_LAS_OR_LAZ)\n",
    "N = len(las.x)\n",
    "print(\"Total points:\", N)\n",
    "\n",
    "xy = np.vstack([las.x, las.y]).T.astype(np.float32)\n",
    "minx, miny = xy[:,0].min(), xy[:,1].min()\n",
    "tx = np.floor((xy[:,0] - minx)/TILE_SIZE).astype(np.int32)\n",
    "ty = np.floor((xy[:,1] - miny)/TILE_SIZE).astype(np.int32)\n",
    "tile_key = tx.astype(np.int64)*1_000_000 + ty.astype(np.int64)\n",
    "\n",
    "order = np.argsort(tile_key)\n",
    "tile_key_s = tile_key[order]\n",
    "uniq, start = np.unique(tile_key_s, return_index=True)\n",
    "\n",
    "pred_all = np.zeros(N, dtype=np.uint8)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(uniq)), desc=\"Predicting tiles\"):\n",
    "        a = start[i]\n",
    "        b = start[i+1] if i+1 < len(uniq) else len(order)\n",
    "        idx = order[a:b]\n",
    "\n",
    "        X = build_features_for_indices(las, idx)\n",
    "        Xn = (X - Xmean[None,:]) / (Xstd[None,:] + 1e-6)\n",
    "\n",
    "        out_idx = np.zeros(len(idx), dtype=np.int64)\n",
    "        for s in range(0, len(idx), BLOCK_N):\n",
    "            e = min(s + BLOCK_N, len(idx))\n",
    "            xb = Xn[s:e]\n",
    "            inp = torch.from_numpy(xb).to(DEVICE).unsqueeze(0).permute(0,2,1)  # (1,F,N)\n",
    "            logits = model(inp)\n",
    "            p = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()\n",
    "            out_idx[s:e] = p\n",
    "\n",
    "        pred_all[idx] = np.vectorize(idx_to_class.get)(out_idx).astype(np.uint8)\n",
    "\n",
    "las.classification = pred_all\n",
    "las.write(OUT_LAS)\n",
    "print(\"Saved LAS:\", OUT_LAS)\n",
    "try:\n",
    "    las.write(OUT_LAZ)\n",
    "    print(\"Saved LAZ:\", OUT_LAZ)\n",
    "except Exception as e:\n",
    "    print(\"LAZ write failed:\", e)\n",
    "\n",
    "u, c = np.unique(pred_all, return_counts=True)\n",
    "print(\"Pred class counts:\", dict(zip(u.tolist(), c.tolist())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a264fdd7-8a4e-48cc-a84f-4b950953c0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n",
      "Loaded: D:/lidarrrrr/anbu/DX3035724 S.GIUSTO000001.laz\n",
      "Points: 12374846\n",
      "X_full shape: (12374846, 10)\n",
      "CKPT keys sample: ['mlp1.weight', 'mlp1.bias', 'mlp2.weight', 'mlp2.bias', 'mlp3.weight', 'mlp3.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias']\n",
      "✅ Legacy ckpt detected. ckpt head classes = 5\n",
      "⚠️ Head class mismatch: ckpt 5 vs target 7. Will load backbone and replace head.\n",
      "Loaded backbone. Missing: ['fc2.weight', 'fc2.bias'] Unexpected: []\n",
      "Blocks created: 119 (block_size=40.0, N=4096)\n",
      "Predicted blocks 0/119\n",
      "Predicted blocks 80/119\n",
      "✅ Wrote LAS: D:/lidarrrrr/anbu/New folder/dl_predicted_fixed.las\n",
      "✅ Wrote LAZ: D:/lidarrrrr/anbu/New folder/dl_predicted_fixed.laz\n",
      "Pred counts (contig 0..6): {0: 12372645, 1: 540, 2: 570, 3: 51, 4: 189, 5: 653, 6: 198}\n",
      "Pred counts (raw classes): {1: 12372645, 2: 540, 3: 570, 6: 51, 7: 189, 12: 653, 13: 198}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import laspy\n",
    "\n",
    "# ============================================================\n",
    "# USER PATHS\n",
    "# ============================================================\n",
    "IN_LAZ   = r\"D:/lidarrrrr/anbu/DX3035724 S.GIUSTO000001.laz\"\n",
    "MODEL_PT = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"   # your ckpt\n",
    "OUT_LAS  = r\"D:/lidarrrrr/anbu/New folder/dl_predicted_fixed.las\"\n",
    "OUT_LAZ  = r\"D:/lidarrrrr/anbu/New folder/dl_predicted_fixed.laz\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ============================================================\n",
    "# CLASSES (raw LAS -> model 0..6 and back)\n",
    "# ============================================================\n",
    "RAW_CLASSES = [1, 2, 3, 6, 7, 12, 13]  # LAS codes\n",
    "RAW_TO_CONTIG = {c: i for i, c in enumerate(RAW_CLASSES)}\n",
    "CONTIG_TO_RAW = {i: c for i, c in enumerate(RAW_CLASSES)}\n",
    "NUM_CLASSES = len(RAW_CLASSES)\n",
    "\n",
    "# ============================================================\n",
    "# NORMALIZATION STATS (PUT YOUR REAL ONES HERE)\n",
    "# If you already have Xmean/Xstd from training, paste them exactly.\n",
    "# Otherwise keep identity; model may perform worse.\n",
    "# ============================================================\n",
    "XMEAN = np.zeros(10, dtype=np.float32)\n",
    "XSTD  = np.ones(10, dtype=np.float32)\n",
    "\n",
    "# Example (your earlier std snippet looked like large values; paste full mean/std if you have them)\n",
    "# XMEAN = np.array([...], dtype=np.float32)\n",
    "# XSTD  = np.array([...], dtype=np.float32)\n",
    "\n",
    "def normalize_X(X: np.ndarray) -> np.ndarray:\n",
    "    std = np.where(XSTD == 0, 1.0, XSTD).astype(np.float32)\n",
    "    return (X.astype(np.float32) - XMEAN.astype(np.float32)) / std\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MODEL LOADING (robust for \"legacy\" ckpts)\n",
    "# ============================================================\n",
    "class PointNetLegacyAuto(nn.Module):\n",
    "    \"\"\"\n",
    "    Matches ckpt keys:\n",
    "      mlp1, mlp2, mlp3 (Conv1d)\n",
    "      fc1, fc2 (Conv1d for segmentation)\n",
    "    Outputs per-point logits: (B, N, C)\n",
    "    \"\"\"\n",
    "    def __init__(self, state, in_ch=10, force_num_classes=None):\n",
    "        super().__init__()\n",
    "\n",
    "        def conv_shape(name):\n",
    "            w = state[f\"{name}.weight\"]\n",
    "            if w.ndim != 3:\n",
    "                raise RuntimeError(f\"{name}.weight expected 3D Conv1d weight, got {tuple(w.shape)}\")\n",
    "            return int(w.shape[0]), int(w.shape[1])\n",
    "\n",
    "        o1, i1 = conv_shape(\"mlp1\")\n",
    "        o2, i2 = conv_shape(\"mlp2\")\n",
    "        o3, i3 = conv_shape(\"mlp3\")\n",
    "\n",
    "        if i1 != in_ch:\n",
    "            print(f\"⚠️ in_ch mismatch: ckpt expects {i1}, you set {in_ch}. Using ckpt value {i1}.\")\n",
    "            in_ch = i1\n",
    "\n",
    "        self.mlp1 = nn.Conv1d(in_ch, o1, 1, bias=True)\n",
    "        self.mlp2 = nn.Conv1d(o1,  o2, 1, bias=True)\n",
    "        self.mlp3 = nn.Conv1d(o2,  o3, 1, bias=True)\n",
    "\n",
    "        fc1_w = state[\"fc1.weight\"]\n",
    "        fc2_w = state[\"fc2.weight\"]\n",
    "\n",
    "        # segmentation expected (Conv1d)\n",
    "        if fc2_w.ndim != 3:\n",
    "            raise RuntimeError(\n",
    "                f\"Your ckpt looks like GLOBAL classification (fc2.weight is {tuple(fc2_w.shape)}). \"\n",
    "                f\"This script expects per-point segmentation ckpt (Conv1d head).\"\n",
    "            )\n",
    "\n",
    "        fc1_out, fc1_in = int(fc1_w.shape[0]), int(fc1_w.shape[1])\n",
    "        fc2_out, fc2_in = int(fc2_w.shape[0]), int(fc2_w.shape[1])\n",
    "\n",
    "        self.fc1 = nn.Conv1d(fc1_in, fc1_out, 1, bias=True)\n",
    "        outc = force_num_classes if force_num_classes is not None else fc2_out\n",
    "        self.fc2 = nn.Conv1d(fc2_in, outc, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,N,C)\n",
    "        x = x.transpose(1, 2)  # (B,C,N)\n",
    "        x = F.relu(self.mlp1(x))\n",
    "        x = F.relu(self.mlp2(x))\n",
    "        x = F.relu(self.mlp3(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)        # (B,C,N)\n",
    "        return x.transpose(1, 2)  # (B,N,C)\n",
    "\n",
    "\n",
    "def load_model(model_path: str, in_ch: int = 10, target_classes: int = 7) -> nn.Module:\n",
    "    ckpt = torch.load(model_path, map_location=DEVICE)\n",
    "    state = ckpt.get(\"model_state\", ckpt)\n",
    "\n",
    "    keys = list(state.keys())\n",
    "    print(\"CKPT keys sample:\", keys[:12])\n",
    "\n",
    "    # Legacy naming\n",
    "    if (\"mlp1.weight\" in state) and (\"fc2.weight\" in state):\n",
    "        ckpt_classes = int(state[\"fc2.weight\"].shape[0])\n",
    "        print(\"✅ Legacy ckpt detected. ckpt head classes =\", ckpt_classes)\n",
    "\n",
    "        if ckpt_classes != target_classes:\n",
    "            print(f\"⚠️ Head class mismatch: ckpt {ckpt_classes} vs target {target_classes}. \"\n",
    "                  f\"Will load backbone and replace head.\")\n",
    "            model = PointNetLegacyAuto(state, in_ch=in_ch, force_num_classes=target_classes).to(DEVICE)\n",
    "\n",
    "            # Load all but fc2.* (new head)\n",
    "            filtered = {k: v for k, v in state.items() if not k.startswith(\"fc2.\")}\n",
    "            missing, unexpected = model.load_state_dict(filtered, strict=False)\n",
    "            print(\"Loaded backbone. Missing:\", missing, \"Unexpected:\", unexpected)\n",
    "\n",
    "        else:\n",
    "            model = PointNetLegacyAuto(state, in_ch=in_ch).to(DEVICE)\n",
    "            model.load_state_dict(state, strict=True)\n",
    "            print(\"✅ Loaded strict=True.\")\n",
    "\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    raise RuntimeError(\n",
    "        \"Checkpoint format not recognized by this script.\\n\"\n",
    "        \"If your ckpt is from a different PointNet implementation (Sequential keys like mlp1.0.weight), \"\n",
    "        \"share the ckpt keys list and I’ll adapt the loader.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FEATURE BUILDING FROM LAZ\n",
    "# Your 10 features:\n",
    "# (x_local, y_local, z, hag, intensity, return_number, number_of_returns, scan_angle, deviation, slope)\n",
    "# ============================================================\n",
    "def get_dim_safe(las, name, default=0.0):\n",
    "    \"\"\"Return dimension array if exists, else a constant array.\"\"\"\n",
    "    try:\n",
    "        arr = getattr(las, name)\n",
    "        return np.asarray(arr)\n",
    "    except Exception:\n",
    "        try:\n",
    "            arr = las[name]  # extra dims\n",
    "            return np.asarray(arr)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def compute_hag_grid(x, y, z, cell=1.0, percentile=5):\n",
    "    \"\"\"\n",
    "    Simple HAG approximation:\n",
    "    - grid the XY plane\n",
    "    - estimate ground per cell as low percentile of Z\n",
    "    - HAG = Z - ground_z(cell)\n",
    "    \"\"\"\n",
    "    x0 = x.min()\n",
    "    y0 = y.min()\n",
    "    gx = np.floor((x - x0) / cell).astype(np.int32)\n",
    "    gy = np.floor((y - y0) / cell).astype(np.int32)\n",
    "\n",
    "    # Hash cell id\n",
    "    key = (gx.astype(np.int64) << 32) ^ gy.astype(np.int64)\n",
    "    order = np.argsort(key)\n",
    "    key_s = key[order]\n",
    "    z_s = z[order]\n",
    "\n",
    "    ground = np.empty_like(z_s, dtype=np.float32)\n",
    "\n",
    "    # iterate groups\n",
    "    start = 0\n",
    "    n = len(z_s)\n",
    "    while start < n:\n",
    "        end = start + 1\n",
    "        while end < n and key_s[end] == key_s[start]:\n",
    "            end += 1\n",
    "        z_cell = z_s[start:end]\n",
    "        g = np.percentile(z_cell, percentile).astype(np.float32)\n",
    "        ground[start:end] = g\n",
    "        start = end\n",
    "\n",
    "    # unsort\n",
    "    ground_unsorted = np.empty_like(ground)\n",
    "    ground_unsorted[order] = ground\n",
    "    hag = (z.astype(np.float32) - ground_unsorted.astype(np.float32))\n",
    "    # clamp negative small noise\n",
    "    hag = np.maximum(hag, 0.0)\n",
    "    return hag\n",
    "\n",
    "def compute_slope_from_hag(x_local, y_local, hag, cell=1.0):\n",
    "    \"\"\"\n",
    "    Approx slope proxy:\n",
    "    - rasterize hag min per cell then compute local gradient magnitude\n",
    "    For speed, we compute slope=0 if too heavy.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x0 = x_local.min()\n",
    "        y0 = y_local.min()\n",
    "        gx = np.floor((x_local - x0) / cell).astype(np.int32)\n",
    "        gy = np.floor((y_local - y0) / cell).astype(np.int32)\n",
    "\n",
    "        # build grid bounds\n",
    "        W = gx.max() + 1\n",
    "        H = gy.max() + 1\n",
    "        grid = np.full((H, W), np.nan, dtype=np.float32)\n",
    "\n",
    "        # fill with min hag per cell\n",
    "        for i in range(len(hag)):\n",
    "            yy, xx = gy[i], gx[i]\n",
    "            v = hag[i]\n",
    "            cur = grid[yy, xx]\n",
    "            if np.isnan(cur) or v < cur:\n",
    "                grid[yy, xx] = v\n",
    "\n",
    "        # replace nans with nearest-ish 0\n",
    "        grid = np.nan_to_num(grid, nan=0.0)\n",
    "\n",
    "        # gradient magnitude\n",
    "        gyg, gxg = np.gradient(grid)\n",
    "        mag = np.sqrt(gxg**2 + gyg**2).astype(np.float32)\n",
    "\n",
    "        slope = mag[gy, gx]\n",
    "        return slope\n",
    "    except Exception:\n",
    "        return np.zeros_like(hag, dtype=np.float32)\n",
    "\n",
    "\n",
    "def build_features_from_las(las):\n",
    "    \"\"\"\n",
    "    Returns X: (N,10) float32 and xyz_local stats.\n",
    "    \"\"\"\n",
    "    x = np.asarray(las.x, dtype=np.float64)\n",
    "    y = np.asarray(las.y, dtype=np.float64)\n",
    "    z = np.asarray(las.z, dtype=np.float32)\n",
    "\n",
    "    # Local coordinates (per file)\n",
    "    x_local = (x - x.min()).astype(np.float32)\n",
    "    y_local = (y - y.min()).astype(np.float32)\n",
    "\n",
    "    # Intensity / returns / scan angle (if present)\n",
    "    intensity = get_dim_safe(las, \"intensity\")\n",
    "    if intensity is None:\n",
    "        intensity = np.zeros_like(z, dtype=np.float32)\n",
    "    else:\n",
    "        intensity = intensity.astype(np.float32)\n",
    "\n",
    "    return_num = get_dim_safe(las, \"return_number\")\n",
    "    if return_num is None:\n",
    "        return_num = np.zeros_like(z, dtype=np.float32)\n",
    "    else:\n",
    "        return_num = return_num.astype(np.float32)\n",
    "\n",
    "    num_returns = get_dim_safe(las, \"number_of_returns\")\n",
    "    if num_returns is None:\n",
    "        num_returns = np.zeros_like(z, dtype=np.float32)\n",
    "    else:\n",
    "        num_returns = num_returns.astype(np.float32)\n",
    "\n",
    "    scan_angle = get_dim_safe(las, \"scan_angle_rank\")\n",
    "    if scan_angle is None:\n",
    "        scan_angle = get_dim_safe(las, \"scan_angle\")\n",
    "    if scan_angle is None:\n",
    "        scan_angle = np.zeros_like(z, dtype=np.float32)\n",
    "    else:\n",
    "        scan_angle = scan_angle.astype(np.float32)\n",
    "\n",
    "    # deviation (often not standard) -> 0 if missing\n",
    "    deviation = get_dim_safe(las, \"deviation\")\n",
    "    if deviation is None:\n",
    "        deviation = np.zeros_like(z, dtype=np.float32)\n",
    "    else:\n",
    "        deviation = deviation.astype(np.float32)\n",
    "\n",
    "    # HAG (approx)\n",
    "    hag = compute_hag_grid(x.astype(np.float32), y.astype(np.float32), z.astype(np.float32), cell=1.0, percentile=5)\n",
    "\n",
    "    # slope (proxy)\n",
    "    slope = compute_slope_from_hag(x_local, y_local, hag, cell=1.0)\n",
    "\n",
    "    X = np.stack([\n",
    "        x_local,          # 0\n",
    "        y_local,          # 1\n",
    "        z.astype(np.float32),  # 2\n",
    "        hag.astype(np.float32),# 3\n",
    "        intensity,        # 4\n",
    "        return_num,       # 5\n",
    "        num_returns,      # 6\n",
    "        scan_angle,       # 7\n",
    "        deviation,        # 8\n",
    "        slope             # 9\n",
    "    ], axis=1).astype(np.float32)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# BLOCKING (4096 points per block with idx mapping)\n",
    "# ============================================================\n",
    "def make_blocks(X: np.ndarray, block_size=40.0, points_per_block=4096, seed=123):\n",
    "    \"\"\"\n",
    "    Creates blocks by XY grid on x_local/y_local.\n",
    "    - For each cell, sample points_per_block (with replacement if needed).\n",
    "    Returns list of (X_block, idx_block)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    x = X[:, 0]  # x_local\n",
    "    y = X[:, 1]  # y_local\n",
    "    gx = np.floor(x / block_size).astype(np.int32)\n",
    "    gy = np.floor(y / block_size).astype(np.int32)\n",
    "\n",
    "    key = gx.astype(np.int64) * 10_000_000 + gy.astype(np.int64)\n",
    "    order = np.argsort(key)\n",
    "    key_s = key[order]\n",
    "\n",
    "    blocks = []\n",
    "    start = 0\n",
    "    n = len(X)\n",
    "\n",
    "    while start < n:\n",
    "        end = start + 1\n",
    "        while end < n and key_s[end] == key_s[start]:\n",
    "            end += 1\n",
    "\n",
    "        idxs = order[start:end]\n",
    "        if idxs.size == 0:\n",
    "            start = end\n",
    "            continue\n",
    "\n",
    "        # sample fixed size\n",
    "        if idxs.size >= points_per_block:\n",
    "            pick = rng.choice(idxs, size=points_per_block, replace=False)\n",
    "        else:\n",
    "            pick = rng.choice(idxs, size=points_per_block, replace=True)\n",
    "\n",
    "        Xb = X[pick]\n",
    "        blocks.append((Xb, pick.astype(np.int64)))\n",
    "\n",
    "        start = end\n",
    "\n",
    "    return blocks\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# INFERENCE + WRITE BACK\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def predict_full(model, X_full: np.ndarray, block_size=40.0, points_per_block=4096, batch_blocks=8):\n",
    "    \"\"\"\n",
    "    Predict for all points by blocks.\n",
    "    Writes predictions back to a full array using idx mapping.\n",
    "    If points appear in multiple blocks (due to replacement), we vote by counts.\n",
    "    \"\"\"\n",
    "    blocks = make_blocks(X_full, block_size=block_size, points_per_block=points_per_block)\n",
    "    print(f\"Blocks created: {len(blocks)} (block_size={block_size}, N={points_per_block})\")\n",
    "\n",
    "    # vote accumulators\n",
    "    N = X_full.shape[0]\n",
    "    votes = np.zeros((N, NUM_CLASSES), dtype=np.uint16)\n",
    "\n",
    "    # batch blocks\n",
    "    for i in range(0, len(blocks), batch_blocks):\n",
    "        chunk = blocks[i:i+batch_blocks]\n",
    "        Xb = np.stack([c[0] for c in chunk], axis=0)  # (B,4096,10)\n",
    "        idxb = [c[1] for c in chunk]\n",
    "\n",
    "        # normalize\n",
    "        Xb = normalize_X(Xb)\n",
    "\n",
    "        xb_t = torch.from_numpy(Xb).to(DEVICE)\n",
    "        logits = model(xb_t)  # (B,4096,C)\n",
    "        pred = logits.argmax(dim=-1).detach().cpu().numpy().astype(np.int32)  # (B,4096)\n",
    "\n",
    "        # accumulate votes\n",
    "        for b in range(pred.shape[0]):\n",
    "            inds = idxb[b]\n",
    "            pb = pred[b]\n",
    "            # vote\n",
    "            for k in range(NUM_CLASSES):\n",
    "                votes[inds, k] += (pb == k).astype(np.uint16)\n",
    "\n",
    "        if (i // batch_blocks) % 10 == 0:\n",
    "            print(f\"Predicted blocks {i}/{len(blocks)}\")\n",
    "\n",
    "    final = votes.argmax(axis=1).astype(np.int32)  # 0..6\n",
    "    return final\n",
    "\n",
    "\n",
    "def write_outputs(las, pred_contig: np.ndarray, out_las: str, out_laz: str):\n",
    "    # Map 0..6 -> raw LAS codes\n",
    "    pred_raw = np.vectorize(CONTIG_TO_RAW.get)(pred_contig).astype(np.uint8)\n",
    "\n",
    "    # Set classification\n",
    "    las.classification = pred_raw\n",
    "\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(os.path.dirname(out_las), exist_ok=True)\n",
    "\n",
    "    # Write LAS\n",
    "    las.write(out_las)\n",
    "    print(\"✅ Wrote LAS:\", out_las)\n",
    "\n",
    "    # Write LAZ (requires laz backend: lazrs or laszip)\n",
    "    try:\n",
    "        las.write(out_laz)\n",
    "        print(\"✅ Wrote LAZ:\", out_laz)\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Could not write LAZ (backend missing). Error:\", str(e))\n",
    "        print(\"   Install one of these in your env:\")\n",
    "        print(\"   pip install lazrs\")\n",
    "        print(\"   (then retry writing .laz)\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "def main():\n",
    "    print(\"Device:\", DEVICE)\n",
    "    if DEVICE == \"cuda\":\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "    # Load LAZ\n",
    "    las = laspy.read(IN_LAZ)\n",
    "    N = len(las.x)\n",
    "    print(\"Loaded:\", IN_LAZ)\n",
    "    print(\"Points:\", N)\n",
    "\n",
    "    # Build features\n",
    "    X_full = build_features_from_las(las)\n",
    "    print(\"X_full shape:\", X_full.shape)\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(MODEL_PT, in_ch=10, target_classes=NUM_CLASSES)\n",
    "\n",
    "    # Predict\n",
    "    pred_contig = predict_full(\n",
    "        model,\n",
    "        X_full,\n",
    "        block_size=40.0,       # change if your data uses different scale\n",
    "        points_per_block=4096,\n",
    "        batch_blocks=8         # RTX 3050 safe\n",
    "    )\n",
    "\n",
    "    # Write outputs\n",
    "    write_outputs(las, pred_contig, OUT_LAS, OUT_LAZ)\n",
    "\n",
    "    # Quick counts\n",
    "    unique, counts = np.unique(pred_contig, return_counts=True)\n",
    "    print(\"Pred counts (contig 0..6):\", dict(zip(unique.tolist(), counts.tolist())))\n",
    "    unique_raw, counts_raw = np.unique(np.vectorize(CONTIG_TO_RAW.get)(pred_contig), return_counts=True)\n",
    "    print(\"Pred counts (raw classes):\", dict(zip(unique_raw.tolist(), counts_raw.tolist())))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8699aa5-2178-4d22-8e26-f72c3eb0d934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n",
      "Loaded: D:/lidarrrrr/anbu/DX3035724 S.GIUSTO000001.laz\n",
      "Points: 12374846\n",
      "X_full shape: (12374846, 10)\n",
      "✅ ckpt head classes = 5\n",
      "Blocks created: 3222 (block_size=20.0, N=4096)\n",
      "Predicted blocks 0/3222\n",
      "Predicted blocks 80/3222\n",
      "Predicted blocks 160/3222\n",
      "Predicted blocks 240/3222\n",
      "Predicted blocks 320/3222\n",
      "Predicted blocks 400/3222\n",
      "Predicted blocks 480/3222\n",
      "Predicted blocks 560/3222\n",
      "Predicted blocks 640/3222\n",
      "Predicted blocks 720/3222\n",
      "Predicted blocks 800/3222\n",
      "Predicted blocks 880/3222\n",
      "Predicted blocks 960/3222\n",
      "Predicted blocks 1040/3222\n",
      "Predicted blocks 1120/3222\n",
      "Predicted blocks 1200/3222\n",
      "Predicted blocks 1280/3222\n",
      "Predicted blocks 1360/3222\n",
      "Predicted blocks 1440/3222\n",
      "Predicted blocks 1520/3222\n",
      "Predicted blocks 1600/3222\n",
      "Predicted blocks 1680/3222\n",
      "Predicted blocks 1760/3222\n",
      "Predicted blocks 1840/3222\n",
      "Predicted blocks 1920/3222\n",
      "Predicted blocks 2000/3222\n",
      "Predicted blocks 2080/3222\n",
      "Predicted blocks 2160/3222\n",
      "Predicted blocks 2240/3222\n",
      "Predicted blocks 2320/3222\n",
      "Predicted blocks 2400/3222\n",
      "Predicted blocks 2480/3222\n",
      "Predicted blocks 2560/3222\n",
      "Predicted blocks 2640/3222\n",
      "Predicted blocks 2720/3222\n",
      "Predicted blocks 2800/3222\n",
      "Predicted blocks 2880/3222\n",
      "Predicted blocks 2960/3222\n",
      "Predicted blocks 3040/3222\n",
      "Predicted blocks 3120/3222\n",
      "Predicted blocks 3200/3222\n",
      "✅ Wrote LAS: D:/lidarrrrr/anbu/New folder/dl_predicted_fixed_5class.las\n",
      "✅ Wrote LAZ: D:/lidarrrrr/anbu/New folder/dl_predicted_fixed_5class.laz\n",
      "Pred counts (contig): {0: 1, 1: 7185068, 2: 33809, 3: 1274169, 4: 3881799}\n",
      "Pred counts (raw): {1: 1, 2: 7185068, 3: 33809, 6: 1274169, 12: 3881799}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import laspy\n",
    "\n",
    "# ============================================================\n",
    "# USER PATHS\n",
    "# ============================================================\n",
    "IN_LAZ   = r\"D:/lidarrrrr/anbu/DX3035724 S.GIUSTO000001.laz\"\n",
    "MODEL_PT = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"   # 5-class ckpt\n",
    "OUT_LAS  = r\"D:/lidarrrrr/anbu/New folder/dl_predicted_fixed_5class.las\"\n",
    "OUT_LAZ  = r\"D:/lidarrrrr/anbu/New folder/dl_predicted_fixed_5class.laz\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ============================================================\n",
    "# IMPORTANT: CKPT TRAINED CLASSES (5 classes)\n",
    "# Most common 5-class training set for LiDAR:\n",
    "#   [1,2,3,6,12]\n",
    "# If your training classes were different, change this list.\n",
    "# Examples:\n",
    "#   [1,2,3,12,13]\n",
    "#   [1,2,3,6,13]\n",
    "# ============================================================\n",
    "CKPT_RAW_CLASSES = [1, 2, 3, 6, 12]   # <-- DEFAULT (change if needed)\n",
    "NUM_CLASSES = len(CKPT_RAW_CLASSES)\n",
    "CONTIG_TO_RAW = {i: c for i, c in enumerate(CKPT_RAW_CLASSES)}\n",
    "\n",
    "# ============================================================\n",
    "# NORMALIZATION STATS\n",
    "# If you have training mean/std for 10 features, paste them.\n",
    "# If not, keep identity (may reduce accuracy but will still run).\n",
    "# ============================================================\n",
    "XMEAN = np.zeros(10, dtype=np.float32)\n",
    "XSTD  = np.ones(10, dtype=np.float32)\n",
    "\n",
    "def normalize_X(X: np.ndarray) -> np.ndarray:\n",
    "    std = np.where(XSTD == 0, 1.0, XSTD).astype(np.float32)\n",
    "    return (X.astype(np.float32) - XMEAN.astype(np.float32)) / std\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Legacy PointNet model matching your ckpt keys exactly\n",
    "# keys: mlp1/2/3, fc1, fc2 (Conv1d => per-point segmentation)\n",
    "# ============================================================\n",
    "class PointNetLegacySeg(nn.Module):\n",
    "    def __init__(self, state, in_ch=10):\n",
    "        super().__init__()\n",
    "\n",
    "        def get_conv(name):\n",
    "            w = state[f\"{name}.weight\"]\n",
    "            if w.ndim != 3:\n",
    "                raise RuntimeError(f\"{name}.weight must be Conv1d (3D), got {tuple(w.shape)}\")\n",
    "            out_ch, in_ch_ = int(w.shape[0]), int(w.shape[1])\n",
    "            layer = nn.Conv1d(in_ch_, out_ch, 1, bias=True)\n",
    "            return layer, in_ch_, out_ch\n",
    "\n",
    "        self.mlp1, in1, o1 = get_conv(\"mlp1\")\n",
    "        self.mlp2, in2, o2 = get_conv(\"mlp2\")\n",
    "        self.mlp3, in3, o3 = get_conv(\"mlp3\")\n",
    "\n",
    "        # fc1/fc2 also Conv1d for segmentation\n",
    "        w1 = state[\"fc1.weight\"]\n",
    "        w2 = state[\"fc2.weight\"]\n",
    "        if w2.ndim != 3:\n",
    "            raise RuntimeError(\n",
    "                f\"fc2.weight is {tuple(w2.shape)} => looks like global classifier. \"\n",
    "                f\"This script expects per-point segmentation ckpt.\"\n",
    "            )\n",
    "\n",
    "        fc1_out, fc1_in = int(w1.shape[0]), int(w1.shape[1])\n",
    "        fc2_out, fc2_in = int(w2.shape[0]), int(w2.shape[1])\n",
    "\n",
    "        self.fc1 = nn.Conv1d(fc1_in, fc1_out, 1, bias=True)\n",
    "        self.fc2 = nn.Conv1d(fc2_in, fc2_out, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,N,C)\n",
    "        x = x.transpose(1, 2)        # (B,C,N)\n",
    "        x = F.relu(self.mlp1(x))\n",
    "        x = F.relu(self.mlp2(x))\n",
    "        x = F.relu(self.mlp3(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)              # (B,C,N)\n",
    "        return x.transpose(1, 2)     # (B,N,C)\n",
    "\n",
    "def load_model_strict_5class(model_path: str) -> nn.Module:\n",
    "    ckpt = torch.load(model_path, map_location=DEVICE)\n",
    "    state = ckpt.get(\"model_state\", ckpt)\n",
    "\n",
    "    # Verify ckpt classes\n",
    "    ckpt_classes = int(state[\"fc2.weight\"].shape[0])\n",
    "    print(\"✅ ckpt head classes =\", ckpt_classes)\n",
    "    if ckpt_classes != NUM_CLASSES:\n",
    "        raise RuntimeError(\n",
    "            f\"Your CKPT outputs {ckpt_classes} classes but CKPT_RAW_CLASSES has {NUM_CLASSES}.\\n\"\n",
    "            f\"Fix CKPT_RAW_CLASSES to match ckpt output classes.\"\n",
    "        )\n",
    "\n",
    "    model = PointNetLegacySeg(state, in_ch=10).to(DEVICE)\n",
    "    model.load_state_dict(state, strict=True)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Feature building (same 10 features names)\n",
    "# NOTE: If you trained with a different HAG/slope pipeline,\n",
    "#       you should plug your training feature builder here.\n",
    "# ============================================================\n",
    "def get_dim_safe(las, name):\n",
    "    try:\n",
    "        return np.asarray(getattr(las, name))\n",
    "    except Exception:\n",
    "        try:\n",
    "            return np.asarray(las[name])  # extra dims\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def compute_hag_grid(x, y, z, cell=1.0, percentile=5):\n",
    "    x0 = x.min()\n",
    "    y0 = y.min()\n",
    "    gx = np.floor((x - x0) / cell).astype(np.int32)\n",
    "    gy = np.floor((y - y0) / cell).astype(np.int32)\n",
    "    key = (gx.astype(np.int64) << 32) ^ gy.astype(np.int64)\n",
    "\n",
    "    order = np.argsort(key)\n",
    "    key_s = key[order]\n",
    "    z_s = z[order]\n",
    "\n",
    "    ground = np.empty_like(z_s, dtype=np.float32)\n",
    "    start = 0\n",
    "    n = len(z_s)\n",
    "    while start < n:\n",
    "        end = start + 1\n",
    "        while end < n and key_s[end] == key_s[start]:\n",
    "            end += 1\n",
    "        g = np.percentile(z_s[start:end], percentile).astype(np.float32)\n",
    "        ground[start:end] = g\n",
    "        start = end\n",
    "\n",
    "    ground_unsorted = np.empty_like(ground)\n",
    "    ground_unsorted[order] = ground\n",
    "\n",
    "    hag = (z.astype(np.float32) - ground_unsorted.astype(np.float32))\n",
    "    hag = np.maximum(hag, 0.0)\n",
    "    return hag\n",
    "\n",
    "def compute_slope_proxy(x_local, y_local, hag, cell=1.0):\n",
    "    # fast/simple: return zeros if you don’t have training-equivalent slope\n",
    "    return np.zeros_like(hag, dtype=np.float32)\n",
    "\n",
    "def build_features_from_las(las):\n",
    "    x = np.asarray(las.x, dtype=np.float64)\n",
    "    y = np.asarray(las.y, dtype=np.float64)\n",
    "    z = np.asarray(las.z, dtype=np.float32)\n",
    "\n",
    "    x_local = (x - x.min()).astype(np.float32)\n",
    "    y_local = (y - y.min()).astype(np.float32)\n",
    "\n",
    "    intensity = get_dim_safe(las, \"intensity\")\n",
    "    if intensity is None: intensity = np.zeros_like(z, dtype=np.float32)\n",
    "    else: intensity = intensity.astype(np.float32)\n",
    "\n",
    "    return_num = get_dim_safe(las, \"return_number\")\n",
    "    if return_num is None: return_num = np.zeros_like(z, dtype=np.float32)\n",
    "    else: return_num = return_num.astype(np.float32)\n",
    "\n",
    "    num_returns = get_dim_safe(las, \"number_of_returns\")\n",
    "    if num_returns is None: num_returns = np.zeros_like(z, dtype=np.float32)\n",
    "    else: num_returns = num_returns.astype(np.float32)\n",
    "\n",
    "    scan_angle = get_dim_safe(las, \"scan_angle_rank\")\n",
    "    if scan_angle is None:\n",
    "        scan_angle = get_dim_safe(las, \"scan_angle\")\n",
    "    if scan_angle is None: scan_angle = np.zeros_like(z, dtype=np.float32)\n",
    "    else: scan_angle = scan_angle.astype(np.float32)\n",
    "\n",
    "    deviation = get_dim_safe(las, \"deviation\")\n",
    "    if deviation is None: deviation = np.zeros_like(z, dtype=np.float32)\n",
    "    else: deviation = deviation.astype(np.float32)\n",
    "\n",
    "    hag = compute_hag_grid(x.astype(np.float32), y.astype(np.float32), z.astype(np.float32), cell=1.0, percentile=5)\n",
    "    slope = compute_slope_proxy(x_local, y_local, hag, cell=1.0)\n",
    "\n",
    "    X = np.stack([\n",
    "        x_local, y_local, z, hag,\n",
    "        intensity, return_num, num_returns, scan_angle,\n",
    "        deviation, slope\n",
    "    ], axis=1).astype(np.float32)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Better blocking: grid cells -> split into chunks of 4096\n",
    "# (covers all points, minimal replacement)\n",
    "# ============================================================\n",
    "def build_blocks_cover_all(X, block_size=20.0, points_per_block=4096):\n",
    "    x = X[:, 0]  # x_local\n",
    "    y = X[:, 1]  # y_local\n",
    "\n",
    "    gx = np.floor(x / block_size).astype(np.int32)\n",
    "    gy = np.floor(y / block_size).astype(np.int32)\n",
    "    key = gx.astype(np.int64) * 10_000_000 + gy.astype(np.int64)\n",
    "\n",
    "    order = np.argsort(key)\n",
    "    key_s = key[order]\n",
    "\n",
    "    blocks = []\n",
    "    start = 0\n",
    "    n = len(X)\n",
    "    rng = np.random.default_rng(123)\n",
    "\n",
    "    while start < n:\n",
    "        end = start + 1\n",
    "        while end < n and key_s[end] == key_s[start]:\n",
    "            end += 1\n",
    "\n",
    "        cell_indices = order[start:end]\n",
    "        # split this cell into multiple 4096 chunks\n",
    "        if cell_indices.size > 0:\n",
    "            rng.shuffle(cell_indices)\n",
    "            for s in range(0, cell_indices.size, points_per_block):\n",
    "                chunk = cell_indices[s:s+points_per_block]\n",
    "                if chunk.size < points_per_block:\n",
    "                    # pad by sampling within the same cell\n",
    "                    pad = rng.choice(cell_indices, size=(points_per_block - chunk.size), replace=True)\n",
    "                    chunk = np.concatenate([chunk, pad], axis=0)\n",
    "                blocks.append(chunk.astype(np.int64))\n",
    "\n",
    "        start = end\n",
    "\n",
    "    return blocks\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_points(model, X_full, block_size=20.0, points_per_block=4096, batch_blocks=8):\n",
    "    blocks = build_blocks_cover_all(X_full, block_size=block_size, points_per_block=points_per_block)\n",
    "    print(f\"Blocks created: {len(blocks)} (block_size={block_size}, N={points_per_block})\")\n",
    "\n",
    "    N = X_full.shape[0]\n",
    "    vote = np.zeros((N, NUM_CLASSES), dtype=np.uint16)\n",
    "\n",
    "    for i in range(0, len(blocks), batch_blocks):\n",
    "        batch = blocks[i:i+batch_blocks]\n",
    "        Xb = np.stack([X_full[idx] for idx in batch], axis=0)  # (B,4096,10)\n",
    "        Xb = normalize_X(Xb)\n",
    "\n",
    "        xb = torch.from_numpy(Xb).to(DEVICE)\n",
    "        logits = model(xb)  # (B,4096,C)\n",
    "        pred = logits.argmax(dim=-1).cpu().numpy().astype(np.int32)\n",
    "\n",
    "        for b, idxs in enumerate(batch):\n",
    "            pb = pred[b]\n",
    "            # vote per point\n",
    "            for c in range(NUM_CLASSES):\n",
    "                vote[idxs, c] += (pb == c).astype(np.uint16)\n",
    "\n",
    "        if (i // batch_blocks) % 10 == 0:\n",
    "            print(f\"Predicted blocks {i}/{len(blocks)}\")\n",
    "\n",
    "    pred_contig = vote.argmax(axis=1).astype(np.int32)  # 0..4\n",
    "    return pred_contig\n",
    "\n",
    "\n",
    "def write_outputs(las, pred_contig, out_las, out_laz):\n",
    "    pred_raw = np.vectorize(CONTIG_TO_RAW.get)(pred_contig).astype(np.uint8)\n",
    "    las.classification = pred_raw\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_las), exist_ok=True)\n",
    "\n",
    "    las.write(out_las)\n",
    "    print(\"✅ Wrote LAS:\", out_las)\n",
    "\n",
    "    try:\n",
    "        las.write(out_laz)\n",
    "        print(\"✅ Wrote LAZ:\", out_laz)\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Could not write LAZ:\", e)\n",
    "        print(\"   Try: pip install lazrs\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Device:\", DEVICE)\n",
    "    if DEVICE == \"cuda\":\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "    las = laspy.read(IN_LAZ)\n",
    "    print(\"Loaded:\", IN_LAZ)\n",
    "    print(\"Points:\", len(las.x))\n",
    "\n",
    "    X_full = build_features_from_las(las)\n",
    "    print(\"X_full shape:\", X_full.shape)\n",
    "\n",
    "    model = load_model_strict_5class(MODEL_PT)\n",
    "\n",
    "    # IMPORTANT: if your earlier run made too few blocks, reduce block_size:\n",
    "    # try 20.0, 10.0, 5.0 based on your x_local/y_local scale.\n",
    "    pred_contig = predict_points(\n",
    "        model,\n",
    "        X_full,\n",
    "        block_size=20.0,        # <-- try 10.0 if still too few blocks\n",
    "        points_per_block=4096,\n",
    "        batch_blocks=8\n",
    "    )\n",
    "\n",
    "    write_outputs(las, pred_contig, OUT_LAS, OUT_LAZ)\n",
    "\n",
    "    u, c = np.unique(pred_contig, return_counts=True)\n",
    "    print(\"Pred counts (contig):\", dict(zip(u.tolist(), c.tolist())))\n",
    "    u2, c2 = np.unique(np.vectorize(CONTIG_TO_RAW.get)(pred_contig), return_counts=True)\n",
    "    print(\"Pred counts (raw):\", dict(zip(u2.tolist(), c2.tolist())))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0b3d56b-21e9-430f-8c00-2eddc1b4fcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = list_npz(TRAIN_DIR)\n",
    "val_files   = list_npz(VAL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e94067fd-1bb2-4e14-a0f7-fb721115584a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train blocks: 22157\n",
      "Val blocks: 5540\n"
     ]
    }
   ],
   "source": [
    "ALL_BLOCKS_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"   # folder that contains ALL npz files\n",
    "\n",
    "all_files = list_npz(ALL_BLOCKS_DIR)\n",
    "\n",
    "# shuffle\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(all_files)\n",
    "\n",
    "# split 80/20\n",
    "split = int(0.8 * len(all_files))\n",
    "train_files = all_files[:split]\n",
    "val_files   = all_files[split:]\n",
    "\n",
    "print(\"Train blocks:\", len(train_files))\n",
    "print(\"Val blocks:\", len(val_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d314247-c969-4256-aa06-d5016c296566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n",
      "Train blocks: 0\n",
      "Val blocks  : 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No train .npz files found. Check TRAIN_DIR.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 554\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone. Best macroF1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_f1)\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 554\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 494\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal blocks  :\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(val_files))\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(train_files) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo train .npz files found. Check TRAIN_DIR.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    496\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m BlocksNPZDataset(train_files)\n\u001b[0;32m    497\u001b[0m val_ds   \u001b[38;5;241m=\u001b[39m BlocksNPZDataset(val_files)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No train .npz files found. Check TRAIN_DIR."
     ]
    }
   ],
   "source": [
    "import os, glob, math, random, time\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "\n",
    "# =========================\n",
    "# CONFIG (EDIT THESE)\n",
    "# =========================\n",
    "\n",
    "# ✅ Start with 5-class (matches your working ckpt mapping):\n",
    "# CKPT_RAW_CLASSES = [1,2,3,6,12]\n",
    "# Later switch to 7-class:\n",
    "# CKPT_RAW_CLASSES = [1,2,3,6,7,12,13]\n",
    "\n",
    "RAW_CLASSES = [1, 2, 3, 6, 12]  # <-- START HERE (5-class). Change to 7-class when ready.\n",
    "MAP = {c: i for i, c in enumerate(RAW_CLASSES)}\n",
    "NUM_CLASSES = len(RAW_CLASSES)\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "# Paths\n",
    "TRAIN_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/train_blocks\"\n",
    "VAL_DIR   = r\"D:/lidarrrrr/anbu/dl_dataset/val_blocks\"\n",
    "OUT_DIR   = r\"D:/lidarrrrr/anbu/dl_models\"\n",
    "        # <-- change\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Data\n",
    "POINTS = 4096\n",
    "IN_CH = 10\n",
    "BATCH = 6              # RTX 3050 safe with PointNet++\n",
    "NUM_WORKERS = 0        # Windows safe; try 2 if stable\n",
    "\n",
    "# Training\n",
    "EPOCHS = 120\n",
    "LR = 1e-3\n",
    "WD = 1e-4\n",
    "LABEL_SMOOTH = 0.05\n",
    "GRAD_CLIP = 1.0\n",
    "USE_AMP = True\n",
    "\n",
    "# Sampling buckets (raw label codes!)\n",
    "RARE_RAW = {7, 13, 6}     # works for 7-class; for 5-class it mainly hits 6\n",
    "MED_RAW  = {12, 3}\n",
    "RATIOS   = (0.50, 0.30, 0.20)  # rare, medium, common\n",
    "\n",
    "# PointNet++ sizes\n",
    "NPOINTS = [1024, 256, 64]\n",
    "NSAMPLE = [32, 32, 32]        # kNN neighbors\n",
    "\n",
    "# =========================\n",
    "# UTILS\n",
    "# =========================\n",
    "\n",
    "def set_seed(seed=123):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(123)\n",
    "\n",
    "def list_npz(folder: str) -> List[str]:\n",
    "    return sorted(glob.glob(os.path.join(folder, \"*.npz\")))\n",
    "\n",
    "def map_labels(y_raw: np.ndarray) -> np.ndarray:\n",
    "    out = np.full_like(y_raw, IGNORE_INDEX)\n",
    "    for raw, new in MAP.items():\n",
    "        out[y_raw == raw] = new\n",
    "    return out\n",
    "\n",
    "def compute_class_weights_from_counts(counts_raw: Dict[int, int]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    weight = 1/sqrt(freq), normalize mean~1, clamp [0.25, 10]\n",
    "    \"\"\"\n",
    "    freqs = []\n",
    "    for raw in RAW_CLASSES:\n",
    "        freqs.append(float(counts_raw.get(raw, 1)))\n",
    "    freqs = np.array(freqs, dtype=np.float64)\n",
    "\n",
    "    w = 1.0 / np.sqrt(freqs)\n",
    "    w = w / (w.mean() + 1e-12)\n",
    "    w = np.clip(w, 0.25, 10.0).astype(np.float32)\n",
    "    return torch.from_numpy(w)\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "\n",
    "class BlocksNPZDataset(Dataset):\n",
    "    def __init__(self, files: List[str]):\n",
    "        self.files = files\n",
    "        self.block_has = []\n",
    "\n",
    "        for f in self.files:\n",
    "            d = np.load(f)\n",
    "            y_raw = d[\"y\"].astype(np.int32)\n",
    "            self.block_has.append(set(np.unique(y_raw).tolist()))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        path = self.files[i]\n",
    "        d = np.load(path)\n",
    "        X = d[\"X\"].astype(np.float32)      # (4096, 10)\n",
    "        y_raw = d[\"y\"].astype(np.int32)    # (4096,)\n",
    "        idx = d[\"idx\"].astype(np.int64)    # (4096,)\n",
    "\n",
    "        y = map_labels(y_raw)\n",
    "\n",
    "        return torch.from_numpy(X), torch.from_numpy(y), torch.from_numpy(idx), path\n",
    "\n",
    "class BucketBlockSampler(Sampler[int]):\n",
    "    def __init__(self, dataset: BlocksNPZDataset, num_samples: Optional[int] = None, seed: int = 123):\n",
    "        self.ds = dataset\n",
    "        self.num_samples = num_samples if num_samples is not None else len(dataset)\n",
    "        self.seed = seed\n",
    "\n",
    "        self.rare_idx, self.med_idx, self.common_idx = self._split()\n",
    "\n",
    "        if len(self.rare_idx) == 0:\n",
    "            print(\"⚠️ Rare bucket empty.\")\n",
    "        if len(self.med_idx) == 0:\n",
    "            print(\"⚠️ Medium bucket empty.\")\n",
    "        if len(self.common_idx) == 0:\n",
    "            print(\"⚠️ Common bucket empty.\")\n",
    "\n",
    "    def _split(self):\n",
    "        rare, med, common = [], [], []\n",
    "        for i, present in enumerate(self.ds.block_has):\n",
    "            if len(present & RARE_RAW) > 0:\n",
    "                rare.append(i)\n",
    "            elif len(present & MED_RAW) > 0:\n",
    "                med.append(i)\n",
    "            else:\n",
    "                common.append(i)\n",
    "        return rare, med, common\n",
    "\n",
    "    def __iter__(self):\n",
    "        rng = random.Random(self.seed)\n",
    "        n = self.num_samples\n",
    "        n_rare = int(n * RATIOS[0])\n",
    "        n_med  = int(n * RATIOS[1])\n",
    "        n_com  = n - n_rare - n_med\n",
    "\n",
    "        def pick(bucket, k):\n",
    "            if len(bucket) == 0:\n",
    "                return []\n",
    "            return [rng.choice(bucket) for _ in range(k)]\n",
    "\n",
    "        indices = []\n",
    "        indices += pick(self.rare_idx, n_rare)\n",
    "        indices += pick(self.med_idx, n_med)\n",
    "        indices += pick(self.common_idx, n_com)\n",
    "        rng.shuffle(indices)\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "# =========================\n",
    "# POINTNET++ CORE OPS (FPS + kNN)\n",
    "# =========================\n",
    "\n",
    "def square_distance(src: torch.Tensor, dst: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    src: (B, N, 3), dst: (B, M, 3) -> (B, N, M)\n",
    "    \"\"\"\n",
    "    B, N, _ = src.shape\n",
    "    _, M, _ = dst.shape\n",
    "    dist = -2 * torch.matmul(src, dst.transpose(1, 2))  # (B,N,M)\n",
    "    dist += torch.sum(src ** 2, dim=-1).unsqueeze(-1)\n",
    "    dist += torch.sum(dst ** 2, dim=-1).unsqueeze(1)\n",
    "    return dist\n",
    "\n",
    "def index_points(points: torch.Tensor, idx: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    points: (B, N, C)\n",
    "    idx: (B, S) or (B, S, K)\n",
    "    \"\"\"\n",
    "    B = points.shape[0]\n",
    "    if idx.dim() == 2:\n",
    "        batch_indices = torch.arange(B, device=points.device).view(B, 1)\n",
    "        return points[batch_indices, idx, :]\n",
    "    else:\n",
    "        batch_indices = torch.arange(B, device=points.device).view(B, 1, 1)\n",
    "        return points[batch_indices, idx, :]\n",
    "\n",
    "def farthest_point_sample(xyz: torch.Tensor, npoint: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    xyz: (B, N, 3) -> idx (B, npoint)\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, _ = xyz.shape\n",
    "    centroids = torch.zeros(B, npoint, dtype=torch.long, device=device)\n",
    "    distance = torch.full((B, N), 1e10, device=device)\n",
    "    farthest = torch.randint(0, N, (B,), dtype=torch.long, device=device)\n",
    "    batch_indices = torch.arange(B, device=device)\n",
    "\n",
    "    for i in range(npoint):\n",
    "        centroids[:, i] = farthest\n",
    "        centroid = xyz[batch_indices, farthest, :].view(B, 1, 3)\n",
    "        dist = torch.sum((xyz - centroid) ** 2, dim=-1)\n",
    "        distance = torch.minimum(distance, dist)\n",
    "        farthest = torch.max(distance, dim=-1)[1]\n",
    "    return centroids\n",
    "\n",
    "def knn_point(k: int, xyz: torch.Tensor, new_xyz: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    xyz: (B, N, 3)\n",
    "    new_xyz: (B, S, 3)\n",
    "    return idx: (B, S, k)\n",
    "    \"\"\"\n",
    "    dist = square_distance(new_xyz, xyz)  # (B,S,N)\n",
    "    idx = dist.topk(k=k, dim=-1, largest=False)[1]\n",
    "    return idx\n",
    "\n",
    "# =========================\n",
    "# POINTNET++ MODULES\n",
    "# =========================\n",
    "\n",
    "class SharedMLP(nn.Module):\n",
    "    def __init__(self, channels: List[int], bn=True):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(channels) - 1):\n",
    "            layers.append(nn.Conv2d(channels[i], channels[i+1], 1))\n",
    "            if bn:\n",
    "                layers.append(nn.BatchNorm2d(channels[i+1]))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class PointNetSetAbstractionKNN(nn.Module):\n",
    "    \"\"\"\n",
    "    SA layer using FPS + kNN grouping + PointNet on local groups\n",
    "    \"\"\"\n",
    "    def __init__(self, npoint: int, nsample: int, in_ch: int, mlp: List[int]):\n",
    "        super().__init__()\n",
    "        self.npoint = npoint\n",
    "        self.nsample = nsample\n",
    "        self.mlp = SharedMLP([in_ch] + mlp, bn=True)\n",
    "\n",
    "    def forward(self, xyz: torch.Tensor, points: Optional[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        xyz: (B, N, 3)\n",
    "        points: (B, N, D) or None\n",
    "        returns:\n",
    "          new_xyz: (B, S, 3)\n",
    "          new_points: (B, S, mlp[-1])\n",
    "        \"\"\"\n",
    "        B, N, _ = xyz.shape\n",
    "        S = self.npoint\n",
    "\n",
    "        fps_idx = farthest_point_sample(xyz, S)        # (B,S)\n",
    "        new_xyz = index_points(xyz, fps_idx)           # (B,S,3)\n",
    "\n",
    "        knn_idx = knn_point(self.nsample, xyz, new_xyz)  # (B,S,K)\n",
    "        grouped_xyz = index_points(xyz, knn_idx)         # (B,S,K,3)\n",
    "        grouped_xyz_norm = grouped_xyz - new_xyz.unsqueeze(2)\n",
    "\n",
    "        if points is not None:\n",
    "            grouped_points = index_points(points, knn_idx)   # (B,S,K,D)\n",
    "            new_group = torch.cat([grouped_xyz_norm, grouped_points], dim=-1)  # (B,S,K,3+D)\n",
    "        else:\n",
    "            new_group = grouped_xyz_norm  # (B,S,K,3)\n",
    "\n",
    "        # (B, C, S, K)\n",
    "        new_group = new_group.permute(0, 3, 1, 2).contiguous()\n",
    "        new_group = self.mlp(new_group)                 # (B, mlp[-1], S, K)\n",
    "        new_points = torch.max(new_group, dim=-1)[0]    # (B, mlp[-1], S)\n",
    "        new_points = new_points.transpose(1, 2).contiguous()  # (B, S, mlp[-1])\n",
    "\n",
    "        return new_xyz, new_points\n",
    "\n",
    "class PointNetFeaturePropagation(nn.Module):\n",
    "    def __init__(self, in_ch: int, mlp: List[int]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_ch\n",
    "        for out in mlp:\n",
    "            layers += [nn.Conv1d(last, out, 1), nn.BatchNorm1d(out), nn.ReLU(inplace=True)]\n",
    "            last = out\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, xyz1, xyz2, points1, points2):\n",
    "        \"\"\"\n",
    "        Interpolate from xyz2 (sparser) to xyz1 (denser)\n",
    "        xyz1: (B, N, 3)\n",
    "        xyz2: (B, S, 3)\n",
    "        points1: (B, N, D1) or None\n",
    "        points2: (B, S, D2)\n",
    "        return new_points: (B, N, mlp[-1])\n",
    "        \"\"\"\n",
    "        B, N, _ = xyz1.shape\n",
    "        _, S, _ = xyz2.shape\n",
    "\n",
    "        if S == 1:\n",
    "            interpolated = points2.repeat(1, N, 1)\n",
    "        else:\n",
    "            dist = square_distance(xyz1, xyz2)          # (B,N,S)\n",
    "            dist, idx = dist.topk(k=3, dim=-1, largest=False)  # (B,N,3)\n",
    "            dist = torch.clamp(dist, min=1e-10)\n",
    "            weight = 1.0 / dist\n",
    "            weight = weight / torch.sum(weight, dim=-1, keepdim=True)\n",
    "\n",
    "            grouped_points = index_points(points2, idx)  # (B,N,3,D2)\n",
    "            interpolated = torch.sum(grouped_points * weight.unsqueeze(-1), dim=2)  # (B,N,D2)\n",
    "\n",
    "        if points1 is not None:\n",
    "            new_points = torch.cat([points1, interpolated], dim=-1)  # (B,N,D1+D2)\n",
    "        else:\n",
    "            new_points = interpolated\n",
    "\n",
    "        new_points = new_points.transpose(1, 2).contiguous()  # (B, C, N)\n",
    "        new_points = self.mlp(new_points)                     # (B, mlp[-1], N)\n",
    "        return new_points.transpose(1, 2).contiguous()        # (B, N, mlp[-1])\n",
    "\n",
    "# =========================\n",
    "# POINTNET++ SEGMENTATION MODEL\n",
    "# =========================\n",
    "\n",
    "class PointNet2SSGSeg(nn.Module):\n",
    "    def __init__(self, num_classes: int, in_ch: int = 10):\n",
    "        super().__init__()\n",
    "        # split xyz + features\n",
    "        # xyz = X[:, :, 0:3]\n",
    "        # feat = X[:, :, 3:10] (7 dims)\n",
    "\n",
    "        self.sa1 = PointNetSetAbstractionKNN(\n",
    "            npoint=NPOINTS[0], nsample=NSAMPLE[0],\n",
    "            in_ch=3 + (in_ch - 3),     # (xyz_norm=3) + feat dims (7) = 10\n",
    "            mlp=[64, 64, 128]\n",
    "        )\n",
    "        self.sa2 = PointNetSetAbstractionKNN(\n",
    "            npoint=NPOINTS[1], nsample=NSAMPLE[1],\n",
    "            in_ch=3 + 128,            # grouped xyz_norm + points(128)\n",
    "            mlp=[128, 128, 256]\n",
    "        )\n",
    "        self.sa3 = PointNetSetAbstractionKNN(\n",
    "            npoint=NPOINTS[2], nsample=NSAMPLE[2],\n",
    "            in_ch=3 + 256,\n",
    "            mlp=[256, 256, 512]\n",
    "        )\n",
    "\n",
    "        self.fp3 = PointNetFeaturePropagation(in_ch=256 + 512, mlp=[256, 256])\n",
    "        self.fp2 = PointNetFeaturePropagation(in_ch=128 + 256, mlp=[256, 128])\n",
    "        self.fp1 = PointNetFeaturePropagation(in_ch=(in_ch - 3) + 128, mlp=[128, 128, 128])\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv1d(128, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv1d(128, num_classes, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: (B, N, 10)\n",
    "        returns logits: (B, N, num_classes)\n",
    "        \"\"\"\n",
    "        xyz = X[:, :, 0:3].contiguous()\n",
    "        feat = X[:, :, 3:].contiguous()  # (B,N,7)\n",
    "\n",
    "        # SA1: use feat as points, but SA expects points grouped in channel-last; we combine inside SA\n",
    "        l1_xyz, l1_points = self.sa1(xyz, feat)          # (B,1024,3), (B,1024,128)\n",
    "        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)  # (B,256,3),  (B,256,256)\n",
    "        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)  # (B,64,3),   (B,64,512)\n",
    "\n",
    "        l2_points_fp = self.fp3(l2_xyz, l3_xyz, l2_points, l3_points)   # (B,256,256)\n",
    "        l1_points_fp = self.fp2(l1_xyz, l2_xyz, l1_points, l2_points_fp) # (B,1024,128)\n",
    "        l0_points_fp = self.fp1(xyz, l1_xyz, feat, l1_points_fp)         # (B,N,128)\n",
    "\n",
    "        x = l0_points_fp.transpose(1, 2).contiguous()   # (B,128,N)\n",
    "        logits = self.classifier(x)                     # (B,C,N)\n",
    "        return logits.transpose(1, 2).contiguous()      # (B,N,C)\n",
    "\n",
    "# =========================\n",
    "# LOSS + METRICS\n",
    "# =========================\n",
    "\n",
    "class WeightedCELoss(nn.Module):\n",
    "    def __init__(self, weight: torch.Tensor, label_smoothing: float, ignore_index: int):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"weight\", weight)\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        B, N, C = logits.shape\n",
    "        logits = logits.reshape(B*N, C)\n",
    "        target = target.reshape(B*N)\n",
    "        return F.cross_entropy(\n",
    "            logits, target,\n",
    "            weight=self.weight,\n",
    "            ignore_index=self.ignore_index,\n",
    "            label_smoothing=self.label_smoothing\n",
    "        )\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics(logits, y, num_classes: int):\n",
    "    pred = logits.argmax(dim=-1)  # (B,N)\n",
    "    mask = (y != IGNORE_INDEX)\n",
    "    correct = (pred[mask] == y[mask]).sum().item()\n",
    "    total = mask.sum().item()\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "\n",
    "    # macro F1\n",
    "    f1s = []\n",
    "    for c in range(num_classes):\n",
    "        tp = ((pred == c) & (y == c) & mask).sum().item()\n",
    "        fp = ((pred == c) & (y != c) & mask).sum().item()\n",
    "        fn = ((pred != c) & (y == c) & mask).sum().item()\n",
    "        denom = 2*tp + fp + fn\n",
    "        f1s.append((2*tp/denom) if denom > 0 else 0.0)\n",
    "    macro_f1 = float(sum(f1s) / len(f1s))\n",
    "    return acc, macro_f1\n",
    "\n",
    "# =========================\n",
    "# TRAIN / VAL\n",
    "# =========================\n",
    "\n",
    "def train_one_epoch(model, loader, optim, scaler, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "\n",
    "    for X, y, _, _ in loader:\n",
    "        X = X.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(USE_AMP and DEVICE==\"cuda\")):\n",
    "            logits = model(X)\n",
    "            loss = loss_fn(logits, y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "    return total_loss / max(total_batches, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "    accs, f1s = [], []\n",
    "\n",
    "    for X, y, _, _ in loader:\n",
    "        X = X.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        logits = model(X)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        acc, macro_f1 = compute_metrics(logits, y, NUM_CLASSES)\n",
    "        accs.append(acc)\n",
    "        f1s.append(macro_f1)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "\n",
    "    return total_loss / max(total_batches, 1), float(np.mean(accs)), float(np.mean(f1s))\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "\n",
    "def main():\n",
    "    print(\"Device:\", DEVICE)\n",
    "    if DEVICE == \"cuda\":\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "    train_files = list_npz(TRAIN_DIR)\n",
    "    val_files   = list_npz(VAL_DIR)\n",
    "\n",
    "    print(\"Train blocks:\", len(train_files))\n",
    "    print(\"Val blocks  :\", len(val_files))\n",
    "    if len(train_files) == 0:\n",
    "        raise RuntimeError(\"No train .npz files found. Check TRAIN_DIR.\")\n",
    "\n",
    "    train_ds = BlocksNPZDataset(train_files)\n",
    "    val_ds   = BlocksNPZDataset(val_files)\n",
    "\n",
    "    sampler = BucketBlockSampler(train_ds, num_samples=len(train_ds), seed=123)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=BATCH, sampler=sampler,\n",
    "        num_workers=NUM_WORKERS, pin_memory=(DEVICE==\"cuda\"),\n",
    "        drop_last=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=BATCH, shuffle=False,\n",
    "        num_workers=NUM_WORKERS, pin_memory=(DEVICE==\"cuda\"),\n",
    "    )\n",
    "\n",
    "    # If you have real counts, put them here. Otherwise use a mild default.\n",
    "    # For 5-class [1,2,3,6,12], you can approximate from your earlier global counts.\n",
    "    counts_guess = {1: 1, 2: 7000000, 3: 50000, 6: 1200000, 12: 3800000}\n",
    "    class_w = compute_class_weights_from_counts(counts_guess).to(DEVICE)\n",
    "    print(\"Class weights:\", class_w.detach().cpu().numpy())\n",
    "\n",
    "    model = PointNet2SSGSeg(num_classes=NUM_CLASSES, in_ch=IN_CH).to(DEVICE)\n",
    "\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=EPOCHS, eta_min=1e-5)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(USE_AMP and DEVICE==\"cuda\"))\n",
    "\n",
    "    loss_fn = WeightedCELoss(class_w, LABEL_SMOOTH, IGNORE_INDEX)\n",
    "\n",
    "    best_f1 = -1.0\n",
    "    best_path = os.path.join(OUT_DIR, f\"pointnetpp_best_{NUM_CLASSES}cls.pt\")\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        t0 = time.time()\n",
    "        tr_loss = train_one_epoch(model, train_loader, optim, scaler, loss_fn)\n",
    "        va_loss, va_acc, va_f1 = validate(model, val_loader, loss_fn)\n",
    "        sched.step()\n",
    "\n",
    "        lr = optim.param_groups[0][\"lr\"]\n",
    "        dt = time.time() - t0\n",
    "        print(f\"Epoch {epoch:03d} | {dt:5.1f}s | lr={lr:.2e} | train={tr_loss:.4f} | val={va_loss:.4f} | acc={va_acc:.4f} | macroF1={va_f1:.4f}\")\n",
    "\n",
    "        if va_f1 > best_f1:\n",
    "            best_f1 = va_f1\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"num_classes\": NUM_CLASSES,\n",
    "                    \"raw_classes\": RAW_CLASSES,\n",
    "                    \"class_weights\": class_w.detach().cpu(),\n",
    "                },\n",
    "                best_path\n",
    "            )\n",
    "            print(\"✅ Saved best:\", best_path)\n",
    "\n",
    "    print(\"Done. Best macroF1:\", best_f1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e70b73b5-0163-457c-833a-d3c38849680a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total blocks: 27697\n",
      "Train blocks: 22157\n",
      "Val blocks: 5540\n",
      "Train loader ready: 3692\n"
     ]
    }
   ],
   "source": [
    "import glob, os, random\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# path to folder containing ALL npz files\n",
    "ALL_BLOCKS_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "\n",
    "def list_npz(folder):\n",
    "    return sorted(glob.glob(os.path.join(folder, \"*.npz\")))\n",
    "\n",
    "all_files = list_npz(ALL_BLOCKS_DIR)\n",
    "\n",
    "print(\"Total blocks:\", len(all_files))\n",
    "\n",
    "# split 80/20\n",
    "random.seed(42)\n",
    "random.shuffle(all_files)\n",
    "split = int(0.8 * len(all_files))\n",
    "\n",
    "train_files = all_files[:split]\n",
    "val_files   = all_files[split:]\n",
    "\n",
    "print(\"Train blocks:\", len(train_files))\n",
    "print(\"Val blocks:\", len(val_files))\n",
    "\n",
    "# dataset objects\n",
    "train_ds = BlocksNPZ(train_files)\n",
    "val_ds   = BlocksNPZ(val_files)\n",
    "\n",
    "# dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"Train loader ready:\", len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b908df1-e115-422f-b19b-dc83d6d3c955",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 332\u001b[0m\n\u001b[0;32m    329\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# ---------------- TRAIN ----------------\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    333\u001b[0m tr_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "    import os, glob, random, time\n",
    "    from typing import List, Dict\n",
    "    \n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    \n",
    "    # =========================\n",
    "    # 0) EDIT THESE PATHS\n",
    "    # =========================\n",
    "    ALL_BLOCKS_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"   # <-- folder containing ALL .npz\n",
    "    OUT_DIR        = r\"D:/lidarrrrr/anbu/dl_models\"    # <-- where to save model\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # =========================\n",
    "    # 1) BASIC SETTINGS\n",
    "    # =========================\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Start with 5-class (matches your current working mapping)\n",
    "    RAW_CLASSES = [1, 2, 3, 6, 12]     # change to [1,2,3,6,7,12,13] later\n",
    "    MAP = {c: i for i, c in enumerate(RAW_CLASSES)}\n",
    "    NUM_CLASSES = len(RAW_CLASSES)\n",
    "    IGNORE_INDEX = -100\n",
    "    \n",
    "    POINTS = 4096\n",
    "    IN_CH = 10\n",
    "    \n",
    "    BATCH = 6            # RTX 3050 safe\n",
    "    EPOCHS = 30          # start small; later set 120+\n",
    "    LR = 1e-3\n",
    "    WD = 1e-4\n",
    "    LABEL_SMOOTH = 0.05\n",
    "    USE_AMP = True\n",
    "    NUM_WORKERS = 0      # Windows safe (try 2 if stable)\n",
    "    \n",
    "    # PointNet++ SSG sizes\n",
    "    NPOINTS = [1024, 256, 64]\n",
    "    NSAMPLE = [32, 32, 32]  # kNN neighbors\n",
    "    \n",
    "    # =========================\n",
    "    # 2) HELPERS\n",
    "    # =========================\n",
    "    def list_npz(folder: str) -> List[str]:\n",
    "        return sorted(glob.glob(os.path.join(folder, \"*.npz\")))\n",
    "    \n",
    "    def map_labels(y_raw: np.ndarray) -> np.ndarray:\n",
    "        out = np.full_like(y_raw, IGNORE_INDEX)\n",
    "        for raw, new in MAP.items():\n",
    "            out[y_raw == raw] = new\n",
    "        return out\n",
    "    \n",
    "    def class_weights_from_counts(counts_raw: Dict[int, int]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        w = 1/sqrt(freq), normalize mean~1, clamp [0.25, 10]\n",
    "        \"\"\"\n",
    "        freqs = []\n",
    "        for raw in RAW_CLASSES:\n",
    "            freqs.append(float(counts_raw.get(raw, 1)))\n",
    "        freqs = np.array(freqs, dtype=np.float64)\n",
    "    \n",
    "        w = 1.0 / np.sqrt(freqs)\n",
    "        w = w / (w.mean() + 1e-12)\n",
    "        w = np.clip(w, 0.25, 10.0).astype(np.float32)\n",
    "        return torch.from_numpy(w)\n",
    "    \n",
    "    # =========================\n",
    "    # 3) DATASET\n",
    "    # =========================\n",
    "    class BlocksNPZ(Dataset):\n",
    "        def __init__(self, files: List[str]):\n",
    "            self.files = files\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.files)\n",
    "    \n",
    "        def __getitem__(self, i):\n",
    "            d = np.load(self.files[i])\n",
    "            X = d[\"X\"].astype(np.float32)      # (4096,10)\n",
    "            y = d[\"y\"].astype(np.int32)        # (4096,)\n",
    "            y = map_labels(y)\n",
    "            return torch.from_numpy(X), torch.from_numpy(y)\n",
    "    \n",
    "    # =========================\n",
    "    # 4) POINTNET++ OPS (FPS + kNN)\n",
    "    # =========================\n",
    "    def square_distance(src, dst):\n",
    "        # src: (B,N,3), dst: (B,M,3) -> (B,N,M)\n",
    "        dist = -2 * torch.matmul(src, dst.transpose(1,2))\n",
    "        dist += torch.sum(src**2, dim=-1).unsqueeze(-1)\n",
    "        dist += torch.sum(dst**2, dim=-1).unsqueeze(1)\n",
    "        return dist\n",
    "    \n",
    "    def index_points(points, idx):\n",
    "        # points: (B,N,C), idx: (B,S) or (B,S,K)\n",
    "        B = points.shape[0]\n",
    "        if idx.dim() == 2:\n",
    "            batch = torch.arange(B, device=points.device).view(B,1)\n",
    "            return points[batch, idx, :]\n",
    "        else:\n",
    "            batch = torch.arange(B, device=points.device).view(B,1,1)\n",
    "            return points[batch, idx, :]\n",
    "    \n",
    "    def farthest_point_sample(xyz, npoint):\n",
    "        # xyz: (B,N,3) -> (B,npoint)\n",
    "        device = xyz.device\n",
    "        B, N, _ = xyz.shape\n",
    "        centroids = torch.zeros(B, npoint, dtype=torch.long, device=device)\n",
    "        distance = torch.full((B, N), 1e10, device=device)\n",
    "        farthest = torch.randint(0, N, (B,), dtype=torch.long, device=device)\n",
    "        batch = torch.arange(B, device=device)\n",
    "    \n",
    "        for i in range(npoint):\n",
    "            centroids[:, i] = farthest\n",
    "            centroid = xyz[batch, farthest].view(B, 1, 3)\n",
    "            dist = torch.sum((xyz - centroid) ** 2, dim=-1)\n",
    "            distance = torch.minimum(distance, dist)\n",
    "            farthest = torch.max(distance, dim=-1)[1]\n",
    "        return centroids\n",
    "    \n",
    "    def knn_point(k, xyz, new_xyz):\n",
    "        # xyz: (B,N,3), new_xyz: (B,S,3) -> idx: (B,S,k)\n",
    "        dist = square_distance(new_xyz, xyz)   # (B,S,N)\n",
    "        idx = dist.topk(k=k, dim=-1, largest=False)[1]\n",
    "        return idx\n",
    "    \n",
    "    # =========================\n",
    "    # 5) POINTNET++ MODULES\n",
    "    # =========================\n",
    "    class SharedMLP(nn.Module):\n",
    "        def __init__(self, channels):\n",
    "            super().__init__()\n",
    "            layers = []\n",
    "            for i in range(len(channels)-1):\n",
    "                layers += [\n",
    "                    nn.Conv2d(channels[i], channels[i+1], 1),\n",
    "                    nn.BatchNorm2d(channels[i+1]),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                ]\n",
    "            self.net = nn.Sequential(*layers)\n",
    "    \n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "    \n",
    "    class SA_KNN(nn.Module):\n",
    "        def __init__(self, npoint, nsample, in_ch, mlp):\n",
    "            super().__init__()\n",
    "            self.npoint = npoint\n",
    "            self.nsample = nsample\n",
    "            self.mlp = SharedMLP([in_ch] + mlp)\n",
    "    \n",
    "        def forward(self, xyz, points):\n",
    "            # xyz: (B,N,3), points: (B,N,D) or None\n",
    "            fps_idx = farthest_point_sample(xyz, self.npoint)  # (B,S)\n",
    "            new_xyz = index_points(xyz, fps_idx)               # (B,S,3)\n",
    "    \n",
    "            knn_idx = knn_point(self.nsample, xyz, new_xyz)    # (B,S,K)\n",
    "            grouped_xyz = index_points(xyz, knn_idx)           # (B,S,K,3)\n",
    "            grouped_xyz = grouped_xyz - new_xyz.unsqueeze(2)   # normalize\n",
    "    \n",
    "            if points is not None:\n",
    "                grouped_points = index_points(points, knn_idx) # (B,S,K,D)\n",
    "                new_group = torch.cat([grouped_xyz, grouped_points], dim=-1)  # (B,S,K,3+D)\n",
    "            else:\n",
    "                new_group = grouped_xyz\n",
    "    \n",
    "            # (B,C,S,K)\n",
    "            new_group = new_group.permute(0,3,1,2).contiguous()\n",
    "            new_group = self.mlp(new_group)                     # (B,mlp[-1],S,K)\n",
    "            new_points = torch.max(new_group, dim=-1)[0]        # (B,mlp[-1],S)\n",
    "            new_points = new_points.transpose(1,2).contiguous() # (B,S,mlp[-1])\n",
    "            return new_xyz, new_points\n",
    "    \n",
    "    class FP(nn.Module):\n",
    "        def __init__(self, in_ch, mlp):\n",
    "            super().__init__()\n",
    "            layers = []\n",
    "            last = in_ch\n",
    "            for out in mlp:\n",
    "                layers += [nn.Conv1d(last, out, 1), nn.BatchNorm1d(out), nn.ReLU(inplace=True)]\n",
    "                last = out\n",
    "            self.mlp = nn.Sequential(*layers)\n",
    "    \n",
    "        def forward(self, xyz1, xyz2, p1, p2):\n",
    "            # xyz1: (B,N,3), xyz2: (B,S,3), p2: (B,S,D2), p1: (B,N,D1) or None\n",
    "            B, N, _ = xyz1.shape\n",
    "            _, S, _ = xyz2.shape\n",
    "    \n",
    "            if S == 1:\n",
    "                interpolated = p2.repeat(1, N, 1)\n",
    "            else:\n",
    "                dist = square_distance(xyz1, xyz2)                  # (B,N,S)\n",
    "                dist, idx = dist.topk(k=3, dim=-1, largest=False)   # (B,N,3)\n",
    "                dist = torch.clamp(dist, min=1e-10)\n",
    "                w = (1.0 / dist)\n",
    "                w = w / torch.sum(w, dim=-1, keepdim=True)\n",
    "    \n",
    "                grouped = index_points(p2, idx)                     # (B,N,3,D2)\n",
    "                interpolated = torch.sum(grouped * w.unsqueeze(-1), dim=2)  # (B,N,D2)\n",
    "    \n",
    "            if p1 is not None:\n",
    "                new_points = torch.cat([p1, interpolated], dim=-1)\n",
    "            else:\n",
    "                new_points = interpolated\n",
    "    \n",
    "            new_points = new_points.transpose(1,2).contiguous()     # (B,C,N)\n",
    "            new_points = self.mlp(new_points)\n",
    "            return new_points.transpose(1,2).contiguous()           # (B,N,out)\n",
    "    \n",
    "    class PointNet2SSGSeg(nn.Module):\n",
    "        def __init__(self, num_classes, in_ch=10):\n",
    "            super().__init__()\n",
    "            self.sa1 = SA_KNN(NPOINTS[0], NSAMPLE[0], in_ch, [64,64,128])     # xyz+feat packed as 10\n",
    "            self.sa2 = SA_KNN(NPOINTS[1], NSAMPLE[1], 3+128, [128,128,256])   # grouped xyz(3)+128\n",
    "            self.sa3 = SA_KNN(NPOINTS[2], NSAMPLE[2], 3+256, [256,256,512])\n",
    "    \n",
    "            self.fp3 = FP(256+512, [256,256])\n",
    "            self.fp2 = FP(128+256, [256,128])\n",
    "            self.fp1 = FP((in_ch-3)+128, [128,128,128])  # feat(7)+128\n",
    "    \n",
    "            self.cls = nn.Sequential(\n",
    "                nn.Conv1d(128, 128, 1),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Conv1d(128, num_classes, 1)\n",
    "            )\n",
    "    \n",
    "        def forward(self, X):\n",
    "            xyz = X[:, :, 0:3].contiguous()      # (B,N,3)\n",
    "            feat = X[:, :, 3:].contiguous()      # (B,N,7)\n",
    "    \n",
    "            # pack xyz_norm+feat inside SA1: we pass points=feat, SA will concat xyz_norm+points\n",
    "            l1_xyz, l1_p = self.sa1(xyz, feat)     # (B,1024,3), (B,1024,128)\n",
    "            l2_xyz, l2_p = self.sa2(l1_xyz, l1_p)  # (B,256,3),  (B,256,256)\n",
    "            l3_xyz, l3_p = self.sa3(l2_xyz, l2_p)  # (B,64,3),   (B,64,512)\n",
    "    \n",
    "            l2_fp = self.fp3(l2_xyz, l3_xyz, l2_p, l3_p)      # (B,256,256)\n",
    "            l1_fp = self.fp2(l1_xyz, l2_xyz, l1_p, l2_fp)     # (B,1024,128)\n",
    "            l0_fp = self.fp1(xyz, l1_xyz, feat, l1_fp)        # (B,N,128)\n",
    "    \n",
    "            x = l0_fp.transpose(1,2).contiguous()             # (B,128,N)\n",
    "            logits = self.cls(x).transpose(1,2).contiguous()  # (B,N,C)\n",
    "            return logits\n",
    "    \n",
    "    # =========================\n",
    "    # 6) LOSS + METRICS\n",
    "    # =========================\n",
    "    class WeightedCELoss(nn.Module):\n",
    "        def __init__(self, w, label_smooth, ignore_index):\n",
    "            super().__init__()\n",
    "            self.register_buffer(\"w\", w)\n",
    "            self.ls = label_smooth\n",
    "            self.ignore = ignore_index\n",
    "    \n",
    "        def forward(self, logits, y):\n",
    "            B, N, C = logits.shape\n",
    "            logits = logits.reshape(B*N, C)\n",
    "            y = y.reshape(B*N)\n",
    "            return F.cross_entropy(logits, y, weight=self.w, ignore_index=self.ignore, label_smoothing=self.ls)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def metrics(logits, y):\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        mask = (y != IGNORE_INDEX)\n",
    "        acc = (pred[mask] == y[mask]).float().mean().item() if mask.any() else 0.0\n",
    "    \n",
    "        # macro F1\n",
    "        f1s = []\n",
    "        for c in range(NUM_CLASSES):\n",
    "            tp = ((pred==c) & (y==c) & mask).sum().item()\n",
    "            fp = ((pred==c) & (y!=c) & mask).sum().item()\n",
    "            fn = ((pred!=c) & (y==c) & mask).sum().item()\n",
    "            denom = 2*tp + fp + fn\n",
    "            f1s.append((2*tp/denom) if denom>0 else 0.0)\n",
    "        return acc, float(sum(f1s)/len(f1s))\n",
    "    \n",
    "    # =========================\n",
    "    # 7) TRAIN\n",
    "    # =========================\n",
    "    def main():\n",
    "        print(\"Device:\", DEVICE)\n",
    "        if DEVICE==\"cuda\":\n",
    "            print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    \n",
    "        all_files = list_npz(ALL_BLOCKS_DIR)\n",
    "        if len(all_files) == 0:\n",
    "            raise RuntimeError(\"No .npz found. Check ALL_BLOCKS_DIR path.\")\n",
    "    \n",
    "        random.seed(42)\n",
    "        random.shuffle(all_files)\n",
    "        split = int(0.8 * len(all_files))\n",
    "        train_files = all_files[:split]\n",
    "        val_files = all_files[split:]\n",
    "    \n",
    "        print(\"All blocks :\", len(all_files))\n",
    "        print(\"Train blocks:\", len(train_files))\n",
    "        print(\"Val blocks  :\", len(val_files))\n",
    "    \n",
    "        train_ds = BlocksNPZ(train_files)\n",
    "        val_ds   = BlocksNPZ(val_files)\n",
    "    \n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,\n",
    "                                  num_workers=NUM_WORKERS, pin_memory=(DEVICE==\"cuda\"), drop_last=True)\n",
    "        val_loader   = DataLoader(val_ds, batch_size=BATCH, shuffle=False,\n",
    "                                  num_workers=NUM_WORKERS, pin_memory=(DEVICE==\"cuda\"))\n",
    "    \n",
    "        # Use your known rough counts (edit if you have better)\n",
    "        # For 5-class [1,2,3,6,12], your inference looked like:\n",
    "        counts_guess = {1: 1, 2: 7185068, 3: 33809, 6: 1274169, 12: 3881799}\n",
    "        w = class_weights_from_counts(counts_guess).to(DEVICE)\n",
    "        print(\"Class weights:\", w.detach().cpu().numpy())\n",
    "    \n",
    "        model = PointNet2SSGSeg(NUM_CLASSES, in_ch=IN_CH).to(DEVICE)\n",
    "    \n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS, eta_min=1e-5)\n",
    "        scaler = torch.amp.GradScaler(\"cuda\", enabled=(USE_AMP and DEVICE==\"cuda\"))\n",
    "        with torch.amp.autocast(\"cuda\", enabled=(USE_AMP and DEVICE==\"cuda\")):\n",
    "            logits = model(X)\n",
    "            loss = loss_fn(logits, y)\n",
    "            best_f1 = -1.0\n",
    "    \n",
    "    best_path = os.path.join(OUT_DIR, f\"pointnetpp_best_{NUM_CLASSES}cls.pt\")\n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        t0 = time.time()\n",
    "    \n",
    "        # ---------------- TRAIN ----------------\n",
    "        model.train()\n",
    "        tr_loss = 0.0\n",
    "    \n",
    "        for X, y in train_loader:\n",
    "            X = X.to(DEVICE, non_blocking=True).float()\n",
    "            y = y.to(DEVICE, non_blocking=True).long()\n",
    "    \n",
    "            opt.zero_grad(set_to_none=True)\n",
    "    \n",
    "            with torch.amp.autocast(\"cuda\", enabled=(USE_AMP and DEVICE == \"cuda\")):\n",
    "                logits = model(X)          # (B,N,C)\n",
    "                loss = loss_fn(logits, y)\n",
    "    \n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "    \n",
    "            tr_loss += loss.item()\n",
    "    \n",
    "        tr_loss /= max(len(train_loader), 1)\n",
    "    \n",
    "        # ---------------- VALIDATE ----------------\n",
    "        model.eval()\n",
    "        va_loss = 0.0\n",
    "        accs, f1s = [], []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X = X.to(DEVICE, non_blocking=True).float()\n",
    "                y = y.to(DEVICE, non_blocking=True).long()\n",
    "    \n",
    "                logits = model(X)\n",
    "                va_loss += loss_fn(logits, y).item()\n",
    "    \n",
    "                acc, f1 = metrics(logits, y)\n",
    "                accs.append(acc)\n",
    "                f1s.append(f1)\n",
    "    \n",
    "        va_loss /= max(len(val_loader), 1)\n",
    "        va_acc = float(np.mean(accs)) if accs else 0.0\n",
    "        va_f1 = float(np.mean(f1s)) if f1s else 0.0\n",
    "    \n",
    "        sch.step()\n",
    "        lr = opt.param_groups[0][\"lr\"]\n",
    "        dt = time.time() - t0\n",
    "    \n",
    "        print(f\"Epoch {epoch:03d} | {dt:5.1f}s | lr={lr:.2e} | train={tr_loss:.4f} | val={va_loss:.4f} | acc={va_acc:.4f} | macroF1={va_f1:.4f}\")\n",
    "    \n",
    "        if va_f1 > best_f1:\n",
    "            best_f1 = va_f1\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"num_classes\": NUM_CLASSES,\n",
    "                    \"raw_classes\": RAW_CLASSES,\n",
    "                    \"class_weights\": w.detach().cpu(),\n",
    "                },\n",
    "                best_path,\n",
    "            )\n",
    "            print(\"✅ Saved best:\", best_path)\n",
    "    \n",
    "    print(\"Done. Best macroF1:\", best_f1)\n",
    "    print(\"Best model:\", best_path)\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6c6edc4-716a-417c-ba26-e65d63e03529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All blocks : 27697\n",
      "Train blocks: 22157\n",
      "Val blocks  : 5540\n"
     ]
    }
   ],
   "source": [
    "# Load dataset files\n",
    "all_files = list_npz(ALL_BLOCKS_DIR)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(all_files)\n",
    "\n",
    "split = int(0.8 * len(all_files))\n",
    "train_files = all_files[:split]\n",
    "val_files   = all_files[split:]\n",
    "\n",
    "print(\"All blocks :\", len(all_files))\n",
    "print(\"Train blocks:\", len(train_files))\n",
    "print(\"Val blocks  :\", len(val_files))\n",
    "\n",
    "# Dataset objects\n",
    "train_ds = BlocksNPZ(train_files)\n",
    "val_ds   = BlocksNPZ(val_files)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(DEVICE == \"cuda\"),\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(DEVICE == \"cuda\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbcd9377-2ea5-49c2-87ec-882793ef30c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loader ready:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_loader\u001b[49m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Train loader ready:\", len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef2d3c72-f31b-488e-bd84-d9a52d329538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total blocks: 27697\n",
      "Train blocks: 22157\n",
      "Val blocks: 5540\n",
      "Train loader ready: 3692\n"
     ]
    }
   ],
   "source": [
    "import glob, os, random\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# path to folder containing ALL npz files\n",
    "ALL_BLOCKS_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "\n",
    "def list_npz(folder):\n",
    "    return sorted(glob.glob(os.path.join(folder, \"*.npz\")))\n",
    "\n",
    "all_files = list_npz(ALL_BLOCKS_DIR)\n",
    "\n",
    "print(\"Total blocks:\", len(all_files))\n",
    "\n",
    "# split 80/20\n",
    "random.seed(42)\n",
    "random.shuffle(all_files)\n",
    "split = int(0.8 * len(all_files))\n",
    "\n",
    "train_files = all_files[:split]\n",
    "val_files   = all_files[split:]\n",
    "\n",
    "print(\"Train blocks:\", len(train_files))\n",
    "print(\"Val blocks:\", len(val_files))\n",
    "\n",
    "# dataset objects\n",
    "train_ds = BlocksNPZ(train_files)\n",
    "val_ds   = BlocksNPZ(val_files)\n",
    "\n",
    "# dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"Train loader ready:\", len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68bd135d-b2d1-4977-99e0-1cd2d78d024b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 332\u001b[0m\n\u001b[0;32m    329\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# ---------------- TRAIN ----------------\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    333\u001b[0m tr_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "    import os, glob, random, time\n",
    "    from typing import List, Dict\n",
    "    \n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    \n",
    "    # =========================\n",
    "    # 0) EDIT THESE PATHS\n",
    "    # =========================\n",
    "    ALL_BLOCKS_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"   # <-- folder containing ALL .npz\n",
    "    OUT_DIR        = r\"D:/lidarrrrr/anbu/dl_models\"    # <-- where to save model\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # =========================\n",
    "    # 1) BASIC SETTINGS\n",
    "    # =========================\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Start with 5-class (matches your current working mapping)\n",
    "    RAW_CLASSES = [1, 2, 3, 6, 12]     # change to [1,2,3,6,7,12,13] later\n",
    "    MAP = {c: i for i, c in enumerate(RAW_CLASSES)}\n",
    "    NUM_CLASSES = len(RAW_CLASSES)\n",
    "    IGNORE_INDEX = -100\n",
    "    \n",
    "    POINTS = 4096\n",
    "    IN_CH = 10\n",
    "    \n",
    "    BATCH = 6            # RTX 3050 safe\n",
    "    EPOCHS = 30          # start small; later set 120+\n",
    "    LR = 1e-3\n",
    "    WD = 1e-4\n",
    "    LABEL_SMOOTH = 0.05\n",
    "    USE_AMP = True\n",
    "    NUM_WORKERS = 0      # Windows safe (try 2 if stable)\n",
    "    \n",
    "    # PointNet++ SSG sizes\n",
    "    NPOINTS = [1024, 256, 64]\n",
    "    NSAMPLE = [32, 32, 32]  # kNN neighbors\n",
    "    \n",
    "    # =========================\n",
    "    # 2) HELPERS\n",
    "    # =========================\n",
    "    def list_npz(folder: str) -> List[str]:\n",
    "        return sorted(glob.glob(os.path.join(folder, \"*.npz\")))\n",
    "    \n",
    "    def map_labels(y_raw: np.ndarray) -> np.ndarray:\n",
    "        out = np.full_like(y_raw, IGNORE_INDEX)\n",
    "        for raw, new in MAP.items():\n",
    "            out[y_raw == raw] = new\n",
    "        return out\n",
    "    \n",
    "    def class_weights_from_counts(counts_raw: Dict[int, int]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        w = 1/sqrt(freq), normalize mean~1, clamp [0.25, 10]\n",
    "        \"\"\"\n",
    "        freqs = []\n",
    "        for raw in RAW_CLASSES:\n",
    "            freqs.append(float(counts_raw.get(raw, 1)))\n",
    "        freqs = np.array(freqs, dtype=np.float64)\n",
    "    \n",
    "        w = 1.0 / np.sqrt(freqs)\n",
    "        w = w / (w.mean() + 1e-12)\n",
    "        w = np.clip(w, 0.25, 10.0).astype(np.float32)\n",
    "        return torch.from_numpy(w)\n",
    "    \n",
    "    # =========================\n",
    "    # 3) DATASET\n",
    "    # =========================\n",
    "    class BlocksNPZ(Dataset):\n",
    "        def __init__(self, files: List[str]):\n",
    "            self.files = files\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.files)\n",
    "    \n",
    "        def __getitem__(self, i):\n",
    "            d = np.load(self.files[i])\n",
    "            X = d[\"X\"].astype(np.float32)      # (4096,10)\n",
    "            y = d[\"y\"].astype(np.int32)        # (4096,)\n",
    "            y = map_labels(y)\n",
    "            return torch.from_numpy(X), torch.from_numpy(y)\n",
    "    \n",
    "    # =========================\n",
    "    # 4) POINTNET++ OPS (FPS + kNN)\n",
    "    # =========================\n",
    "    def square_distance(src, dst):\n",
    "        # src: (B,N,3), dst: (B,M,3) -> (B,N,M)\n",
    "        dist = -2 * torch.matmul(src, dst.transpose(1,2))\n",
    "        dist += torch.sum(src**2, dim=-1).unsqueeze(-1)\n",
    "        dist += torch.sum(dst**2, dim=-1).unsqueeze(1)\n",
    "        return dist\n",
    "    \n",
    "    def index_points(points, idx):\n",
    "        # points: (B,N,C), idx: (B,S) or (B,S,K)\n",
    "        B = points.shape[0]\n",
    "        if idx.dim() == 2:\n",
    "            batch = torch.arange(B, device=points.device).view(B,1)\n",
    "            return points[batch, idx, :]\n",
    "        else:\n",
    "            batch = torch.arange(B, device=points.device).view(B,1,1)\n",
    "            return points[batch, idx, :]\n",
    "    \n",
    "    def farthest_point_sample(xyz, npoint):\n",
    "        # xyz: (B,N,3) -> (B,npoint)\n",
    "        device = xyz.device\n",
    "        B, N, _ = xyz.shape\n",
    "        centroids = torch.zeros(B, npoint, dtype=torch.long, device=device)\n",
    "        distance = torch.full((B, N), 1e10, device=device)\n",
    "        farthest = torch.randint(0, N, (B,), dtype=torch.long, device=device)\n",
    "        batch = torch.arange(B, device=device)\n",
    "    \n",
    "        for i in range(npoint):\n",
    "            centroids[:, i] = farthest\n",
    "            centroid = xyz[batch, farthest].view(B, 1, 3)\n",
    "            dist = torch.sum((xyz - centroid) ** 2, dim=-1)\n",
    "            distance = torch.minimum(distance, dist)\n",
    "            farthest = torch.max(distance, dim=-1)[1]\n",
    "        return centroids\n",
    "    \n",
    "    def knn_point(k, xyz, new_xyz):\n",
    "        # xyz: (B,N,3), new_xyz: (B,S,3) -> idx: (B,S,k)\n",
    "        dist = square_distance(new_xyz, xyz)   # (B,S,N)\n",
    "        idx = dist.topk(k=k, dim=-1, largest=False)[1]\n",
    "        return idx\n",
    "    \n",
    "    # =========================\n",
    "    # 5) POINTNET++ MODULES\n",
    "    # =========================\n",
    "    class SharedMLP(nn.Module):\n",
    "        def __init__(self, channels):\n",
    "            super().__init__()\n",
    "            layers = []\n",
    "            for i in range(len(channels)-1):\n",
    "                layers += [\n",
    "                    nn.Conv2d(channels[i], channels[i+1], 1),\n",
    "                    nn.BatchNorm2d(channels[i+1]),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                ]\n",
    "            self.net = nn.Sequential(*layers)\n",
    "    \n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "    \n",
    "    class SA_KNN(nn.Module):\n",
    "        def __init__(self, npoint, nsample, in_ch, mlp):\n",
    "            super().__init__()\n",
    "            self.npoint = npoint\n",
    "            self.nsample = nsample\n",
    "            self.mlp = SharedMLP([in_ch] + mlp)\n",
    "    \n",
    "        def forward(self, xyz, points):\n",
    "            # xyz: (B,N,3), points: (B,N,D) or None\n",
    "            fps_idx = farthest_point_sample(xyz, self.npoint)  # (B,S)\n",
    "            new_xyz = index_points(xyz, fps_idx)               # (B,S,3)\n",
    "    \n",
    "            knn_idx = knn_point(self.nsample, xyz, new_xyz)    # (B,S,K)\n",
    "            grouped_xyz = index_points(xyz, knn_idx)           # (B,S,K,3)\n",
    "            grouped_xyz = grouped_xyz - new_xyz.unsqueeze(2)   # normalize\n",
    "    \n",
    "            if points is not None:\n",
    "                grouped_points = index_points(points, knn_idx) # (B,S,K,D)\n",
    "                new_group = torch.cat([grouped_xyz, grouped_points], dim=-1)  # (B,S,K,3+D)\n",
    "            else:\n",
    "                new_group = grouped_xyz\n",
    "    \n",
    "            # (B,C,S,K)\n",
    "            new_group = new_group.permute(0,3,1,2).contiguous()\n",
    "            new_group = self.mlp(new_group)                     # (B,mlp[-1],S,K)\n",
    "            new_points = torch.max(new_group, dim=-1)[0]        # (B,mlp[-1],S)\n",
    "            new_points = new_points.transpose(1,2).contiguous() # (B,S,mlp[-1])\n",
    "            return new_xyz, new_points\n",
    "    \n",
    "    class FP(nn.Module):\n",
    "        def __init__(self, in_ch, mlp):\n",
    "            super().__init__()\n",
    "            layers = []\n",
    "            last = in_ch\n",
    "            for out in mlp:\n",
    "                layers += [nn.Conv1d(last, out, 1), nn.BatchNorm1d(out), nn.ReLU(inplace=True)]\n",
    "                last = out\n",
    "            self.mlp = nn.Sequential(*layers)\n",
    "    \n",
    "        def forward(self, xyz1, xyz2, p1, p2):\n",
    "            # xyz1: (B,N,3), xyz2: (B,S,3), p2: (B,S,D2), p1: (B,N,D1) or None\n",
    "            B, N, _ = xyz1.shape\n",
    "            _, S, _ = xyz2.shape\n",
    "    \n",
    "            if S == 1:\n",
    "                interpolated = p2.repeat(1, N, 1)\n",
    "            else:\n",
    "                dist = square_distance(xyz1, xyz2)                  # (B,N,S)\n",
    "                dist, idx = dist.topk(k=3, dim=-1, largest=False)   # (B,N,3)\n",
    "                dist = torch.clamp(dist, min=1e-10)\n",
    "                w = (1.0 / dist)\n",
    "                w = w / torch.sum(w, dim=-1, keepdim=True)\n",
    "    \n",
    "                grouped = index_points(p2, idx)                     # (B,N,3,D2)\n",
    "                interpolated = torch.sum(grouped * w.unsqueeze(-1), dim=2)  # (B,N,D2)\n",
    "    \n",
    "            if p1 is not None:\n",
    "                new_points = torch.cat([p1, interpolated], dim=-1)\n",
    "            else:\n",
    "                new_points = interpolated\n",
    "    \n",
    "            new_points = new_points.transpose(1,2).contiguous()     # (B,C,N)\n",
    "            new_points = self.mlp(new_points)\n",
    "            return new_points.transpose(1,2).contiguous()           # (B,N,out)\n",
    "    \n",
    "    class PointNet2SSGSeg(nn.Module):\n",
    "        def __init__(self, num_classes, in_ch=10):\n",
    "            super().__init__()\n",
    "            self.sa1 = SA_KNN(NPOINTS[0], NSAMPLE[0], in_ch, [64,64,128])     # xyz+feat packed as 10\n",
    "            self.sa2 = SA_KNN(NPOINTS[1], NSAMPLE[1], 3+128, [128,128,256])   # grouped xyz(3)+128\n",
    "            self.sa3 = SA_KNN(NPOINTS[2], NSAMPLE[2], 3+256, [256,256,512])\n",
    "    \n",
    "            self.fp3 = FP(256+512, [256,256])\n",
    "            self.fp2 = FP(128+256, [256,128])\n",
    "            self.fp1 = FP((in_ch-3)+128, [128,128,128])  # feat(7)+128\n",
    "    \n",
    "            self.cls = nn.Sequential(\n",
    "                nn.Conv1d(128, 128, 1),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Conv1d(128, num_classes, 1)\n",
    "            )\n",
    "    \n",
    "        def forward(self, X):\n",
    "            xyz = X[:, :, 0:3].contiguous()      # (B,N,3)\n",
    "            feat = X[:, :, 3:].contiguous()      # (B,N,7)\n",
    "    \n",
    "            # pack xyz_norm+feat inside SA1: we pass points=feat, SA will concat xyz_norm+points\n",
    "            l1_xyz, l1_p = self.sa1(xyz, feat)     # (B,1024,3), (B,1024,128)\n",
    "            l2_xyz, l2_p = self.sa2(l1_xyz, l1_p)  # (B,256,3),  (B,256,256)\n",
    "            l3_xyz, l3_p = self.sa3(l2_xyz, l2_p)  # (B,64,3),   (B,64,512)\n",
    "    \n",
    "            l2_fp = self.fp3(l2_xyz, l3_xyz, l2_p, l3_p)      # (B,256,256)\n",
    "            l1_fp = self.fp2(l1_xyz, l2_xyz, l1_p, l2_fp)     # (B,1024,128)\n",
    "            l0_fp = self.fp1(xyz, l1_xyz, feat, l1_fp)        # (B,N,128)\n",
    "    \n",
    "            x = l0_fp.transpose(1,2).contiguous()             # (B,128,N)\n",
    "            logits = self.cls(x).transpose(1,2).contiguous()  # (B,N,C)\n",
    "            return logits\n",
    "    \n",
    "    # =========================\n",
    "    # 6) LOSS + METRICS\n",
    "    # =========================\n",
    "    class WeightedCELoss(nn.Module):\n",
    "        def __init__(self, w, label_smooth, ignore_index):\n",
    "            super().__init__()\n",
    "            self.register_buffer(\"w\", w)\n",
    "            self.ls = label_smooth\n",
    "            self.ignore = ignore_index\n",
    "    \n",
    "        def forward(self, logits, y):\n",
    "            B, N, C = logits.shape\n",
    "            logits = logits.reshape(B*N, C)\n",
    "            y = y.reshape(B*N)\n",
    "            return F.cross_entropy(logits, y, weight=self.w, ignore_index=self.ignore, label_smoothing=self.ls)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def metrics(logits, y):\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        mask = (y != IGNORE_INDEX)\n",
    "        acc = (pred[mask] == y[mask]).float().mean().item() if mask.any() else 0.0\n",
    "    \n",
    "        # macro F1\n",
    "        f1s = []\n",
    "        for c in range(NUM_CLASSES):\n",
    "            tp = ((pred==c) & (y==c) & mask).sum().item()\n",
    "            fp = ((pred==c) & (y!=c) & mask).sum().item()\n",
    "            fn = ((pred!=c) & (y==c) & mask).sum().item()\n",
    "            denom = 2*tp + fp + fn\n",
    "            f1s.append((2*tp/denom) if denom>0 else 0.0)\n",
    "        return acc, float(sum(f1s)/len(f1s))\n",
    "    \n",
    "    # =========================\n",
    "    # 7) TRAIN\n",
    "    # =========================\n",
    "    def main():\n",
    "        print(\"Device:\", DEVICE)\n",
    "        if DEVICE==\"cuda\":\n",
    "            print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    \n",
    "        all_files = list_npz(ALL_BLOCKS_DIR)\n",
    "        if len(all_files) == 0:\n",
    "            raise RuntimeError(\"No .npz found. Check ALL_BLOCKS_DIR path.\")\n",
    "    \n",
    "        random.seed(42)\n",
    "        random.shuffle(all_files)\n",
    "        split = int(0.8 * len(all_files))\n",
    "        train_files = all_files[:split]\n",
    "        val_files = all_files[split:]\n",
    "    \n",
    "        print(\"All blocks :\", len(all_files))\n",
    "        print(\"Train blocks:\", len(train_files))\n",
    "        print(\"Val blocks  :\", len(val_files))\n",
    "    \n",
    "        train_ds = BlocksNPZ(train_files)\n",
    "        val_ds   = BlocksNPZ(val_files)\n",
    "    \n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,\n",
    "                                  num_workers=NUM_WORKERS, pin_memory=(DEVICE==\"cuda\"), drop_last=True)\n",
    "        val_loader   = DataLoader(val_ds, batch_size=BATCH, shuffle=False,\n",
    "                                  num_workers=NUM_WORKERS, pin_memory=(DEVICE==\"cuda\"))\n",
    "    \n",
    "        # Use your known rough counts (edit if you have better)\n",
    "        # For 5-class [1,2,3,6,12], your inference looked like:\n",
    "        counts_guess = {1: 1, 2: 7185068, 3: 33809, 6: 1274169, 12: 3881799}\n",
    "        w = class_weights_from_counts(counts_guess).to(DEVICE)\n",
    "        print(\"Class weights:\", w.detach().cpu().numpy())\n",
    "    \n",
    "        model = PointNet2SSGSeg(NUM_CLASSES, in_ch=IN_CH).to(DEVICE)\n",
    "    \n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS, eta_min=1e-5)\n",
    "        scaler = torch.amp.GradScaler(\"cuda\", enabled=(USE_AMP and DEVICE==\"cuda\"))\n",
    "        with torch.amp.autocast(\"cuda\", enabled=(USE_AMP and DEVICE==\"cuda\")):\n",
    "            logits = model(X)\n",
    "            loss = loss_fn(logits, y)\n",
    "            best_f1 = -1.0\n",
    "    \n",
    "    best_path = os.path.join(OUT_DIR, f\"pointnetpp_best_{NUM_CLASSES}cls.pt\")\n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        t0 = time.time()\n",
    "    \n",
    "        # ---------------- TRAIN ----------------\n",
    "        model.train()\n",
    "        tr_loss = 0.0\n",
    "    \n",
    "        for X, y in train_loader:\n",
    "            X = X.to(DEVICE, non_blocking=True).float()\n",
    "            y = y.to(DEVICE, non_blocking=True).long()\n",
    "    \n",
    "            opt.zero_grad(set_to_none=True)\n",
    "    \n",
    "            with torch.amp.autocast(\"cuda\", enabled=(USE_AMP and DEVICE == \"cuda\")):\n",
    "                logits = model(X)          # (B,N,C)\n",
    "                loss = loss_fn(logits, y)\n",
    "    \n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "    \n",
    "            tr_loss += loss.item()\n",
    "    \n",
    "        tr_loss /= max(len(train_loader), 1)\n",
    "    \n",
    "        # ---------------- VALIDATE ----------------\n",
    "        model.eval()\n",
    "        va_loss = 0.0\n",
    "        accs, f1s = [], []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X = X.to(DEVICE, non_blocking=True).float()\n",
    "                y = y.to(DEVICE, non_blocking=True).long()\n",
    "    \n",
    "                logits = model(X)\n",
    "                va_loss += loss_fn(logits, y).item()\n",
    "    \n",
    "                acc, f1 = metrics(logits, y)\n",
    "                accs.append(acc)\n",
    "                f1s.append(f1)\n",
    "    \n",
    "        va_loss /= max(len(val_loader), 1)\n",
    "        va_acc = float(np.mean(accs)) if accs else 0.0\n",
    "        va_f1 = float(np.mean(f1s)) if f1s else 0.0\n",
    "    \n",
    "        sch.step()\n",
    "        lr = opt.param_groups[0][\"lr\"]\n",
    "        dt = time.time() - t0\n",
    "    \n",
    "        print(f\"Epoch {epoch:03d} | {dt:5.1f}s | lr={lr:.2e} | train={tr_loss:.4f} | val={va_loss:.4f} | acc={va_acc:.4f} | macroF1={va_f1:.4f}\")\n",
    "    \n",
    "        if va_f1 > best_f1:\n",
    "            best_f1 = va_f1\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"num_classes\": NUM_CLASSES,\n",
    "                    \"raw_classes\": RAW_CLASSES,\n",
    "                    \"class_weights\": w.detach().cpu(),\n",
    "                },\n",
    "                best_path,\n",
    "            )\n",
    "            print(\"✅ Saved best:\", best_path)\n",
    "    \n",
    "    print(\"Done. Best macroF1:\", best_f1)\n",
    "    print(\"Best model:\", best_path)\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f55e93-1133-493c-b3af-dd1f898142e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n",
      "Total blocks: 27697\n",
      "Train blocks: 22157\n",
      "Val blocks: 5540\n",
      "Epoch 001 | train 0.4055 | val 0.3576 | acc 0.8403 | F1 0.7848\n",
      "Saved best model\n",
      "Epoch 002 | train 0.3479 | val 0.3407 | acc 0.8467 | F1 0.8067\n",
      "Saved best model\n",
      "Epoch 003 | train 0.3343 | val 0.3319 | acc 0.8503 | F1 0.8159\n",
      "Saved best model\n",
      "Training complete\n",
      "Best model: D:/lidarrrrr/anbu/dl_models\\pointnetpp_best.pt\n"
     ]
    }
   ],
   "source": [
    "import os, glob, random, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# =========================\n",
    "# SETTINGS (EDIT PATH ONLY)\n",
    "# =========================\n",
    "ALL_BLOCKS_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "OUT_DIR        = r\"D:/lidarrrrr/anbu/dl_models\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# classes used in your trained pipeline\n",
    "RAW_CLASSES = [1,2,3,6,12]\n",
    "MAP = {c:i for i,c in enumerate(RAW_CLASSES)}\n",
    "NUM_CLASSES = len(RAW_CLASSES)\n",
    "\n",
    "BATCH = 6\n",
    "EPOCHS = 3\n",
    "LR = 5e-4\n",
    "USE_AMP = False\n",
    "\n",
    "# =========================\n",
    "# DATASET\n",
    "# =========================\n",
    "def list_npz(folder):\n",
    "    return sorted(glob.glob(os.path.join(folder, \"*.npz\")))\n",
    "\n",
    "def map_labels(y_raw):\n",
    "    y = np.full_like(y_raw, -100)\n",
    "    for raw,new in MAP.items():\n",
    "        y[y_raw==raw] = new\n",
    "    return y\n",
    "\n",
    "class BlocksNPZ(Dataset):\n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        d = np.load(self.files[i])\n",
    "        X = d[\"X\"].astype(np.float32)\n",
    "\n",
    "        # normalize per block (safe method)\n",
    "        mean = X.mean(axis=0)\n",
    "        std  = X.std(axis=0) + 1e-6\n",
    "        X = (X - mean) / std\n",
    "\n",
    "        y = map_labels(d[\"y\"].astype(np.int32))\n",
    "        return torch.from_numpy(X), torch.from_numpy(y)\n",
    "\n",
    "# =========================\n",
    "# SIMPLE POINTNET++ MODEL\n",
    "# =========================\n",
    "class SimplePointNetPP(nn.Module):\n",
    "    def __init__(self, num_classes, in_ch=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Conv1d(in_ch,64,1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64,128,1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Conv1d(128,256,1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(256,128,1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128,num_classes,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X: (B,N,10)\n",
    "        x = X.transpose(1,2)     # (B,10,N)\n",
    "        x = self.mlp1(x)\n",
    "        x = self.mlp2(x)\n",
    "        x = self.head(x)\n",
    "        return x.transpose(1,2)  # (B,N,C)\n",
    "\n",
    "# =========================\n",
    "# METRICS\n",
    "# =========================\n",
    "def compute_metrics(logits, y):\n",
    "    pred = logits.argmax(dim=-1)\n",
    "    mask = y != -100\n",
    "    correct = (pred[mask] == y[mask]).sum().item()\n",
    "    total = mask.sum().item()\n",
    "    acc = correct/total if total>0 else 0\n",
    "\n",
    "    f1s=[]\n",
    "    for c in range(NUM_CLASSES):\n",
    "        tp = ((pred==c)&(y==c)&mask).sum().item()\n",
    "        fp = ((pred==c)&(y!=c)&mask).sum().item()\n",
    "        fn = ((pred!=c)&(y==c)&mask).sum().item()\n",
    "        denom = 2*tp+fp+fn\n",
    "        f1s.append((2*tp/denom) if denom>0 else 0)\n",
    "\n",
    "    return acc, sum(f1s)/len(f1s)\n",
    "\n",
    "# =========================\n",
    "# MAIN TRAINING\n",
    "# =========================\n",
    "def main():\n",
    "    print(\"Device:\", DEVICE)\n",
    "    if DEVICE==\"cuda\":\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "    # load files\n",
    "    all_files = list_npz(ALL_BLOCKS_DIR)\n",
    "    random.shuffle(all_files)\n",
    "\n",
    "    split = int(0.8 * len(all_files))\n",
    "    train_files = all_files[:split]\n",
    "    val_files   = all_files[split:]\n",
    "\n",
    "    print(\"Total blocks:\", len(all_files))\n",
    "    print(\"Train blocks:\", len(train_files))\n",
    "    print(\"Val blocks:\", len(val_files))\n",
    "\n",
    "    train_ds = BlocksNPZ(train_files)\n",
    "    val_ds   = BlocksNPZ(val_files)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=0, drop_last=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "\n",
    "    model = SimplePointNetPP(NUM_CLASSES).to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(USE_AMP and DEVICE==\"cuda\"))\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_path = os.path.join(OUT_DIR,\"pointnetpp_best.pt\")\n",
    "\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        t0=time.time()\n",
    "        model.train()\n",
    "        train_loss=0\n",
    "\n",
    "        for X,y in train_loader:\n",
    "            X=X.to(DEVICE).float()\n",
    "            y=y.to(DEVICE).long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\", enabled=(USE_AMP and DEVICE==\"cuda\")):\n",
    "                logits=model(X)\n",
    "                loss=F.cross_entropy(\n",
    "                    logits.reshape(-1,NUM_CLASSES),\n",
    "                    y.reshape(-1),\n",
    "                    ignore_index=-100\n",
    "                )\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss+=loss.item()\n",
    "\n",
    "        train_loss/=len(train_loader)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_loss=0\n",
    "        accs=[]; f1s=[]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X,y in val_loader:\n",
    "                X=X.to(DEVICE).float()\n",
    "                y=y.to(DEVICE).long()\n",
    "                logits=model(X)\n",
    "\n",
    "                loss=F.cross_entropy(\n",
    "                    logits.reshape(-1,NUM_CLASSES),\n",
    "                    y.reshape(-1),\n",
    "                    ignore_index=-100\n",
    "                )\n",
    "                val_loss+=loss.item()\n",
    "                acc,f1=compute_metrics(logits,y)\n",
    "                accs.append(acc)\n",
    "                f1s.append(f1)\n",
    "\n",
    "        val_loss/=len(val_loader)\n",
    "        val_acc=np.mean(accs)\n",
    "        val_f1=np.mean(f1s)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | train {train_loss:.4f} | val {val_loss:.4f} | acc {val_acc:.4f} | F1 {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1>best_f1:\n",
    "            best_f1=val_f1\n",
    "            torch.save(model.state_dict(),best_path)\n",
    "            print(\"Saved best model\")\n",
    "\n",
    "    print(\"Training complete\")\n",
    "    print(\"Best model:\",best_path)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b59836a5-daf0-4148-bfbd-5cc1e3dc5045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked: 300 Bad found: 0\n"
     ]
    }
   ],
   "source": [
    "import glob, os, numpy as np\n",
    "\n",
    "ALL_BLOCKS_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "files = sorted(glob.glob(os.path.join(ALL_BLOCKS_DIR, \"*.npz\")))\n",
    "\n",
    "bad = 0\n",
    "for f in files[:300]:  # check first 300 blocks\n",
    "    d = np.load(f)\n",
    "    X = d[\"X\"].astype(np.float32)\n",
    "    if not np.isfinite(X).all():\n",
    "        bad += 1\n",
    "        print(\"BAD:\", f, \"nan/inf count:\", np.sum(~np.isfinite(X)))\n",
    "        break\n",
    "\n",
    "print(\"Checked:\", min(300, len(files)), \"Bad found:\", bad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8213183f-da66-4faa-896b-f140402e9f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min [-1.378125e+02 -2.095000e+02 -8.483579e-01  0.000000e+00  0.000000e+00\n",
      "  1.000000e+00  1.000000e+00 -5.900000e+01  0.000000e+00  3.999996e-02]\n",
      "max [1.4343750e+02 1.5750000e+02 1.0561642e+01 1.0889999e+01 4.4083000e+04\n",
      " 5.0000000e+00 5.0000000e+00 5.4000000e+01 0.0000000e+00 1.6670000e+01]\n",
      "finite? True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, glob, os\n",
    "f = sorted(glob.glob(os.path.join(r\"D:/lidarrrrr/anbu/dl_dataset/blocks\",\"*.npz\")))[0]\n",
    "d=np.load(f)\n",
    "X=d[\"X\"].astype(np.float32)\n",
    "print(\"min\", np.nanmin(X,0))\n",
    "print(\"max\", np.nanmax(X,0))\n",
    "print(\"finite?\", np.isfinite(X).all())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64622e2d-f7c0-41c0-ad20-af4f5bfa4694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n",
      "Loaded: D:/lidarrrrr/anbu/DX3035724 S.GIUSTO000001.laz\n",
      "Points: 12374846\n",
      "X_full shape: (12374846, 10)\n",
      "Blocks created: 3222 (block_size=20.0, N=4096)\n",
      "Predicted blocks 0/3222\n",
      "Predicted blocks 160/3222\n",
      "Predicted blocks 320/3222\n",
      "Predicted blocks 480/3222\n",
      "Predicted blocks 640/3222\n",
      "Predicted blocks 800/3222\n",
      "Predicted blocks 960/3222\n",
      "Predicted blocks 1120/3222\n",
      "Predicted blocks 1280/3222\n",
      "Predicted blocks 1440/3222\n",
      "Predicted blocks 1600/3222\n",
      "Predicted blocks 1760/3222\n",
      "Predicted blocks 1920/3222\n",
      "Predicted blocks 2080/3222\n",
      "Predicted blocks 2240/3222\n",
      "Predicted blocks 2400/3222\n",
      "Predicted blocks 2560/3222\n",
      "Predicted blocks 2720/3222\n",
      "Predicted blocks 2880/3222\n",
      "Predicted blocks 3040/3222\n",
      "Predicted blocks 3200/3222\n",
      "✅ Wrote LAS: D:/lidarrrrr/anbu/New folder/pointnetpp_pred.las\n",
      "✅ Wrote LAZ: D:/lidarrrrr/anbu/New folder/pointnetpp_pred.laz\n",
      "Pred counts (raw): {1: 3792962, 2: 6918647, 3: 501661, 6: 1073272, 12: 88304}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import laspy\n",
    "\n",
    "# =========================\n",
    "# PATHS (EDIT)\n",
    "# =========================\n",
    "IN_LAZ   = r\"D:/lidarrrrr/anbu/DX3035724 S.GIUSTO000001.laz\"\n",
    "MODEL_PT = r\"D:/lidarrrrr/anbu/dl_models/pointnetpp_best.pt\"\n",
    "OUT_LAS  = r\"D:/lidarrrrr/anbu/New folder/pointnetpp_pred.las\"\n",
    "OUT_LAZ  = r\"D:/lidarrrrr/anbu/New folder/pointnetpp_pred.laz\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# =========================\n",
    "# CLASSES (5-class)\n",
    "# =========================\n",
    "RAW_CLASSES = [1, 2, 3, 6, 12]  # your trained mapping\n",
    "NUM_CLASSES = len(RAW_CLASSES)\n",
    "CONTIG_TO_RAW = {i: c for i, c in enumerate(RAW_CLASSES)}\n",
    "\n",
    "# =========================\n",
    "# INFERENCE SETTINGS\n",
    "# =========================\n",
    "POINTS_PER_BLOCK = 4096\n",
    "BLOCK_SIZE = 20.0      # same scale you used when it started working well\n",
    "BATCH_BLOCKS = 8       # RTX 3050 safe\n",
    "USE_AMP = True         # inference AMP is safe\n",
    "\n",
    "# =========================\n",
    "# MODEL (MUST MATCH TRAINING)\n",
    "# This matches your one-file training model: SimplePointNetPP\n",
    "# =========================\n",
    "class SimplePointNetPP(nn.Module):\n",
    "    def __init__(self, num_classes, in_ch=10):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 64, 1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(64, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(256, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(128, num_classes, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X: (B,N,10)\n",
    "        x = X.transpose(1, 2).contiguous()   # (B,10,N)\n",
    "        x = self.mlp1(x)\n",
    "        x = self.mlp2(x)\n",
    "        x = self.head(x)                     # (B,C,N)\n",
    "        return x.transpose(1, 2).contiguous()  # (B,N,C)\n",
    "\n",
    "def load_model():\n",
    "    model = SimplePointNetPP(NUM_CLASSES, in_ch=10).to(DEVICE)\n",
    "    state = torch.load(MODEL_PT, map_location=DEVICE)\n",
    "    # if you saved plain state_dict, this works:\n",
    "    if isinstance(state, dict) and \"model_state\" in state:\n",
    "        model.load_state_dict(state[\"model_state\"], strict=True)\n",
    "    else:\n",
    "        model.load_state_dict(state, strict=True)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# FEATURE BUILDING (10 features)\n",
    "# (x_local, y_local, z, hag, intensity, return_number,\n",
    "#  number_of_returns, scan_angle, deviation, slope)\n",
    "# =========================\n",
    "def get_dim_safe(las, name):\n",
    "    try:\n",
    "        return np.asarray(getattr(las, name))\n",
    "    except Exception:\n",
    "        try:\n",
    "            return np.asarray(las[name])  # extra dims\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def compute_hag_grid(x, y, z, cell=1.0, percentile=5):\n",
    "    x0 = x.min()\n",
    "    y0 = y.min()\n",
    "    gx = np.floor((x - x0) / cell).astype(np.int32)\n",
    "    gy = np.floor((y - y0) / cell).astype(np.int32)\n",
    "    key = (gx.astype(np.int64) << 32) ^ gy.astype(np.int64)\n",
    "\n",
    "    order = np.argsort(key)\n",
    "    key_s = key[order]\n",
    "    z_s = z[order]\n",
    "\n",
    "    ground = np.empty_like(z_s, dtype=np.float32)\n",
    "    start = 0\n",
    "    n = len(z_s)\n",
    "    while start < n:\n",
    "        end = start + 1\n",
    "        while end < n and key_s[end] == key_s[start]:\n",
    "            end += 1\n",
    "        g = np.percentile(z_s[start:end], percentile).astype(np.float32)\n",
    "        ground[start:end] = g\n",
    "        start = end\n",
    "\n",
    "    ground_unsorted = np.empty_like(ground)\n",
    "    ground_unsorted[order] = ground\n",
    "\n",
    "    hag = (z.astype(np.float32) - ground_unsorted.astype(np.float32))\n",
    "    hag = np.maximum(hag, 0.0)\n",
    "    return hag\n",
    "\n",
    "def compute_slope_proxy(x_local, y_local, hag):\n",
    "    # keep simple (same as your training: slope often not critical)\n",
    "    return np.zeros_like(hag, dtype=np.float32)\n",
    "\n",
    "def build_features_from_las(las):\n",
    "    x = np.asarray(las.x, dtype=np.float64)\n",
    "    y = np.asarray(las.y, dtype=np.float64)\n",
    "    z = np.asarray(las.z, dtype=np.float32)\n",
    "\n",
    "    x_local = (x - x.min()).astype(np.float32)\n",
    "    y_local = (y - y.min()).astype(np.float32)\n",
    "\n",
    "    intensity = get_dim_safe(las, \"intensity\")\n",
    "    intensity = intensity.astype(np.float32) if intensity is not None else np.zeros_like(z, dtype=np.float32)\n",
    "\n",
    "    return_num = get_dim_safe(las, \"return_number\")\n",
    "    return_num = return_num.astype(np.float32) if return_num is not None else np.zeros_like(z, dtype=np.float32)\n",
    "\n",
    "    num_returns = get_dim_safe(las, \"number_of_returns\")\n",
    "    num_returns = num_returns.astype(np.float32) if num_returns is not None else np.zeros_like(z, dtype=np.float32)\n",
    "\n",
    "    scan_angle = get_dim_safe(las, \"scan_angle_rank\")\n",
    "    if scan_angle is None:\n",
    "        scan_angle = get_dim_safe(las, \"scan_angle\")\n",
    "    scan_angle = scan_angle.astype(np.float32) if scan_angle is not None else np.zeros_like(z, dtype=np.float32)\n",
    "\n",
    "    deviation = get_dim_safe(las, \"deviation\")\n",
    "    deviation = deviation.astype(np.float32) if deviation is not None else np.zeros_like(z, dtype=np.float32)\n",
    "\n",
    "    hag = compute_hag_grid(x.astype(np.float32), y.astype(np.float32), z.astype(np.float32), cell=1.0, percentile=5)\n",
    "    slope = compute_slope_proxy(x_local, y_local, hag)\n",
    "\n",
    "    X = np.stack([\n",
    "        x_local, y_local, z, hag,\n",
    "        intensity, return_num, num_returns, scan_angle,\n",
    "        deviation, slope\n",
    "    ], axis=1).astype(np.float32)\n",
    "\n",
    "    return X\n",
    "\n",
    "# =========================\n",
    "# BLOCKING (cover-all)\n",
    "# =========================\n",
    "def build_blocks_cover_all(X, block_size=20.0, points_per_block=4096, seed=123):\n",
    "    x = X[:, 0]  # x_local\n",
    "    y = X[:, 1]  # y_local\n",
    "\n",
    "    gx = np.floor(x / block_size).astype(np.int32)\n",
    "    gy = np.floor(y / block_size).astype(np.int32)\n",
    "    key = gx.astype(np.int64) * 10_000_000 + gy.astype(np.int64)\n",
    "\n",
    "    order = np.argsort(key)\n",
    "    key_s = key[order]\n",
    "\n",
    "    blocks = []\n",
    "    start = 0\n",
    "    n = len(X)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    while start < n:\n",
    "        end = start + 1\n",
    "        while end < n and key_s[end] == key_s[start]:\n",
    "            end += 1\n",
    "\n",
    "        cell_indices = order[start:end]\n",
    "        if cell_indices.size > 0:\n",
    "            rng.shuffle(cell_indices)\n",
    "            for s in range(0, cell_indices.size, points_per_block):\n",
    "                chunk = cell_indices[s:s + points_per_block]\n",
    "                if chunk.size < points_per_block:\n",
    "                    pad = rng.choice(cell_indices, size=(points_per_block - chunk.size), replace=True)\n",
    "                    chunk = np.concatenate([chunk, pad], axis=0)\n",
    "                blocks.append(chunk.astype(np.int64))\n",
    "\n",
    "        start = end\n",
    "\n",
    "    return blocks\n",
    "\n",
    "def normalize_per_block(Xb: np.ndarray) -> np.ndarray:\n",
    "    # SAME as your training fix: per-block mean/std normalization\n",
    "    mean = Xb.mean(axis=0, keepdims=True)\n",
    "    std = Xb.std(axis=0, keepdims=True) + 1e-6\n",
    "    return (Xb - mean) / std\n",
    "\n",
    "# =========================\n",
    "# PREDICT (probability voting)\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def predict_full(model, X_full):\n",
    "    blocks = build_blocks_cover_all(X_full, block_size=BLOCK_SIZE, points_per_block=POINTS_PER_BLOCK)\n",
    "    print(f\"Blocks created: {len(blocks)} (block_size={BLOCK_SIZE}, N={POINTS_PER_BLOCK})\")\n",
    "\n",
    "    N = X_full.shape[0]\n",
    "    prob_sum = np.zeros((N, NUM_CLASSES), dtype=np.float32)\n",
    "\n",
    "    for i in range(0, len(blocks), BATCH_BLOCKS):\n",
    "        batch = blocks[i:i + BATCH_BLOCKS]\n",
    "\n",
    "        Xb = np.stack([X_full[idx] for idx in batch], axis=0)  # (B,4096,10)\n",
    "        # normalize per block\n",
    "        for b in range(Xb.shape[0]):\n",
    "            Xb[b] = normalize_per_block(Xb[b])\n",
    "\n",
    "        xb = torch.from_numpy(Xb).to(DEVICE, non_blocking=True).float()\n",
    "\n",
    "        with torch.amp.autocast(\"cuda\", enabled=(USE_AMP and DEVICE == \"cuda\")):\n",
    "            logits = model(xb)  # (B,4096,C)\n",
    "            probs = torch.softmax(logits, dim=-1).detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "        for b, idxs in enumerate(batch):\n",
    "            prob_sum[idxs] += probs[b]\n",
    "\n",
    "        if (i // BATCH_BLOCKS) % 20 == 0:\n",
    "            print(f\"Predicted blocks {i}/{len(blocks)}\")\n",
    "\n",
    "    pred_contig = prob_sum.argmax(axis=1).astype(np.int32)  # 0..4\n",
    "    return pred_contig\n",
    "\n",
    "# =========================\n",
    "# WRITE BACK\n",
    "# =========================\n",
    "def write_outputs(las, pred_contig):\n",
    "    pred_raw = np.vectorize(CONTIG_TO_RAW.get)(pred_contig).astype(np.uint8)\n",
    "    las.classification = pred_raw\n",
    "\n",
    "    os.makedirs(os.path.dirname(OUT_LAS), exist_ok=True)\n",
    "\n",
    "    las.write(OUT_LAS)\n",
    "    print(\"✅ Wrote LAS:\", OUT_LAS)\n",
    "\n",
    "    try:\n",
    "        las.write(OUT_LAZ)\n",
    "        print(\"✅ Wrote LAZ:\", OUT_LAZ)\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Could not write LAZ:\", e)\n",
    "        print(\"   Fix: pip install lazrs\")\n",
    "\n",
    "    u, c = np.unique(pred_raw, return_counts=True)\n",
    "    print(\"Pred counts (raw):\", dict(zip(u.tolist(), c.tolist())))\n",
    "\n",
    "def main():\n",
    "    print(\"Device:\", DEVICE)\n",
    "    if DEVICE == \"cuda\":\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "    las = laspy.read(IN_LAZ)\n",
    "    print(\"Loaded:\", IN_LAZ)\n",
    "    print(\"Points:\", len(las.x))\n",
    "\n",
    "    X_full = build_features_from_las(las)\n",
    "    print(\"X_full shape:\", X_full.shape)\n",
    "\n",
    "    model = load_model()\n",
    "    pred_contig = predict_full(model, X_full)\n",
    "\n",
    "    write_outputs(las, pred_contig)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be051a1-7d90-48d5-ba9f-dbf71c4ee429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lidar]",
   "language": "python",
   "name": "conda-env-lidar-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
