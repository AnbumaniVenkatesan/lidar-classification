{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab4f9e9a-6b28-4a21-9cbb-29b55d4709fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f22b8e7-ed75-407d-a892-586cd0153a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af19baa9-4954-4980-bd76-c2bf23e2aa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total blocks: 27697\n",
      "Train: 23542 Val: 4155\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "\n",
    "all_files = sorted(glob.glob(DATASET_DIR + \"/*.npz\"))\n",
    "print(\"Total blocks:\", len(all_files))\n",
    "\n",
    "# split\n",
    "split = int(len(all_files) * 0.85)\n",
    "train_files = all_files[:split]\n",
    "val_files   = all_files[split:]\n",
    "\n",
    "print(\"Train:\", len(train_files), \"Val:\", len(val_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ce103d4-36df-4b97-8469-443d2122ed31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [1, 2, 3, 6, 12]\n"
     ]
    }
   ],
   "source": [
    "def get_all_classes(files, max_files=200):\n",
    "    s = set()\n",
    "    for f in files[:max_files]:\n",
    "        d = np.load(f)\n",
    "        s.update(np.unique(d[\"y\"]).tolist())\n",
    "    return sorted(list(s))\n",
    "\n",
    "classes = get_all_classes(train_files)\n",
    "print(\"Classes:\", classes)\n",
    "\n",
    "class_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "idx_to_class = {i:c for c,i in class_to_idx.items()}\n",
    "\n",
    "NUM_CLASSES = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69324321-bf77-4e27-ae3b-4d393aee6834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total blocks: 27697\n",
      "Train: 23542 Val: 4155\n",
      "Example: D:/lidarrrrr/anbu/dl_dataset/blocks\\block_0000000.npz\n"
     ]
    }
   ],
   "source": [
    "all_files = sorted(glob.glob(os.path.join(DATASET_DIR, \"*.npz\")))\n",
    "if not all_files:\n",
    "    raise RuntimeError(\"No .npz blocks found in: \" + DATASET_DIR)\n",
    "\n",
    "print(\"Total blocks:\", len(all_files))\n",
    "\n",
    "split = int(len(all_files) * 0.85)\n",
    "train_files = all_files[:split]\n",
    "val_files   = all_files[split:]\n",
    "\n",
    "print(\"Train:\", len(train_files), \"Val:\", len(val_files))\n",
    "print(\"Example:\", train_files[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e017178b-b6ee-4176-be77-6dcdd03cfee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original classes found in blocks (sample): [1, 2, 3, 6, 7, 12, 13]\n",
      "NUM_CLASSES: 7\n",
      "class_to_idx: {1: 0, 2: 1, 3: 2, 6: 3, 7: 4, 12: 5, 13: 6}\n"
     ]
    }
   ],
   "source": [
    "def detect_classes(files, stride=30):\n",
    "    s = set()\n",
    "    for f in files[::stride]:\n",
    "        d = np.load(f)\n",
    "        s.update(np.unique(d[\"y\"]).tolist())\n",
    "    return sorted(list(s))\n",
    "\n",
    "orig_classes = detect_classes(train_files, stride=20)\n",
    "print(\"Original classes found in blocks (sample):\", orig_classes)\n",
    "\n",
    "# IMPORTANT: ensure DEFAULT class 1 exists for fallback\n",
    "if 1 not in orig_classes:\n",
    "    orig_classes = [1] + orig_classes\n",
    "\n",
    "orig_classes = sorted(orig_classes)\n",
    "\n",
    "class_to_idx = {c:i for i,c in enumerate(orig_classes)}\n",
    "idx_to_class = {i:c for c,i in class_to_idx.items()}\n",
    "\n",
    "NUM_CLASSES = len(orig_classes)\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES)\n",
    "print(\"class_to_idx:\", class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d579129b-f8fb-4eab-8efe-2746b755387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockDataset(Dataset):\n",
    "    def __init__(self, files, class_to_idx):\n",
    "        self.files = files\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.default_idx = self.class_to_idx.get(1, 0)  # fallback to class 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = np.load(self.files[idx])\n",
    "        X = d[\"X\"].astype(np.float32)   # (N, F)\n",
    "        y = d[\"y\"].astype(np.int64)     # (N,)\n",
    "\n",
    "        # ---- per-block normalization (VERY IMPORTANT) ----\n",
    "        mu = X.mean(axis=0, keepdims=True)\n",
    "        sd = X.std(axis=0, keepdims=True) + 1e-6\n",
    "        X = (X - mu) / sd\n",
    "\n",
    "        # ---- safe label remap (unknown -> default class 1) ----\n",
    "        y = np.array([self.class_to_idx.get(int(v), self.default_idx) for v in y], dtype=np.int64)\n",
    "\n",
    "        return torch.from_numpy(X), torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8e3221a-6276-4797-936b-b3cf5c336a89",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WeightedRandomSampler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m rare_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m rare_classes \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m orig_classes])\n\u001b[0;32m     13\u001b[0m weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([block_weight(f, rare_set) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m train_files], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 14\u001b[0m sampler \u001b[38;5;241m=\u001b[39m \u001b[43mWeightedRandomSampler\u001b[49m(weights, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_files), replacement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRare set used:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(rare_set)))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSampler weights stats:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(weights\u001b[38;5;241m.\u001b[39mmin()), \u001b[38;5;28mfloat\u001b[39m(weights\u001b[38;5;241m.\u001b[39mmax()))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WeightedRandomSampler' is not defined"
     ]
    }
   ],
   "source": [
    "def block_weight(npz_path, rare_set):\n",
    "    d = np.load(npz_path)\n",
    "    y = d[\"y\"]\n",
    "    u = set(np.unique(y).tolist())\n",
    "    # weight up if block contains rare classes\n",
    "    return 5.0 if len(u.intersection(rare_set)) > 0 else 1.0\n",
    "\n",
    "# Pick rare classes (you can edit)\n",
    "# Typically: 7(outliers), 9(sea), 10(bridge), 12(overlap), 13(lakes)\n",
    "rare_classes = {7, 9, 10, 12, 13, 6, 3}  # include 6/3 to strengthen building/veg\n",
    "rare_set = set([c for c in rare_classes if c in orig_classes])\n",
    "\n",
    "weights = np.array([block_weight(f, rare_set) for f in train_files], dtype=np.float32)\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(train_files), replacement=True)\n",
    "\n",
    "print(\"Rare set used:\", sorted(list(rare_set)))\n",
    "print(\"Sampler weights stats:\", float(weights.min()), float(weights.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a0f036-1a84-4298-81ec-db69dece3250",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8  # RTX3050 safe; try 12 if stable\n",
    "\n",
    "train_ds = BlockDataset(train_files, class_to_idx)\n",
    "val_ds   = BlockDataset(val_files, class_to_idx)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,          # balanced sampling\n",
    "    num_workers=0,            # Windows safe\n",
    "    pin_memory=(device.type==\"cuda\")\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=(device.type==\"cuda\")\n",
    ")\n",
    "\n",
    "Xb, yb = next(iter(train_loader))\n",
    "print(\"Batch:\", Xb.shape, yb.shape)\n",
    "print(\"Label range:\", yb.min().item(), yb.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baefb50e-6372-4ba3-b231-57eab3a8a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(loader, num_classes, max_batches=120):\n",
    "    counts = torch.zeros(num_classes, dtype=torch.float64)\n",
    "\n",
    "    it = iter(loader)\n",
    "    for _ in tqdm(range(max_batches), desc=\"Counting labels\"):\n",
    "        try:\n",
    "            _, y = next(it)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        y = y.reshape(-1)\n",
    "        for c in range(num_classes):\n",
    "            counts[c] += (y == c).sum().item()\n",
    "\n",
    "    counts = counts + 1.0\n",
    "    w = 1.0 / counts\n",
    "    w = w / w.sum() * num_classes\n",
    "    return w.float()\n",
    "\n",
    "class_weights = compute_class_weights(train_loader, NUM_CLASSES, max_batches=120).to(device)\n",
    "print(\"Class weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f384e8-8b82-4a04-bc5f-dd99452c39c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetSmall(nn.Module):\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Conv1d(in_features, 64, 1)\n",
    "        self.mlp2 = nn.Conv1d(64, 128, 1)\n",
    "        self.mlp3 = nn.Conv1d(128, 256, 1)\n",
    "        self.head1 = nn.Conv1d(256, 128, 1)\n",
    "        self.head2 = nn.Conv1d(128, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,N,F) -> (B,F,N)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu(self.mlp1(x))\n",
    "        x = F.relu(self.mlp2(x))\n",
    "        x = F.relu(self.mlp3(x))\n",
    "        x = F.relu(self.head1(x))\n",
    "        x = self.head2(x)         # (B,C,N)\n",
    "        x = x.transpose(1, 2)     # (B,N,C)\n",
    "        return x\n",
    "\n",
    "IN_FEATURES = Xb.shape[-1]\n",
    "model = PointNetSmall(IN_FEATURES, NUM_CLASSES).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00b26d-1621-4d44-a6a3-1efee04e2973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy\n",
    "\n",
    "ckpt = torch.load(MODEL_PATH, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "orig_classes = ckpt[\"orig_classes\"]\n",
    "idx_to_class = {i:c for i,c in enumerate(orig_classes)}\n",
    "NUM_CLASSES  = ckpt[\"num_classes\"]\n",
    "IN_FEATURES  = ckpt[\"in_features\"]\n",
    "\n",
    "print(\"Loaded model. Classes:\", orig_classes)\n",
    "\n",
    "las = laspy.read(PRED_IN_LAZ)\n",
    "xyz = np.vstack([las.x, las.y, las.z]).T.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f33bdec-15cf-47a6-9904-703f441845d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "\n",
    "EPOCHS = 25\n",
    "best_val = 1e9\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # ---- train ----\n",
    "    model.train()\n",
    "    tr_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} [train]\")\n",
    "\n",
    "    for X, y in pbar:\n",
    "        X = X.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        out = model(X)  # (B,N,C)\n",
    "\n",
    "        loss = criterion(out.reshape(-1, NUM_CLASSES), y.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        pbar.set_postfix(loss=float(loss.item()))\n",
    "\n",
    "    tr_loss /= len(train_loader)\n",
    "\n",
    "    # ---- val ----\n",
    "    model.eval()\n",
    "    va_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X = X.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            out = model(X)\n",
    "            loss = criterion(out.reshape(-1, NUM_CLASSES), y.reshape(-1))\n",
    "            va_loss += loss.item()\n",
    "\n",
    "            pred = out.argmax(dim=2)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.numel()\n",
    "\n",
    "    va_loss /= len(val_loader)\n",
    "    va_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch} | train_loss={tr_loss:.4f} | val_loss={va_loss:.4f} | val_acc={va_acc:.4f}\")\n",
    "\n",
    "    # save best\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        torch.save({\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"in_features\": IN_FEATURES,\n",
    "            \"num_classes\": NUM_CLASSES,\n",
    "            \"orig_classes\": orig_classes,   # to map back to LAS classes\n",
    "        }, MODEL_PATH)\n",
    "        print(\"✅ Saved best:\", MODEL_PATH)\n",
    "\n",
    "print(\"DONE training. Best val_loss:\", best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c7e06da-6930-438d-8ef4-5aa3d57a56d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X first row: [ 6.1937500e+01 -1.2750000e+02 -3.8357973e-02  5.9999943e-02\n",
      "  3.4995000e+04  1.0000000e+00  1.0000000e+00  2.0000000e+01\n",
      "  0.0000000e+00  1.0999999e+00]\n",
      "X mean: [-2.2071991e+00 -1.8538818e+00 -3.7940481e-04  2.1671166e-01\n",
      "  3.5122293e+04  1.0463867e+00  1.0629883e+00  5.8276367e-01\n",
      "  0.0000000e+00  8.9575773e-01]\n",
      "X std : [6.2053955e+01 9.5265366e+01 4.6353060e-01 4.3087605e-01 5.3501665e+03\n",
      " 2.6663089e-01 3.1083465e-01 2.5165798e+01 0.0000000e+00 9.3632799e-01]\n"
     ]
    }
   ],
   "source": [
    "d = np.load(train_files[0])\n",
    "print(\"X first row:\", d[\"X\"][0])\n",
    "print(\"X mean:\", d[\"X\"].mean(axis=0))\n",
    "print(\"X std :\", d[\"X\"].std(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c39b27b5-1c9b-4909-89dd-49fc70ad0390",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xpred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXpred first row:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mXpred\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXpred mean:\u001b[39m\u001b[38;5;124m\"\u001b[39m, Xpred[:\u001b[38;5;241m200000\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXpred std :\u001b[39m\u001b[38;5;124m\"\u001b[39m, Xpred[:\u001b[38;5;241m200000\u001b[39m]\u001b[38;5;241m.\u001b[39mstd(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Xpred' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Xpred first row:\", Xpred[0])\n",
    "print(\"Xpred mean:\", Xpred[:200000].mean(axis=0))\n",
    "print(\"Xpred std :\", Xpred[:200000].std(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4daa7476-8d6a-433d-a938-a25baa989df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Xpred created\n",
      "Xpred shape: (12374846, 10)\n",
      "Xpred first row: [ 6.1720931e+05  4.8372410e+06  1.5000000e+00  2.1000004e-01\n",
      "  3.4602000e+04  1.0000000e+00  1.0000000e+00 -1.8000000e+01\n",
      "  0.0000000e+00  3.4596376e-02]\n",
      "Xpred mean (first 200k): [ 6.1622381e+05  4.8452960e+06  1.7995203e+00  1.0270847e-01\n",
      "  3.9152746e+04  1.0005800e+00  1.0010350e+00 -1.1144625e+01\n",
      "  0.0000000e+00  3.7791225e-01]\n",
      "Xpred std  (first 200k): [9.7383453e+02 8.0233877e+03 1.6374594e-01 1.2724312e-01 1.2612378e+03\n",
      " 2.4069469e-02 3.2138728e-02 1.8500088e+01 0.0000000e+00 1.8576619e-01]\n",
      "Xpred col mins: [ 6.171695e+05  4.837240e+06  1.370000e+00  0.000000e+00  0.000000e+00\n",
      "  1.000000e+00  1.000000e+00 -3.900000e+01  0.000000e+00  0.000000e+00]\n",
      "Xpred col maxs: [6.1721131e+05 4.8372975e+06 7.3600001e+00 6.0700002e+00 4.5459000e+04\n",
      " 2.0000000e+00 2.0000000e+00 4.3000000e+01 0.0000000e+00 9.9999982e-01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import laspy\n",
    "\n",
    "IN_LAS_OR_LAZ = r\"D:/lidarrrrr/anbu/New folder/stage1_outputs/DX3035724_stage1_ground_v2.las\"\n",
    "CELL = 3.0  # same as training\n",
    "\n",
    "def safe_dim(las, name, fallback=0.0, dtype=np.float32):\n",
    "    # normal dimension\n",
    "    try:\n",
    "        return np.asarray(las[name]).astype(dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # scan angle variants\n",
    "    if name == \"scan_angle\":\n",
    "        for alt in [\"scan_angle\", \"scan_angle_rank\"]:\n",
    "            try:\n",
    "                return np.asarray(las[alt]).astype(dtype)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # fallback\n",
    "    return np.full(len(las.x), fallback, dtype=dtype)\n",
    "\n",
    "def compute_hag_and_slope(xyz, cls, cell=3.0, ground_class=2):\n",
    "    x, y, z = xyz[:,0], xyz[:,1], xyz[:,2]\n",
    "    minx, miny = x.min(), y.min()\n",
    "    gx = np.floor((x - minx)/cell).astype(np.int32)\n",
    "    gy = np.floor((y - miny)/cell).astype(np.int32)\n",
    "\n",
    "    # ground surface per cell\n",
    "    cell_min = {}\n",
    "    g_idx = np.where(cls == ground_class)[0]\n",
    "    for i in g_idx:\n",
    "        k = (gx[i], gy[i])\n",
    "        zi = float(z[i])\n",
    "        if (k not in cell_min) or (zi < cell_min[k]):\n",
    "            cell_min[k] = zi\n",
    "\n",
    "    hag = np.zeros(len(z), dtype=np.float32)\n",
    "    has_ground = np.zeros(len(z), dtype=bool)\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        if k in cell_min:\n",
    "            hag[i] = z[i] - cell_min[k]\n",
    "            has_ground[i] = True\n",
    "        else:\n",
    "            hag[i] = 0.0\n",
    "\n",
    "    # local range proxy for slope\n",
    "    cell_zmin, cell_zmax = {}, {}\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        zi = float(z[i])\n",
    "        if k not in cell_zmin:\n",
    "            cell_zmin[k] = zi\n",
    "            cell_zmax[k] = zi\n",
    "        else:\n",
    "            if zi < cell_zmin[k]: cell_zmin[k] = zi\n",
    "            if zi > cell_zmax[k]: cell_zmax[k] = zi\n",
    "\n",
    "    local_range = np.zeros(len(z), dtype=np.float32)\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        local_range[i] = float(cell_zmax[k] - cell_zmin[k])\n",
    "\n",
    "    slope = (hag / (local_range + 1e-6)).astype(np.float32)\n",
    "    return hag, slope\n",
    "\n",
    "# -------- build Xpred (10 features) --------\n",
    "las = laspy.read(IN_LAS_OR_LAZ)\n",
    "\n",
    "xyz = np.vstack([las.x, las.y, las.z]).T.astype(np.float32)\n",
    "cls = np.asarray(las.classification, dtype=np.int32)\n",
    "\n",
    "intensity = safe_dim(las, \"intensity\", fallback=0.0)\n",
    "ret_num   = safe_dim(las, \"return_number\", fallback=1.0)\n",
    "n_returns = safe_dim(las, \"number_of_returns\", fallback=1.0)\n",
    "scan_ang  = safe_dim(las, \"scan_angle\", fallback=0.0)\n",
    "\n",
    "# You used \"Deviation\" earlier; if not present it becomes zeros (OK)\n",
    "deviation = safe_dim(las, \"Deviation\", fallback=0.0)\n",
    "\n",
    "hag, slope = compute_hag_and_slope(xyz, cls, cell=CELL, ground_class=2)\n",
    "\n",
    "Xpred = np.stack([\n",
    "    xyz[:,0], xyz[:,1], xyz[:,2],\n",
    "    hag,\n",
    "    intensity,\n",
    "    ret_num,\n",
    "    n_returns,\n",
    "    scan_ang,\n",
    "    deviation,\n",
    "    slope\n",
    "], axis=1).astype(np.float32)\n",
    "\n",
    "print(\"✅ Xpred created\")\n",
    "print(\"Xpred shape:\", Xpred.shape)\n",
    "print(\"Xpred first row:\", Xpred[0])\n",
    "print(\"Xpred mean (first 200k):\", Xpred[:200000].mean(axis=0))\n",
    "print(\"Xpred std  (first 200k):\", Xpred[:200000].std(axis=0))\n",
    "print(\"Xpred col mins:\", Xpred[:200000].min(axis=0))\n",
    "print(\"Xpred col maxs:\", Xpred[:200000].max(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06bb2782-851c-461b-95a5-fd4f455c5c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Xpred_centered(las, cell=3.0):\n",
    "    xyz = np.vstack([las.x, las.y, las.z]).T.astype(np.float32)\n",
    "    cls = np.asarray(las.classification, dtype=np.int32)\n",
    "\n",
    "    intensity = get_dim(las, \"intensity\", fallback=0.0, dtype=np.float32)\n",
    "    ret_num   = get_dim(las, \"return_number\", fallback=1.0, dtype=np.float32)\n",
    "    n_returns = get_dim(las, \"number_of_returns\", fallback=1.0, dtype=np.float32)\n",
    "    scan_ang  = get_dim(las, \"scan_angle\", fallback=0.0, dtype=np.float32)\n",
    "    deviation = get_dim(las, \"Deviation\", fallback=0.0, dtype=np.float32)\n",
    "\n",
    "    hag, slope, has_ground = compute_hag_and_slope(xyz, cls, cell=cell)\n",
    "\n",
    "    # ✅ CENTER like training blocks (critical)\n",
    "    x = xyz[:,0]; y = xyz[:,1]; z = xyz[:,2]\n",
    "    x_rel = x - x.mean()\n",
    "    y_rel = y - y.mean()\n",
    "    z_rel = z - z.mean()\n",
    "\n",
    "    X = np.stack([\n",
    "        x_rel, y_rel, z_rel,\n",
    "        hag,\n",
    "        intensity,\n",
    "        ret_num,\n",
    "        n_returns,\n",
    "        scan_ang,\n",
    "        deviation,\n",
    "        slope\n",
    "    ], axis=1).astype(np.float32)\n",
    "\n",
    "    return X, xyz, cls, has_ground\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82aae5b8-d0fb-43aa-b022-d3fab3c990fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Xpred, xyz, cls_in, has_ground \u001b[38;5;241m=\u001b[39m \u001b[43mmake_Xpred_centered\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCELL\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m, in \u001b[0;36mmake_Xpred_centered\u001b[1;34m(las, cell)\u001b[0m\n\u001b[0;32m      2\u001b[0m xyz \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([las\u001b[38;5;241m.\u001b[39mx, las\u001b[38;5;241m.\u001b[39my, las\u001b[38;5;241m.\u001b[39mz])\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(las\u001b[38;5;241m.\u001b[39mclassification, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m----> 5\u001b[0m intensity \u001b[38;5;241m=\u001b[39m \u001b[43mget_dim\u001b[49m(las, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintensity\u001b[39m\u001b[38;5;124m\"\u001b[39m, fallback\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      6\u001b[0m ret_num   \u001b[38;5;241m=\u001b[39m get_dim(las, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_number\u001b[39m\u001b[38;5;124m\"\u001b[39m, fallback\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      7\u001b[0m n_returns \u001b[38;5;241m=\u001b[39m get_dim(las, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber_of_returns\u001b[39m\u001b[38;5;124m\"\u001b[39m, fallback\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_dim' is not defined"
     ]
    }
   ],
   "source": [
    "Xpred, xyz, cls_in, has_ground = make_Xpred_centered(las, cell=CELL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b821f4f-896e-4265-b4b5-4ebdef63a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_dim(las, name, fallback=0.0, dtype=np.float32):\n",
    "    # Try normal dimension name\n",
    "    try:\n",
    "        return np.asarray(las[name]).astype(dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # scan angle sometimes named differently\n",
    "    if name == \"scan_angle\":\n",
    "        for alt in [\"scan_angle\", \"scan_angle_rank\"]:\n",
    "            try:\n",
    "                return np.asarray(las[alt]).astype(dtype)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # RGB or extra bytes may not exist\n",
    "    return np.full(len(las.x), fallback, dtype=dtype)\n",
    "\n",
    "\n",
    "def compute_hag_and_slope(xyz, cls, cell=3.0, ground_class=2):\n",
    "    x, y, z = xyz[:,0], xyz[:,1], xyz[:,2]\n",
    "\n",
    "    minx, miny = x.min(), y.min()\n",
    "    gx = np.floor((x - minx)/cell).astype(np.int32)\n",
    "    gy = np.floor((y - miny)/cell).astype(np.int32)\n",
    "\n",
    "    # ground surface\n",
    "    cell_min = {}\n",
    "    g_idx = np.where(cls == ground_class)[0]\n",
    "\n",
    "    for i in g_idx:\n",
    "        k = (gx[i], gy[i])\n",
    "        zi = z[i]\n",
    "        if (k not in cell_min) or (zi < cell_min[k]):\n",
    "            cell_min[k] = zi\n",
    "\n",
    "    hag = np.zeros(len(z), dtype=np.float32)\n",
    "\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        if k in cell_min:\n",
    "            hag[i] = z[i] - cell_min[k]\n",
    "\n",
    "    # slope proxy\n",
    "    cell_zmin = {}\n",
    "    cell_zmax = {}\n",
    "\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        zi = z[i]\n",
    "        if k not in cell_zmin:\n",
    "            cell_zmin[k] = zi\n",
    "            cell_zmax[k] = zi\n",
    "        else:\n",
    "            cell_zmin[k] = min(cell_zmin[k], zi)\n",
    "            cell_zmax[k] = max(cell_zmax[k], zi)\n",
    "\n",
    "    local_range = np.zeros(len(z), dtype=np.float32)\n",
    "\n",
    "    for i in range(len(z)):\n",
    "        k = (gx[i], gy[i])\n",
    "        local_range[i] = cell_zmax[k] - cell_zmin[k]\n",
    "\n",
    "    slope = hag / (local_range + 1e-6)\n",
    "\n",
    "    return hag, slope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0d84842-5294-4f80-8080-81d1417d9226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Xpred_centered(las, cell=3.0):\n",
    "    xyz = np.vstack([las.x, las.y, las.z]).T.astype(np.float32)\n",
    "    cls = np.asarray(las.classification, dtype=np.int32)\n",
    "\n",
    "    intensity = get_dim(las, \"intensity\", fallback=0.0)\n",
    "    ret_num   = get_dim(las, \"return_number\", fallback=1.0)\n",
    "    n_returns = get_dim(las, \"number_of_returns\", fallback=1.0)\n",
    "    scan_ang  = get_dim(las, \"scan_angle\", fallback=0.0)\n",
    "    deviation = get_dim(las, \"Deviation\", fallback=0.0)\n",
    "\n",
    "    hag, slope = compute_hag_and_slope(xyz, cls, cell)\n",
    "\n",
    "    # IMPORTANT: center coordinates like training\n",
    "    x_rel = xyz[:,0] - xyz[:,0].mean()\n",
    "    y_rel = xyz[:,1] - xyz[:,1].mean()\n",
    "    z_rel = xyz[:,2] - xyz[:,2].mean()\n",
    "\n",
    "    X = np.stack([\n",
    "        x_rel,\n",
    "        y_rel,\n",
    "        z_rel,\n",
    "        hag,\n",
    "        intensity,\n",
    "        ret_num,\n",
    "        n_returns,\n",
    "        scan_ang,\n",
    "        deviation,\n",
    "        slope\n",
    "    ], axis=1).astype(np.float32)\n",
    "\n",
    "    return X, xyz, cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8dca3d6-d32d-47e0-a4a5-6c371a082817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xpred shape: (12374846, 10)\n",
      "Xpred mins: [-429.625    -577.5       -10.199776    0.          0.          1.\n",
      "    1.       -114.          0.          0.      ]\n",
      "Xpred maxs: [5.959375e+02 4.015000e+02 8.576022e+01 8.705000e+01 6.553500e+04\n",
      " 5.000000e+00 5.000000e+00 9.800000e+01 0.000000e+00 1.000000e+00]\n"
     ]
    }
   ],
   "source": [
    "Xpred, xyz, cls_in = make_Xpred_centered(las, cell=CELL)\n",
    "\n",
    "print(\"Xpred shape:\", Xpred.shape)\n",
    "print(\"Xpred mins:\", Xpred.min(axis=0))\n",
    "print(\"Xpred maxs:\", Xpred.max(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02cec28c-08bd-4087-933e-02c40a78b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Xpred_centered(las, cell=3.0):\n",
    "    xyz = np.vstack([las.x, las.y, las.z]).T.astype(np.float32)\n",
    "    cls = np.asarray(las.classification, dtype=np.int32)\n",
    "\n",
    "    intensity = get_dim(las, \"intensity\", fallback=0.0)\n",
    "    ret_num   = get_dim(las, \"return_number\", fallback=1.0)\n",
    "    n_returns = get_dim(las, \"number_of_returns\", fallback=1.0)\n",
    "    scan_ang  = get_dim(las, \"scan_angle\", fallback=0.0)\n",
    "    deviation = get_dim(las, \"Deviation\", fallback=0.0)\n",
    "\n",
    "    hag, slope = compute_hag_and_slope(xyz, cls, cell)\n",
    "\n",
    "    # --- CLIPS (important) ---\n",
    "    intensity = np.clip(intensity, 0, 45000).astype(np.float32)\n",
    "    scan_ang  = np.clip(scan_ang, -60, 60).astype(np.float32)\n",
    "    hag       = np.clip(hag, 0, 40).astype(np.float32)\n",
    "\n",
    "    # --- CENTER coords (must match training) ---\n",
    "    x_rel = xyz[:,0] - xyz[:,0].mean()\n",
    "    y_rel = xyz[:,1] - xyz[:,1].mean()\n",
    "    z_rel = xyz[:,2] - xyz[:,2].mean()\n",
    "    z_rel = np.clip(z_rel, -20, 20).astype(np.float32)\n",
    "\n",
    "\n",
    "    X = np.stack([\n",
    "        x_rel, y_rel, z_rel,\n",
    "        hag,\n",
    "        intensity,\n",
    "        ret_num,\n",
    "        n_returns,\n",
    "        scan_ang,\n",
    "        deviation,\n",
    "        slope\n",
    "    ], axis=1).astype(np.float32)\n",
    "\n",
    "    return X, xyz, cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52459a01-0317-49c3-9345-b17c20ee5441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xpred shape: (12374846, 10)\n",
      "Xpred mins: [-429.625    -577.5       -10.199776    0.          0.          1.\n",
      "    1.        -60.          0.          0.      ]\n",
      "Xpred maxs: [5.959375e+02 4.015000e+02 2.000000e+01 4.000000e+01 4.500000e+04\n",
      " 5.000000e+00 5.000000e+00 6.000000e+01 0.000000e+00 1.000000e+00]\n"
     ]
    }
   ],
   "source": [
    "Xpred, xyz, cls_in = make_Xpred_centered(las, cell=CELL)\n",
    "\n",
    "print(\"Xpred shape:\", Xpred.shape)\n",
    "print(\"Xpred mins:\", Xpred.min(axis=0))\n",
    "print(\"Xpred maxs:\", Xpred.max(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43a62d02-b685-4769-8300-365c8ce07a4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xmean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# after Xpred built and normalized:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m Xn \u001b[38;5;241m=\u001b[39m (Xpred \u001b[38;5;241m-\u001b[39m \u001b[43mXmean\u001b[49m[\u001b[38;5;28;01mNone\u001b[39;00m,:]) \u001b[38;5;241m/\u001b[39m (Xstd[\u001b[38;5;28;01mNone\u001b[39;00m,:] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRED norm mean:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mround(Xn[:\u001b[38;5;241m200000\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRED norm std :\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mround(Xn[:\u001b[38;5;241m200000\u001b[39m]\u001b[38;5;241m.\u001b[39mstd(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m3\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Xmean' is not defined"
     ]
    }
   ],
   "source": [
    "# after Xpred built and normalized:\n",
    "Xn = (Xpred - Xmean[None,:]) / (Xstd[None,:] + 1e-6)\n",
    "\n",
    "print(\"PRED norm mean:\", np.round(Xn[:200000].mean(axis=0), 3))\n",
    "print(\"PRED norm std :\", np.round(Xn[:200000].std(axis=0), 3))\n",
    "print(\"PRED norm min :\", np.round(Xn[:200000].min(axis=0), 3))\n",
    "print(\"PRED norm max :\", np.round(Xn[:200000].max(axis=0), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb9c193f-3485-46bf-829c-c343ddda62d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xmean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Xn \u001b[38;5;241m=\u001b[39m (Xpred \u001b[38;5;241m-\u001b[39m \u001b[43mXmean\u001b[49m[\u001b[38;5;28;01mNone\u001b[39;00m,:]) \u001b[38;5;241m/\u001b[39m (Xstd[\u001b[38;5;28;01mNone\u001b[39;00m,:] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRED norm mean:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mround(Xn[:\u001b[38;5;241m200000\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRED norm std :\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mround(Xn[:\u001b[38;5;241m200000\u001b[39m]\u001b[38;5;241m.\u001b[39mstd(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\u001b[38;5;241m3\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Xmean' is not defined"
     ]
    }
   ],
   "source": [
    "Xn = (Xpred - Xmean[None,:]) / (Xstd[None,:] + 1e-6)\n",
    "\n",
    "print(\"PRED norm mean:\", np.round(Xn[:200000].mean(axis=0),3))\n",
    "print(\"PRED norm std :\", np.round(Xn[:200000].std(axis=0),3))\n",
    "print(\"PRED norm min :\", np.round(Xn[:200000].min(axis=0),3))\n",
    "print(\"PRED norm max :\", np.round(Xn[:200000].max(axis=0),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b7e8a72-bea4-4111-9c7e-a3d476b5bc2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_PT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[43mMODEL_PT\u001b[49m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m Xmean \u001b[38;5;241m=\u001b[39m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXmean\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m Xstd  \u001b[38;5;241m=\u001b[39m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXstd\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MODEL_PT' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ckpt = torch.load(MODEL_PT, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "Xmean = ckpt[\"Xmean\"].astype(\"float32\")\n",
    "Xstd  = ckpt[\"Xstd\"].astype(\"float32\")\n",
    "\n",
    "print(\"Xmean:\", Xmean)\n",
    "print(\"Xstd :\", Xstd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f220caa-bbec-4d06-9723-061a2a970933",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PT = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e959eb0-b2c6-4e1a-94de-0c3ae15a9343",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Xmean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(MODEL_PT, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m Xmean \u001b[38;5;241m=\u001b[39m \u001b[43mckpt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mXmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      7\u001b[0m Xstd  \u001b[38;5;241m=\u001b[39m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXstd\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded normalization values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Xmean'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "ckpt = torch.load(MODEL_PT, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "Xmean = ckpt[\"Xmean\"].astype(np.float32)\n",
    "Xstd  = ckpt[\"Xstd\"].astype(np.float32)\n",
    "\n",
    "print(\"Loaded normalization values\")\n",
    "print(\"Xmean:\", Xmean)\n",
    "print(\"Xstd :\", Xstd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ec7e090-9f94-493e-b8a0-ec866797c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Xmean = np.array([\n",
    "-2.2071991e+00, -1.8538818e+00, -3.7940481e-04, 2.1671166e-01,\n",
    "3.5122293e+04, 1.0463867e+00, 1.0629883e+00, 5.8276367e-01,\n",
    "0.0000000e+00, 8.9575773e-01\n",
    "], dtype=np.float32)\n",
    "\n",
    "Xstd = np.array([\n",
    "6.2053955e+01, 9.5265366e+01, 4.6353060e-01, 4.3087605e-01,\n",
    "5.3501665e+03, 2.6663089e-01, 3.1083465e-01, 2.5165798e+01,\n",
    "1.0, 9.3632799e-01\n",
    "], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80c5a76f-2fdc-4e0d-937d-8b81f6e60e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED norm mean: [-2.424 -5.779 -2.416 -0.264  0.753 -0.172 -0.2   -0.466  0.    -0.553]\n",
      "PRED norm std : [0.154 0.129 0.353 0.295 0.236 0.09  0.103 0.735 0.    0.198]\n"
     ]
    }
   ],
   "source": [
    "Xn = (Xpred - Xmean[None,:]) / (Xstd[None,:] + 1e-6)\n",
    "\n",
    "print(\"PRED norm mean:\", np.round(Xn[:200000].mean(axis=0),3))\n",
    "print(\"PRED norm std :\", np.round(Xn[:200000].std(axis=0),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8093e48d-f5ac-4c27-9e3f-5322915e0833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xmean: [ 2.4770452e-01  4.4691383e+01 -1.3224035e-05  3.2253304e-01\n",
      "  3.6415633e+04  1.0537578e+00  1.0711324e+00 -1.5879369e+02\n",
      "  8.8634253e-01  9.2550790e-01]\n",
      "Xstd : [2.9587668e+02 2.3407706e+02 3.8826628e+00 1.0009346e+00 6.9030566e+03\n",
      " 2.6688504e-01 3.0669588e-01 9.5657166e+02 4.5917196e+00 1.5607965e+00]\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "\n",
    "BLOCK_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "files = sorted(glob.glob(os.path.join(BLOCK_DIR, \"*.npz\")))\n",
    "assert files, \"No blocks found\"\n",
    "\n",
    "# Welford streaming mean/std\n",
    "n = 0\n",
    "mean = None\n",
    "M2 = None\n",
    "\n",
    "for fp in files:\n",
    "    d = np.load(fp)\n",
    "    X = d[\"X\"].astype(np.float64)  # (4096,10)\n",
    "    if mean is None:\n",
    "        mean = np.zeros(X.shape[1], dtype=np.float64)\n",
    "        M2   = np.zeros(X.shape[1], dtype=np.float64)\n",
    "\n",
    "    n_batch = X.shape[0]\n",
    "    n_new = n + n_batch\n",
    "\n",
    "    batch_mean = X.mean(axis=0)\n",
    "    batch_var  = X.var(axis=0)\n",
    "\n",
    "    delta = batch_mean - mean\n",
    "    mean = mean + delta * (n_batch / n_new)\n",
    "\n",
    "    # combine variances\n",
    "    M2 = M2 + batch_var * n_batch + (delta**2) * (n * n_batch / n_new)\n",
    "\n",
    "    n = n_new\n",
    "\n",
    "std = np.sqrt(M2 / max(n - 1, 1))\n",
    "\n",
    "mean = mean.astype(np.float32)\n",
    "std  = std.astype(np.float32)\n",
    "\n",
    "print(\"Xmean:\", mean)\n",
    "print(\"Xstd :\", std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f708c947-c769-4786-b1d9-60b097adf367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: CUDA\n",
      "Targets: [2, 3, 4, 5, 6] | Others-> 1\n",
      "======================================================================\n",
      "Real RandLA-Net style training (classes 2..6 only)\n",
      "======================================================================\n",
      "Device: CUDA\n",
      "GPU: NVIDIA GeForce RTX 3050\n",
      "Indexing: pt013390.laz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   0%|                                                           | 0/600 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 6.00 GiB of which 3.01 GiB is free. Of the allocated memory 512.86 MiB is allocated by PyTorch, and 291.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 510\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTargets:\u001b[39m\u001b[33m\"\u001b[39m, CFG[\u001b[33m\"\u001b[39m\u001b[33mtarget_classes\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33m| Others->\u001b[39m\u001b[33m\"\u001b[39m, CFG[\u001b[33m\"\u001b[39m\u001b[33mother_class\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    509\u001b[39m \u001b[38;5;66;03m# 1) Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[38;5;66;03m# 2) Predict\u001b[39;00m\n\u001b[32m    513\u001b[39m predict_raw(CFG)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 367\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m    364\u001b[39m     logits = model(xyz, feat, knn)                     \u001b[38;5;66;03m# (B,N,5)\u001b[39;00m\n\u001b[32m    365\u001b[39m     loss = criterion(logits.permute(\u001b[32m0\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m1\u001b[39m), lbl)        \u001b[38;5;66;03m# CE expects (B,C,N)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m scaler.step(opt)\n\u001b[32m    369\u001b[39m scaler.update()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 6.00 GiB of which 3.01 GiB is free. Of the allocated memory 512.86 MiB is allocated by PyTorch, and 291.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os, sys, math, time, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import laspy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "CFG = {\n",
    "    # Training files (classified)\n",
    "    \"train_files\": [\n",
    "        r\"d:\\lidarrrrr\\anbu\\pt013390.laz\",\n",
    "        # add more here...\n",
    "    ],\n",
    "\n",
    "    # Raw file (unclassified) to predict\n",
    "    \"raw_file\": r\"d:\\lidarrrrr\\anbu\\DX3035724 S.GIUSTO000001.laz\",\n",
    "    \"out_file\": r\"d:\\lidarrrrr\\anbu\\classified_output_randlanet.laz\",\n",
    "\n",
    "    # Save\n",
    "    \"model_path\": r\"d:\\lidarrrrr\\anbu\\randlanet_2_6.pth\",\n",
    "\n",
    "    # Classes\n",
    "    \"target_classes\": [2, 3, 4, 5, 6],   # Train only these\n",
    "    \"other_class\": 1,                    # Map others to 1 in training, and for low-confidence in prediction\n",
    "\n",
    "    # Block sampling\n",
    "    \"tile_size\": 50.0,      # meters (XY tiling)\n",
    "    \"n_points\": 4096,       # points per block\n",
    "    \"k\": 16,                # neighbors\n",
    "\n",
    "    # Train\n",
    "    \"epochs\": 30,\n",
    "    \"steps_per_epoch\": 600,  # how many blocks per epoch\n",
    "    \"batch_size\": 6,          # 6 blocks x 4096 points ~ good for 6GB\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "\n",
    "    # Predict\n",
    "    \"pred_batch_points\": 4096,  # predict in blocks of 4096\n",
    "    \"conf_thresh\": 0.0,         # set e.g. 0.55 to map low-confidence to class 1\n",
    "\n",
    "    # Device\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# UTIL: safe torch save/load with PyTorch 2.6+\n",
    "# ============================================================\n",
    "\n",
    "def torch_save_safe(obj, path):\n",
    "    # Ensure pure python ints in mappings\n",
    "    if \"label_map\" in obj:\n",
    "        obj[\"label_map\"] = {int(k): int(v) for k, v in obj[\"label_map\"].items()}\n",
    "    torch.save(obj, path)\n",
    "\n",
    "def torch_load_trusted(path, device):\n",
    "    # If you saved it yourself, safe to load weights_only=False\n",
    "    return torch.load(path, weights_only=False, map_location=device)\n",
    "\n",
    "# ============================================================\n",
    "# DATA: load laz + build XY tiles index for fast block sampling\n",
    "# ============================================================\n",
    "\n",
    "class LidarFileIndex:\n",
    "    \"\"\"\n",
    "    Loads one LAZ into memory (XYZ + a few features + labels),\n",
    "    creates tile-based indexing for fast block sampling.\n",
    "    \"\"\"\n",
    "    def __init__(self, path, tile_size):\n",
    "        self.path = path\n",
    "        self.tile_size = float(tile_size)\n",
    "\n",
    "        las = laspy.read(path)\n",
    "        dims = list(las.point_format.dimension_names)\n",
    "\n",
    "        self.x = np.asarray(las.x, dtype=np.float32)\n",
    "        self.y = np.asarray(las.y, dtype=np.float32)\n",
    "        self.z = np.asarray(las.z, dtype=np.float32)\n",
    "        self.n = self.x.shape[0]\n",
    "\n",
    "        # Optional point features (keep simple + common)\n",
    "        self.intensity = np.asarray(las.intensity, dtype=np.float32) if \"intensity\" in dims else np.zeros(self.n, np.float32)\n",
    "        self.return_num = np.asarray(las.return_number, dtype=np.float32) if \"return_number\" in dims else np.ones(self.n, np.float32)\n",
    "        self.num_returns = np.asarray(las.number_of_returns, dtype=np.float32) if \"number_of_returns\" in dims else np.ones(self.n, np.float32)\n",
    "\n",
    "        self.cls = np.asarray(las.classification, dtype=np.int32) if \"classification\" in dims else np.zeros(self.n, np.int32)\n",
    "\n",
    "        # Tile indexing\n",
    "        xmin, ymin = float(self.x.min()), float(self.y.min())\n",
    "        self.xmin, self.ymin = xmin, ymin\n",
    "\n",
    "        ix = np.floor((self.x - xmin) / self.tile_size).astype(np.int32)\n",
    "        iy = np.floor((self.y - ymin) / self.tile_size).astype(np.int32)\n",
    "\n",
    "        # combine into a single int key\n",
    "        # (assumes ix,iy not insanely large; works for typical LiDAR extents)\n",
    "        key = (ix.astype(np.int64) << 32) ^ (iy.astype(np.int64) & 0xFFFFFFFF)\n",
    "\n",
    "        order = np.argsort(key)\n",
    "        key_sorted = key[order]\n",
    "\n",
    "        uniq, start, counts = np.unique(key_sorted, return_index=True, return_counts=True)\n",
    "\n",
    "        self.order = order\n",
    "        self.tile_keys = uniq\n",
    "        self.tile_start = start\n",
    "        self.tile_counts = counts\n",
    "\n",
    "    def sample_block_indices(self, n_points):\n",
    "        \"\"\"\n",
    "        Sample a tile then sample n_points indices from that tile.\n",
    "        \"\"\"\n",
    "        # pick random tile\n",
    "        t = np.random.randint(0, len(self.tile_keys))\n",
    "        s = int(self.tile_start[t])\n",
    "        c = int(self.tile_counts[t])\n",
    "        idx = self.order[s:s+c]\n",
    "\n",
    "        if c >= n_points:\n",
    "            pick = np.random.choice(idx, n_points, replace=False)\n",
    "        else:\n",
    "            pick = np.random.choice(idx, n_points, replace=True)\n",
    "        return pick\n",
    "\n",
    "# ============================================================\n",
    "# PREPROCESS: build block (points + features) + kNN graph\n",
    "# ============================================================\n",
    "\n",
    "def build_block(file_idx: LidarFileIndex, idx, target_set):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      pts: (N,3) float32\n",
    "      feats: (N,F) float32\n",
    "      labels: (N,) int64 mapped to 0..4 (for classes 2..6) OR -1 for ignored\n",
    "      knn_idx: (N,k) int64 neighbor indices inside block [0..N-1]\n",
    "    \"\"\"\n",
    "    x = file_idx.x[idx]; y = file_idx.y[idx]; z = file_idx.z[idx]\n",
    "    pts = np.stack([x, y, z], axis=1).astype(np.float32)\n",
    "\n",
    "    # Normalize coordinates inside block (center + scale)\n",
    "    center = pts.mean(axis=0, keepdims=True)\n",
    "    pts0 = pts - center\n",
    "    scale = np.max(np.linalg.norm(pts0[:, :2], axis=1)) + 1e-6\n",
    "    ptsn = pts0 / scale\n",
    "\n",
    "    # Features: (x,y,z norm) + intensity norm + return ratio\n",
    "    inten = file_idx.intensity[idx]\n",
    "    inten = (inten - inten.min()) / (inten.max() - inten.min() + 1e-6)\n",
    "\n",
    "    ret_ratio = file_idx.return_num[idx] / (file_idx.num_returns[idx] + 1e-6)\n",
    "\n",
    "    feats = np.stack([ptsn[:,0], ptsn[:,1], ptsn[:,2], inten, ret_ratio], axis=1).astype(np.float32)\n",
    "\n",
    "    # Labels: keep only target classes; others ignored (=-1)\n",
    "    cls = file_idx.cls[idx].astype(np.int32)\n",
    "    labels = np.full(cls.shape, -1, dtype=np.int64)\n",
    "\n",
    "    # Map {2,3,4,5,6} -> {0,1,2,3,4}\n",
    "    targets = sorted(list(target_set))\n",
    "    map_to = {c:i for i,c in enumerate(targets)}\n",
    "    m = np.isin(cls, targets)\n",
    "    labels[m] = np.vectorize(map_to.get)(cls[m]).astype(np.int64)\n",
    "\n",
    "    # kNN in 3D within block\n",
    "    tree = KDTree(ptsn, leaf_size=32)\n",
    "    knn_idx = tree.query(ptsn, k=CFG[\"k\"], return_distance=False).astype(np.int64)\n",
    "\n",
    "    return pts, feats, labels, knn_idx\n",
    "\n",
    "# ============================================================\n",
    "# RandLA-Net style building blocks\n",
    "# (neighbor aggregation + relative position encoding + attentive pooling)\n",
    "# ============================================================\n",
    "\n",
    "def index_points(points, idx):\n",
    "    # points: (B,N,C), idx: (B,N,k) -> (B,N,k,C)\n",
    "    B, N, C = points.shape\n",
    "    _, _, k = idx.shape\n",
    "    idx_expand = idx.unsqueeze(-1).expand(-1, -1, -1, C)\n",
    "    return torch.gather(points.unsqueeze(1).expand(B, N, N, C), 2, idx_expand)\n",
    "\n",
    "class SharedMLP(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_ch, out_ch, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_ch)\n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,N,C)\n",
    "        B,N,C = x.shape\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x.reshape(B*N, -1)).reshape(B, N, -1)\n",
    "        return self.act(x)\n",
    "\n",
    "class AttentivePooling(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.score = nn.Linear(in_ch, 1, bias=False)\n",
    "        self.mlp = SharedMLP(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, neigh_feat):\n",
    "        # neigh_feat: (B,N,k,C)\n",
    "        score = self.score(neigh_feat)              # (B,N,k,1)\n",
    "        attn = torch.softmax(score, dim=2)          # (B,N,k,1)\n",
    "        agg = torch.sum(attn * neigh_feat, dim=2)   # (B,N,C)\n",
    "        return self.mlp(agg)                        # (B,N,out)\n",
    "\n",
    "class LocalFeatureAggregation(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.mlp1 = SharedMLP(in_ch, out_ch//2)\n",
    "        self.mlp2 = SharedMLP(out_ch//2 + 10, out_ch//2)  # + relative pos enc\n",
    "        self.pool = AttentivePooling(out_ch//2, out_ch)\n",
    "        self.short = SharedMLP(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, xyz, feat, knn_idx):\n",
    "        \"\"\"\n",
    "        xyz: (B,N,3), feat: (B,N,Cin), knn_idx: (B,N,k)\n",
    "        \"\"\"\n",
    "        # initial\n",
    "        f1 = self.mlp1(feat)  # (B,N,out/2)\n",
    "\n",
    "        # gather neighbors\n",
    "        neigh_xyz = index_points(xyz, knn_idx)  # (B,N,k,3)\n",
    "        center_xyz = xyz.unsqueeze(2)           # (B,N,1,3)\n",
    "\n",
    "        rel = neigh_xyz - center_xyz            # (B,N,k,3)\n",
    "        dist = torch.norm(rel, dim=-1, keepdim=True)  # (B,N,k,1)\n",
    "\n",
    "        # position encoding (RandLA style-ish)\n",
    "        # [rel(xyz)=3, dist=1, center_xyz=3, neigh_xyz=3] -> 10\n",
    "        pe = torch.cat([rel, dist, center_xyz.expand_as(neigh_xyz), neigh_xyz], dim=-1)  # (B,N,k,10)\n",
    "\n",
    "        neigh_f = index_points(f1, knn_idx)     # (B,N,k,out/2)\n",
    "        x = torch.cat([neigh_f, pe], dim=-1)    # (B,N,k,out/2+10)\n",
    "\n",
    "        # mlp on neighbor features\n",
    "        B,N,k,C = x.shape\n",
    "        x2 = self.mlp2(x.reshape(B, N*k, C)).reshape(B, N, k, -1)  # (B,N,k,out/2)\n",
    "\n",
    "        # attentive pool to get per-point feature\n",
    "        out = self.pool(x2)                     # (B,N,out)\n",
    "\n",
    "        # residual\n",
    "        return out + self.short(feat)\n",
    "\n",
    "class RandLANetLite(nn.Module):\n",
    "    \"\"\"\n",
    "    Real neighbor-based encoder/decoder (lite) for block classification.\n",
    "    Input: xyz (B,N,3), feat (B,N,F), knn (B,N,k)\n",
    "    Output: logits (B,N,5) for classes 2..6 mapped -> 0..4\n",
    "    \"\"\"\n",
    "    def __init__(self, in_feat, num_classes=5, k=16):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.lfa1 = LocalFeatureAggregation(in_feat, 64)\n",
    "        self.lfa2 = LocalFeatureAggregation(64, 128)\n",
    "        self.lfa3 = LocalFeatureAggregation(128, 256)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(256, 256, bias=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, xyz, feat, knn_idx):\n",
    "        x = self.lfa1(xyz, feat, knn_idx)\n",
    "        x = self.lfa2(xyz, x, knn_idx)\n",
    "        x = self.lfa3(xyz, x, knn_idx)\n",
    "\n",
    "        B,N,C = x.shape\n",
    "        logits = self.head(x.reshape(B*N, C)).reshape(B, N, -1)\n",
    "        return logits\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN LOOP (block sampling)\n",
    "# ============================================================\n",
    "\n",
    "def train_model(cfg):\n",
    "    device = cfg[\"device\"]\n",
    "    print(\"=\"*70)\n",
    "    print(\"Real RandLA-Net style training (classes 2..6 only)\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Device:\", device.upper())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "    target_set = set(cfg[\"target_classes\"])\n",
    "    label_map = {i: c for i, c in enumerate(sorted(list(target_set)))}  # 0..4 -> 2..6\n",
    "\n",
    "    # Load file indexes into memory\n",
    "    files = []\n",
    "    for p in cfg[\"train_files\"]:\n",
    "        if not os.path.exists(p):\n",
    "            print(\"Missing:\", p)\n",
    "            continue\n",
    "        print(\"Indexing:\", os.path.basename(p))\n",
    "        files.append(LidarFileIndex(p, cfg[\"tile_size\"]))\n",
    "\n",
    "    if not files:\n",
    "        raise RuntimeError(\"No training files found.\")\n",
    "\n",
    "    # Model\n",
    "    model = RandLANetLite(in_feat=5, num_classes=5, k=cfg[\"k\"]).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n",
    "    sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg[\"epochs\"])\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "    # class weights (helps imbalance)\n",
    "    class_weights = torch.tensor([1.0, 1.2, 1.2, 1.2, 1.2], device=device)  # tweak if needed\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "    best = 0.0\n",
    "\n",
    "    for epoch in range(cfg[\"epochs\"]):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        seen = 0\n",
    "\n",
    "        pbar = tqdm(range(cfg[\"steps_per_epoch\"]), desc=f\"Epoch {epoch+1}/{cfg['epochs']}\", ncols=100)\n",
    "        for _ in pbar:\n",
    "            # Build a batch of blocks\n",
    "            B = cfg[\"batch_size\"]\n",
    "            xyz_b, feat_b, lbl_b, knn_b = [], [], [], []\n",
    "\n",
    "            tries = 0\n",
    "            while len(xyz_b) < B and tries < B*10:\n",
    "                tries += 1\n",
    "                f = files[np.random.randint(0, len(files))]\n",
    "                idx = f.sample_block_indices(cfg[\"n_points\"])\n",
    "                pts, feats, labels, knn = build_block(f, idx, target_set)\n",
    "\n",
    "                # If block has almost no target labels, skip\n",
    "                if (labels >= 0).sum() < cfg[\"n_points\"] * 0.2:\n",
    "                    continue\n",
    "\n",
    "                xyz_b.append(pts.astype(np.float32))\n",
    "                feat_b.append(feats.astype(np.float32))\n",
    "                lbl_b.append(labels.astype(np.int64))\n",
    "                knn_b.append(knn.astype(np.int64))\n",
    "\n",
    "            if len(xyz_b) < B:\n",
    "                continue\n",
    "\n",
    "            xyz = torch.from_numpy(np.stack(xyz_b)).to(device)     # (B,N,3)\n",
    "            feat = torch.from_numpy(np.stack(feat_b)).to(device)   # (B,N,5)\n",
    "            lbl  = torch.from_numpy(np.stack(lbl_b)).to(device)    # (B,N)\n",
    "            knn  = torch.from_numpy(np.stack(knn_b)).to(device)    # (B,N,k)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "                logits = model(xyz, feat, knn)                     # (B,N,5)\n",
    "                loss = criterion(logits.permute(0,2,1), lbl)        # CE expects (B,C,N)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "\n",
    "            # accuracy on labeled points only\n",
    "            with torch.no_grad():\n",
    "                pred = logits.argmax(dim=-1)\n",
    "                mask = (lbl >= 0)\n",
    "                correct = (pred[mask] == lbl[mask]).float().sum().item()\n",
    "                total = mask.float().sum().item() + 1e-6\n",
    "                acc = correct / total\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_acc += acc\n",
    "            seen += 1\n",
    "            pbar.set_postfix(loss=running_loss/max(seen,1), acc=running_acc/max(seen,1))\n",
    "\n",
    "        sch.step()\n",
    "\n",
    "        # simple \"best\" based on train acc (you can add real validation later)\n",
    "        epoch_acc = running_acc / max(seen, 1)\n",
    "        if epoch_acc > best:\n",
    "            best = epoch_acc\n",
    "            torch_save_safe({\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"label_map\": label_map,       # 0..4 -> 2..6\n",
    "                \"in_feat\": 5,\n",
    "                \"k\": int(cfg[\"k\"]),\n",
    "            }, cfg[\"model_path\"])\n",
    "\n",
    "        print(f\"Epoch {epoch+1} done | avg acc={epoch_acc*100:.2f}% | best={best*100:.2f}%\")\n",
    "\n",
    "    print(\"Saved best model:\", cfg[\"model_path\"])\n",
    "    return cfg[\"model_path\"]\n",
    "\n",
    "# ============================================================\n",
    "# PREDICT (tile-based, block inference)\n",
    "# ============================================================\n",
    "\n",
    "def predict_raw(cfg):\n",
    "    device = cfg[\"device\"]\n",
    "    ckpt = torch_load_trusted(cfg[\"model_path\"], device)\n",
    "    label_map = ckpt[\"label_map\"]  # 0..4 -> 2..6\n",
    "    inv = {int(k): int(v) for k, v in label_map.items()}\n",
    "\n",
    "    model = RandLANetLite(in_feat=ckpt[\"in_feat\"], num_classes=5, k=ckpt[\"k\"]).to(device)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.eval()\n",
    "\n",
    "    raw_path = cfg[\"raw_file\"]\n",
    "    if not os.path.exists(raw_path):\n",
    "        raise FileNotFoundError(raw_path)\n",
    "\n",
    "    las = laspy.read(raw_path)\n",
    "    dims = list(las.point_format.dimension_names)\n",
    "    x = np.asarray(las.x, dtype=np.float32)\n",
    "    y = np.asarray(las.y, dtype=np.float32)\n",
    "    z = np.asarray(las.z, dtype=np.float32)\n",
    "    n = len(x)\n",
    "\n",
    "    intensity = np.asarray(las.intensity, dtype=np.float32) if \"intensity\" in dims else np.zeros(n, np.float32)\n",
    "    return_num = np.asarray(las.return_number, dtype=np.float32) if \"return_number\" in dims else np.ones(n, np.float32)\n",
    "    num_returns = np.asarray(las.number_of_returns, dtype=np.float32) if \"number_of_returns\" in dims else np.ones(n, np.float32)\n",
    "\n",
    "    # Tile indexing for prediction\n",
    "    tile = float(cfg[\"tile_size\"])\n",
    "    xmin, ymin = float(x.min()), float(y.min())\n",
    "    ix = np.floor((x - xmin)/tile).astype(np.int32)\n",
    "    iy = np.floor((y - ymin)/tile).astype(np.int32)\n",
    "    key = (ix.astype(np.int64) << 32) ^ (iy.astype(np.int64) & 0xFFFFFFFF)\n",
    "\n",
    "    order = np.argsort(key)\n",
    "    key_sorted = key[order]\n",
    "    uniq, start, counts = np.unique(key_sorted, return_index=True, return_counts=True)\n",
    "\n",
    "    out_cls = np.full(n, cfg[\"other_class\"], dtype=np.uint8)\n",
    "\n",
    "    print(\"\\nPredicting tiles...\")\n",
    "    for t in tqdm(range(len(uniq)), ncols=100):\n",
    "        s = int(start[t])\n",
    "        c = int(counts[t])\n",
    "        idx_tile = order[s:s+c]\n",
    "\n",
    "        # Process tile points in chunks of N=4096\n",
    "        # (keeps GPU memory stable)\n",
    "        for s2 in range(0, c, cfg[\"pred_batch_points\"]):\n",
    "            e2 = min(s2 + cfg[\"pred_batch_points\"], c)\n",
    "            idx = idx_tile[s2:e2]\n",
    "\n",
    "            pts = np.stack([x[idx], y[idx], z[idx]], axis=1).astype(np.float32)\n",
    "            center = pts.mean(axis=0, keepdims=True)\n",
    "            pts0 = pts - center\n",
    "            scale = np.max(np.linalg.norm(pts0[:, :2], axis=1)) + 1e-6\n",
    "            ptsn = pts0 / scale\n",
    "\n",
    "            inten = intensity[idx]\n",
    "            inten = (inten - inten.min()) / (inten.max() - inten.min() + 1e-6)\n",
    "            ret_ratio = return_num[idx] / (num_returns[idx] + 1e-6)\n",
    "\n",
    "            feat = np.stack([ptsn[:,0], ptsn[:,1], ptsn[:,2], inten, ret_ratio], axis=1).astype(np.float32)\n",
    "\n",
    "            # kNN inside this chunk (local geometry)\n",
    "            tree = KDTree(ptsn, leaf_size=32)\n",
    "            knn = tree.query(ptsn, k=cfg[\"k\"], return_distance=False).astype(np.int64)\n",
    "\n",
    "            xyz_t = torch.from_numpy(pts.astype(np.float32)).unsqueeze(0).to(device)      # (1,N,3) original scale ok\n",
    "            feat_t= torch.from_numpy(feat).unsqueeze(0).to(device)                        # (1,N,5)\n",
    "            knn_t = torch.from_numpy(knn).unsqueeze(0).to(device)                         # (1,N,k)\n",
    "\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "                logits = model(xyz_t, feat_t, knn_t)[0]     # (N,5)\n",
    "                prob = torch.softmax(logits, dim=-1)\n",
    "                conf, pred = torch.max(prob, dim=-1)        # (N,)\n",
    "\n",
    "            pred = pred.cpu().numpy().astype(np.int32)\n",
    "            conf = conf.cpu().numpy().astype(np.float32)\n",
    "\n",
    "            # map 0..4 -> 2..6\n",
    "            mapped = np.vectorize(inv.get)(pred).astype(np.uint8)\n",
    "\n",
    "            # optional confidence threshold -> class 1\n",
    "            if cfg[\"conf_thresh\"] > 0:\n",
    "                mapped[conf < float(cfg[\"conf_thresh\"])] = np.uint8(cfg[\"other_class\"])\n",
    "\n",
    "            out_cls[idx] = mapped\n",
    "\n",
    "    # Write output\n",
    "    out = laspy.LasData(las.header)\n",
    "    out.points = las.points\n",
    "    out.classification = out_cls\n",
    "    out.write(cfg[\"out_file\"])\n",
    "    print(\"Saved:\", cfg[\"out_file\"])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Device:\", CFG[\"device\"].upper())\n",
    "    print(\"Targets:\", CFG[\"target_classes\"], \"| Others->\", CFG[\"other_class\"])\n",
    "\n",
    "    # 1) Train\n",
    "    train_model(CFG)\n",
    "\n",
    "    # 2) Predict\n",
    "    predict_raw(CFG)\n",
    "\n",
    "    print(\"\\nDone ✅ Open output in CloudCompare (Colors -> Classification).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26028f08-fecb-46a7-ae74-b3b8af557cba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
