{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da69ab5f-3a71-4e5b-b09f-e8675b2e3292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4063f700-ec4e-4a01-a7d4-8cc59abc6caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887734e1-9177-4425-b56c-2c229bbc5322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, file_list):\n",
    "        self.files = file_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = np.load(self.files[idx])\n",
    "\n",
    "        points = data[\"points\"].astype(np.float32)\n",
    "        labels = data[\"labels\"].astype(np.int64)\n",
    "\n",
    "        return torch.from_numpy(points), torch.from_numpy(labels)\n",
    "\n",
    "print(\"Dataset class loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88665553-d565-4ca7-bcbc-dd4f34d53370",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_files\u001b[49m), \u001b[38;5;28mlen\u001b[39m(val_files))\n",
      "\u001b[31mNameError\u001b[39m: name 'train_files' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(train_files), len(val_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "634917b3-99ad-4c4f-930f-40378efc3c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total blocks found: 3543\n",
      "Example file: D:/lidarrrrr/anbu/dl_dataset/blocks\\block_0000000.npz\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "DATA_DIR =  r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"   # folder where your .npz blocks are saved\n",
    "\n",
    "all_files = sorted(glob.glob(os.path.join(DATA_DIR, \"*.npz\")))\n",
    "\n",
    "print(\"Total blocks found:\", len(all_files))\n",
    "print(\"Example file:\", all_files[0] if len(all_files) else \"No files found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "245a0c22-78d6-4d4c-9ed6-880fd85a2e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: 3011\n",
      "Val files: 532\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(all_files)\n",
    "\n",
    "split = int(0.85 * len(all_files))\n",
    "train_files = all_files[:split]\n",
    "val_files   = all_files[split:]\n",
    "\n",
    "print(\"Train files:\", len(train_files))\n",
    "print(\"Val files:\", len(val_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a3ef260-5a95-4487-9ff0-8ab044c68bbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PointCloudDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_dataset = \u001b[43mPointCloudDataset\u001b[49m(train_files)\n\u001b[32m      2\u001b[39m val_dataset   = PointCloudDataset(val_files)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDatasets ready\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'PointCloudDataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = PointCloudDataset(train_files)\n",
    "val_dataset   = PointCloudDataset(val_files)\n",
    "\n",
    "print(\"Datasets ready\")\n",
    "print(\"Train:\", len(train_dataset))\n",
    "print(\"Val:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1326f6d3-5765-4f3c-b893-bf219d2a56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = np.load(self.files[idx])\n",
    "\n",
    "        points = data[\"points\"].astype(np.float32)   # shape: (N, features)\n",
    "        labels = data[\"labels\"].astype(np.int64)     # shape: (N,)\n",
    "\n",
    "        points = torch.from_numpy(points)\n",
    "        labels = torch.from_numpy(labels)\n",
    "\n",
    "        return points, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f915d58-bff1-4d1f-a27e-3fa4ddc72ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total blocks: 3543\n",
      "Train: 3011\n",
      "Val: 532\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "DATA_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "\n",
    "all_files = sorted(glob.glob(os.path.join(DATA_DIR, \"*.npz\")))\n",
    "print(\"Total blocks:\", len(all_files))\n",
    "\n",
    "random.shuffle(all_files)\n",
    "\n",
    "split = int(0.85 * len(all_files))\n",
    "train_files = all_files[:split]\n",
    "val_files   = all_files[split:]\n",
    "\n",
    "print(\"Train:\", len(train_files))\n",
    "print(\"Val:\", len(val_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a48945fa-9019-4d1e-bb4f-07643110ce57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets ready\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PointCloudDataset(train_files)\n",
    "val_dataset   = PointCloudDataset(val_files)\n",
    "\n",
    "print(\"Datasets ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e34ced3-1c9d-49f3-a099-0930063fcf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders ready\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"DataLoaders ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f06b852a-7678-469b-a4d0-9c4c7a13334f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'points is not a file in the archive'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     12\u001b[39m val_loader = DataLoader(\n\u001b[32m     13\u001b[39m     val_dataset,\n\u001b[32m     14\u001b[39m     batch_size=BATCH_SIZE,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# quick test\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m pts, lbl = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBatch points:\u001b[39m\u001b[33m\"\u001b[39m, pts.shape, \u001b[33m\"\u001b[39m\u001b[33mBatch labels:\u001b[39m\u001b[33m\"\u001b[39m, lbl.shape)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLabel sample:\u001b[39m\u001b[33m\"\u001b[39m, torch.unique(lbl)[:\u001b[32m15\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:741\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    740\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    744\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    745\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    746\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    747\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:801\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    800\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m801\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    803\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     52\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     56\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mPointCloudDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m     13\u001b[39m     data = np.load(\u001b[38;5;28mself\u001b[39m.files[idx])\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     points = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpoints\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.astype(np.float32)   \u001b[38;5;66;03m# shape: (N, features)\u001b[39;00m\n\u001b[32m     16\u001b[39m     labels = data[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m].astype(np.int64)     \u001b[38;5;66;03m# shape: (N,)\u001b[39;00m\n\u001b[32m     18\u001b[39m     points = torch.from_numpy(points)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:243\u001b[39m, in \u001b[36mNpzFile.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    241\u001b[39m     key = \u001b[38;5;28mself\u001b[39m._files[key]\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a file in the archive\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.zip.open(key) \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'points is not a file in the archive'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 8  # if CUDA OOM, reduce to 6\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,     # IMPORTANT on Windows/Jupyter\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# quick test\n",
    "pts, lbl = next(iter(train_loader))\n",
    "print(\"Batch points:\", pts.shape, \"Batch labels:\", lbl.shape)\n",
    "print(\"Label sample:\", torch.unique(lbl)[:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63fc595-9afa-4153-9a0b-2febbdd5339d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in npz: ['X', 'y']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "d = np.load(all_files[0])\n",
    "print(\"Keys in npz:\", d.files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9323408-1afa-4d35-bf47-a1b21a17fe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = np.load(self.files[idx])\n",
    "\n",
    "        # --- support both naming styles ---\n",
    "        if \"points\" in d.files:\n",
    "            points = d[\"points\"].astype(np.float32)\n",
    "        elif \"X\" in d.files:\n",
    "            points = d[\"X\"].astype(np.float32)\n",
    "        else:\n",
    "            raise KeyError(f\"No points array found. Available keys: {d.files}\")\n",
    "\n",
    "        if \"labels\" in d.files:\n",
    "            labels = d[\"labels\"].astype(np.int64)\n",
    "        elif \"y\" in d.files:\n",
    "            labels = d[\"y\"].astype(np.int64)\n",
    "        else:\n",
    "            raise KeyError(f\"No labels array found. Available keys: {d.files}\")\n",
    "\n",
    "        return torch.from_numpy(points), torch.from_numpy(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "013d9737-3763-4f3c-b5ef-24fe44713c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch points: torch.Size([8, 4096, 10])\n",
      "Batch labels: torch.Size([8, 4096])\n",
      "Unique labels sample: tensor([1, 2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = PointCloudDataset(train_files)\n",
    "val_dataset   = PointCloudDataset(val_files)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "pts, lbl = next(iter(train_loader))\n",
    "print(\"Batch points:\", pts.shape)\n",
    "print(\"Batch labels:\", lbl.shape)\n",
    "print(\"Unique labels sample:\", torch.unique(lbl)[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d06858-e7d4-4811-ac02-8a37927ca68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.10.0+cu126\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ee80654-b720-408d-b38a-6c920cff4968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total blocks: 3543\n",
      "Example: D:/lidarrrrr/anbu/dl_dataset/blocks\\block_0000000.npz\n",
      "Train: 3011 Val: 532\n",
      "Keys in your npz: ['X', 'y']\n",
      "Batch points shape: torch.Size([8, 4096, 10])\n",
      "Batch labels shape: torch.Size([8, 4096])\n",
      "Unique labels (sample): tensor([ 1,  2,  3,  6,  7, 12])\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# 1) Imports + file list\n",
    "# ============================\n",
    "import os, glob, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "DATA_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"  # <-- your blocks folder\n",
    "\n",
    "all_files = sorted(glob.glob(os.path.join(DATA_DIR, \"*.npz\")))\n",
    "print(\"Total blocks:\", len(all_files))\n",
    "print(\"Example:\", all_files[0] if all_files else \"NO FILES\")\n",
    "\n",
    "random.shuffle(all_files)\n",
    "split = int(0.85 * len(all_files))\n",
    "train_files = all_files[:split]\n",
    "val_files   = all_files[split:]\n",
    "print(\"Train:\", len(train_files), \"Val:\", len(val_files))\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 2) Dataset that supports X/y OR points/labels\n",
    "# ============================\n",
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = np.load(self.files[idx])\n",
    "\n",
    "        # points\n",
    "        if \"points\" in d.files:\n",
    "            points = d[\"points\"].astype(np.float32)\n",
    "        elif \"X\" in d.files:\n",
    "            points = d[\"X\"].astype(np.float32)\n",
    "        else:\n",
    "            raise KeyError(f\"No points array found. Keys={d.files}\")\n",
    "\n",
    "        # labels\n",
    "        if \"labels\" in d.files:\n",
    "            labels = d[\"labels\"].astype(np.int64)\n",
    "        elif \"y\" in d.files:\n",
    "            labels = d[\"y\"].astype(np.int64)\n",
    "        else:\n",
    "            raise KeyError(f\"No labels array found. Keys={d.files}\")\n",
    "\n",
    "        return torch.from_numpy(points), torch.from_numpy(labels)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 3) Build datasets + loaders\n",
    "# ============================\n",
    "train_dataset = PointCloudDataset(train_files)\n",
    "val_dataset   = PointCloudDataset(val_files)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,      # Windows-safe\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 4) Quick sanity test\n",
    "# ============================\n",
    "# show keys inside first file\n",
    "d0 = np.load(all_files[0])\n",
    "print(\"Keys in your npz:\", d0.files)\n",
    "\n",
    "pts, lbl = next(iter(train_loader))\n",
    "print(\"Batch points shape:\", pts.shape)\n",
    "print(\"Batch labels shape:\", lbl.shape)\n",
    "print(\"Unique labels (sample):\", torch.unique(lbl)[:25])\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea9dd6d-bb9d-45eb-9254-ae549042e79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | GPU: NVIDIA GeForce RTX 3050\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m         s\u001b[38;5;241m.\u001b[39mupdate(u\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(s))\n\u001b[1;32m---> 41\u001b[0m classes \u001b[38;5;241m=\u001b[39m get_all_classes_from_loader(\u001b[43mtrain_loader\u001b[49m, max_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClasses found (sample):\u001b[39m\u001b[38;5;124m\"\u001b[39m, classes)\n\u001b[0;32m     44\u001b[0m class_to_idx \u001b[38;5;241m=\u001b[39m {c:i \u001b[38;5;28;01mfor\u001b[39;00m i,c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classes)}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FULL DL TRAINING (PointNet-Small) for your blocks (X,y)\n",
    "# X: (B,4096,10)  y: (B,4096)\n",
    "# Classes in your blocks: 1,2,3,6,7,12 (+ maybe 13 etc later)\n",
    "# Saves best model: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n",
    "# ============================================================\n",
    "\n",
    "import os, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm  \n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# PATHS\n",
    "# -----------------------------\n",
    "OUT_DIR = r\"D:/lidarrrrr/anbu/dl_models\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "MODEL_PATH = os.path.join(OUT_DIR, \"pointnet_best.pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device, \"| GPU:\", torch.cuda.get_device_name(0) if device.type==\"cuda\" else \"cpu\")\n",
    "\n",
    "# -----------------------------\n",
    "# Build class mapping dynamically from training labels\n",
    "# -----------------------------\n",
    "def get_all_classes_from_loader(loader, max_batches=50):\n",
    "    s = set()\n",
    "    it = iter(loader)\n",
    "    for _ in range(max_batches):\n",
    "        try:\n",
    "            _, y = next(it)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        u = torch.unique(y)\n",
    "        s.update(u.cpu().numpy().tolist())\n",
    "    return sorted(list(s))\n",
    "\n",
    "classes = get_all_classes_from_loader(train_loader, max_batches=80)\n",
    "print(\"Classes found (sample):\", classes)\n",
    "\n",
    "class_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "idx_to_class = {i:c for c,i in class_to_idx.items()}\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "def remap_labels(y):\n",
    "    # y is torch tensor (B,N) containing original class ids\n",
    "    # map to 0..C-1\n",
    "    out = torch.empty_like(y, dtype=torch.long)\n",
    "    for c, i in class_to_idx.items():\n",
    "        out[y == c] = i\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# Compute class weights (fast sample pass)\n",
    "# -----------------------------\n",
    "def compute_class_weights(loader, max_batches=120):\n",
    "    counts = {c:0 for c in classes}\n",
    "    it = iter(loader)\n",
    "    for _ in tqdm(range(max_batches), desc=\"Computing weights (sample)\"):\n",
    "        try:\n",
    "            _, y = next(it)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        y_np = y.cpu().numpy()\n",
    "        u, cts = np.unique(y_np, return_counts=True)\n",
    "        for uu, cc in zip(u.tolist(), cts.tolist()):\n",
    "            if uu in counts:\n",
    "                counts[uu] += cc\n",
    "\n",
    "    freqs = np.array([counts[c] for c in classes], dtype=np.float64)\n",
    "    inv = 1.0 / np.maximum(freqs, 1.0)\n",
    "    inv = inv / inv.mean()   # normalize mean=1\n",
    "    w = torch.tensor(inv, dtype=torch.float32)\n",
    "    print(\"Counts(sample):\", counts)\n",
    "    print(\"Weights:\", {c: float(w[class_to_idx[c]]) for c in classes})\n",
    "    return w\n",
    "\n",
    "class_weights = compute_class_weights(train_loader, max_batches=120).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Model: PointNet-small (fast)\n",
    "# Input: (B,N,10)\n",
    "# Output: (B,N,C)\n",
    "# -----------------------------\n",
    "class PointNetSmall(nn.Module):\n",
    "    def __init__(self, in_ch, num_classes):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(in_ch, 64), nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 256), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(256, 256), nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.mlp1(x)   # (B,N,256)\n",
    "        out  = self.mlp2(feat) # (B,N,C)\n",
    "        return out\n",
    "\n",
    "model = PointNetSmall(in_ch=10, num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type==\"cuda\"))\n",
    "\n",
    "# -----------------------------\n",
    "# Train / Val\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_pts = 0\n",
    "    correct = 0\n",
    "\n",
    "    for X, y in val_loader:\n",
    "        X = X.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        y2 = remap_labels(y)\n",
    "\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits.reshape(-1, NUM_CLASSES), y2.reshape(-1))\n",
    "\n",
    "        total_loss += float(loss.item()) * y.numel()\n",
    "        total_pts  += y.numel()\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct += int((pred == y2).sum().item())\n",
    "\n",
    "    return total_loss / max(total_pts, 1), correct / max(total_pts, 1)\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch, epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_pts = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} [train]\")\n",
    "    for X, y in pbar:\n",
    "        X = X.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        y2 = remap_labels(y)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=(device.type==\"cuda\")):\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits.reshape(-1, NUM_CLASSES), y2.reshape(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += float(loss.item()) * y.numel()\n",
    "        total_pts  += y.numel()\n",
    "\n",
    "        pbar.set_postfix(loss=float(loss.item()))\n",
    "\n",
    "    return total_loss / max(total_pts, 1)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run training\n",
    "# -----------------------------\n",
    "EPOCHS = 10   # start with 10; later you can set 25\n",
    "\n",
    "best_val = 1e9\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr_loss = train_one_epoch(ep, EPOCHS)\n",
    "    va_loss, va_acc = evaluate()\n",
    "\n",
    "    print(f\"Epoch {ep}/{EPOCHS} | train_loss={tr_loss:.6f} | val_loss={va_loss:.6f} | val_acc={va_acc:.4f}\")\n",
    "\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        torch.save({\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"classes\": classes,\n",
    "            \"class_to_idx\": class_to_idx,\n",
    "            \"idx_to_class\": idx_to_class\n",
    "        }, MODEL_PATH)\n",
    "        print(\"âœ… Saved best:\", MODEL_PATH)\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        print(\"GPU allocated MB:\", torch.cuda.memory_allocated()/1024/1024)\n",
    "\n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58595004-2d1a-4e82-bb98-1f29a36d9b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in d:\\anaconda\\envs\\lidar\\lib\\site-packages (8.1.8)\n",
      "Requirement already satisfied: comm>=0.1.3 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from ipywidgets) (8.37.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.3.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: wcwidth in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.6.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure_eval in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd0ad3fb-c536-4b53-87e5-71c488abb3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total blocks: 3543\n",
      "Train: 3011\n",
      "Val: 532\n"
     ]
    }
   ],
   "source": [
    "import os, glob, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "DATA_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "\n",
    "all_files = sorted(glob.glob(os.path.join(DATA_DIR, \"*.npz\")))\n",
    "print(\"Total blocks:\", len(all_files))\n",
    "\n",
    "random.shuffle(all_files)\n",
    "split = int(0.85 * len(all_files))\n",
    "train_files = all_files[:split]\n",
    "val_files   = all_files[split:]\n",
    "\n",
    "print(\"Train:\", len(train_files))\n",
    "print(\"Val:\", len(val_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4aceb4a-f0bc-4d10-bd4b-37afb4af05ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = np.load(self.files[idx])\n",
    "\n",
    "        if \"X\" in d.files:\n",
    "            points = d[\"X\"].astype(np.float32)\n",
    "        else:\n",
    "            points = d[\"points\"].astype(np.float32)\n",
    "\n",
    "        if \"y\" in d.files:\n",
    "            labels = d[\"y\"].astype(np.int64)\n",
    "        else:\n",
    "            labels = d[\"labels\"].astype(np.int64)\n",
    "\n",
    "        return torch.from_numpy(points), torch.from_numpy(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41ba4f0-0246-4155-9fd0-23e9166f42e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch points: torch.Size([8, 4096, 10])\n",
      "Batch labels: torch.Size([8, 4096])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PointCloudDataset(train_files)\n",
    "val_dataset   = PointCloudDataset(val_files)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "pts, lbl = next(iter(train_loader))\n",
    "print(\"Batch points:\", pts.shape)\n",
    "print(\"Batch labels:\", lbl.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7649115f-046a-43b2-b63b-de85cfb9331f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b9d13bc-1510-4809-97e8-fa631ab58356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | GPU: NVIDIA GeForce RTX 3050\n",
      "Classes found (sample): [1, 2, 3, 6, 7, 12, 13]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954231e72e25436987c1fc95521c38c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing weights (sample):   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts(sample): {1: 1856818, 2: 1650179, 3: 263761, 6: 105286, 7: 380, 12: 54904, 13: 832}\n",
      "Weights: {1: 0.0009751020697876811, 2: 0.0010972064919769764, 3: 0.00686449883505702, 6: 0.01719684526324272, 7: 4.764702796936035, 12: 0.03297732397913933, 13: 2.1761863231658936}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699c8cf389cd4b88aad127329871587c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 [train]:   0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | train_loss=nan | val_loss=nan | val_acc=0.4851\n",
      "GPU allocated MB: 19.41552734375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77025bffddc64c4094b8e57ca82c20fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 [train]:   0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | train_loss=nan | val_loss=nan | val_acc=0.4851\n",
      "GPU allocated MB: 19.41552734375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8b74cafa7f462b91b2e7597e9c0c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 [train]:   0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | train_loss=nan | val_loss=nan | val_acc=0.4851\n",
      "GPU allocated MB: 19.41552734375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbe514ec0dc4cd6beb01fd018e44a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 [train]:   0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | train_loss=nan | val_loss=nan | val_acc=0.4851\n",
      "GPU allocated MB: 19.41552734375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cccacf536244ea869d8e030feab513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 [train]:   0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | train_loss=nan | val_loss=nan | val_acc=0.4851\n",
      "GPU allocated MB: 19.41552734375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b92f94f64e649e9bec21b98ebb7b367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 [train]:   0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | train_loss=nan | val_loss=nan | val_acc=0.4851\n",
      "GPU allocated MB: 19.41552734375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11916600b6b84f8ca2836e599e29fdb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10 [train]:   0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | train_loss=nan | val_loss=nan | val_acc=0.4851\n",
      "GPU allocated MB: 19.41552734375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4ea6e4d77d45c797c1d83bb1735bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10 [train]:   0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | train_loss=nan | val_loss=nan | val_acc=0.4851\n",
      "GPU allocated MB: 19.41552734375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed8bd30baed4b62ac4b0796ea2cf5e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10 [train]:   0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | train_loss=nan | val_loss=nan | val_acc=0.4851\n",
      "GPU allocated MB: 19.41552734375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ba10e7ce5b4869a6eb63bd4f9861dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10 [train]:   0%|          | 0/377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | train_loss=nan | val_loss=nan | val_acc=0.4851\n",
      "GPU allocated MB: 19.41552734375\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FULL DL TRAINING (PointNet-Small) for your blocks (X,y)\n",
    "# X: (B,4096,10)  y: (B,4096)\n",
    "# Classes in your blocks: 1,2,3,6,7,12 (+ maybe 13 etc later)\n",
    "# Saves best model: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n",
    "# ============================================================\n",
    "\n",
    "import os, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# PATHS\n",
    "# -----------------------------\n",
    "OUT_DIR = r\"D:/lidarrrrr/anbu/dl_models\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "MODEL_PATH = os.path.join(OUT_DIR, \"pointnet_best.pt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device, \"| GPU:\", torch.cuda.get_device_name(0) if device.type==\"cuda\" else \"cpu\")\n",
    "\n",
    "# -----------------------------\n",
    "# Build class mapping dynamically from training labels\n",
    "# -----------------------------\n",
    "def get_all_classes_from_loader(loader, max_batches=50):\n",
    "    s = set()\n",
    "    it = iter(loader)\n",
    "    for _ in range(max_batches):\n",
    "        try:\n",
    "            _, y = next(it)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        u = torch.unique(y)\n",
    "        s.update(u.cpu().numpy().tolist())\n",
    "    return sorted(list(s))\n",
    "\n",
    "classes = get_all_classes_from_loader(train_loader, max_batches=80)\n",
    "print(\"Classes found (sample):\", classes)\n",
    "\n",
    "class_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "idx_to_class = {i:c for c,i in class_to_idx.items()}\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "def remap_labels(y):\n",
    "    # y is torch tensor (B,N) containing original class ids\n",
    "    # map to 0..C-1\n",
    "    out = torch.empty_like(y, dtype=torch.long)\n",
    "    for c, i in class_to_idx.items():\n",
    "        out[y == c] = i\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# Compute class weights (fast sample pass)\n",
    "# -----------------------------\n",
    "def compute_class_weights(loader, max_batches=120):\n",
    "    counts = {c:0 for c in classes}\n",
    "    it = iter(loader)\n",
    "    for _ in tqdm(range(max_batches), desc=\"Computing weights (sample)\"):\n",
    "        try:\n",
    "            _, y = next(it)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        y_np = y.cpu().numpy()\n",
    "        u, cts = np.unique(y_np, return_counts=True)\n",
    "        for uu, cc in zip(u.tolist(), cts.tolist()):\n",
    "            if uu in counts:\n",
    "                counts[uu] += cc\n",
    "\n",
    "    freqs = np.array([counts[c] for c in classes], dtype=np.float64)\n",
    "    inv = 1.0 / np.maximum(freqs, 1.0)\n",
    "    inv = inv / inv.mean()   # normalize mean=1\n",
    "    w = torch.tensor(inv, dtype=torch.float32)\n",
    "    print(\"Counts(sample):\", counts)\n",
    "    print(\"Weights:\", {c: float(w[class_to_idx[c]]) for c in classes})\n",
    "    return w\n",
    "\n",
    "class_weights = compute_class_weights(train_loader, max_batches=120).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Model: PointNet-small (fast)\n",
    "# Input: (B,N,10)\n",
    "# Output: (B,N,C)\n",
    "# -----------------------------\n",
    "class PointNetSmall(nn.Module):\n",
    "    def __init__(self, in_ch, num_classes):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(in_ch, 64), nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 256), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(256, 256), nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.mlp1(x)   # (B,N,256)\n",
    "        out  = self.mlp2(feat) # (B,N,C)\n",
    "        return out\n",
    "\n",
    "model = PointNetSmall(in_ch=10, num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type==\"cuda\"))\n",
    "\n",
    "# -----------------------------\n",
    "# Train / Val\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_pts = 0\n",
    "    correct = 0\n",
    "\n",
    "    for X, y in val_loader:\n",
    "        X = X.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        y2 = remap_labels(y)\n",
    "\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits.reshape(-1, NUM_CLASSES), y2.reshape(-1))\n",
    "\n",
    "        total_loss += float(loss.item()) * y.numel()\n",
    "        total_pts  += y.numel()\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct += int((pred == y2).sum().item())\n",
    "\n",
    "    return total_loss / max(total_pts, 1), correct / max(total_pts, 1)\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch, epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_pts = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} [train]\")\n",
    "    for X, y in pbar:\n",
    "        X = X.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        y2 = remap_labels(y)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=(device.type==\"cuda\")):\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits.reshape(-1, NUM_CLASSES), y2.reshape(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += float(loss.item()) * y.numel()\n",
    "        total_pts  += y.numel()\n",
    "\n",
    "        pbar.set_postfix(loss=float(loss.item()))\n",
    "\n",
    "    return total_loss / max(total_pts, 1)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run training\n",
    "# -----------------------------\n",
    "EPOCHS = 10   # start with 10; later you can set 25\n",
    "\n",
    "best_val = 1e9\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr_loss = train_one_epoch(ep, EPOCHS)\n",
    "    va_loss, va_acc = evaluate()\n",
    "\n",
    "    print(f\"Epoch {ep}/{EPOCHS} | train_loss={tr_loss:.6f} | val_loss={va_loss:.6f} | val_acc={va_acc:.4f}\")\n",
    "\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        torch.save({\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"classes\": classes,\n",
    "            \"class_to_idx\": class_to_idx,\n",
    "            \"idx_to_class\": idx_to_class\n",
    "        }, MODEL_PATH)\n",
    "        print(\"âœ… Saved best:\", MODEL_PATH)\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        print(\"GPU allocated MB:\", torch.cuda.memory_allocated()/1024/1024)\n",
    "\n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bea54c8a-9c8e-434b-80c2-4e0cfe73b9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in X: False\n",
      "Inf in X: False\n",
      "NaN in y: False\n",
      "X min/max: -11.875 41942.0\n",
      "Labels: [1 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "d = np.load(train_files[0])\n",
    "X = d[\"X\"]\n",
    "y = d[\"y\"]\n",
    "\n",
    "print(\"NaN in X:\", np.isnan(X).any())\n",
    "print(\"Inf in X:\", np.isinf(X).any())\n",
    "print(\"NaN in y:\", np.isnan(y).any())\n",
    "\n",
    "print(\"X min/max:\", X.min(), X.max())\n",
    "print(\"Labels:\", np.unique(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faa25b94-daa7-48fc-ad4d-1da75c5cb052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | GPU: NVIDIA GeForce RTX 3050\n",
      "Classes found (sample): [1, 2, 3, 6, 7, 12, 13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:01<00:00, 85.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: {1: 1756388, 2: 1729828, 3: 265786, 6: 120339, 7: 263, 12: 59146, 13: 410}\n",
      "Weights: {1: 0.20000000298023224, 2: 0.20000000298023224, 3: 0.20000000298023224, 6: 0.20000000298023224, 7: 4.244001388549805, 12: 0.20000000298023224, 13: 2.7223715782165527}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 377/377 [00:12<00:00, 29.05it/s, loss=0.945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | train_loss=6.151658 | val_loss=1.605170 | val_acc=0.3688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 377/377 [00:11<00:00, 32.75it/s, loss=1.02] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | train_loss=1.200394 | val_loss=1.159095 | val_acc=0.4948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 377/377 [00:11<00:00, 32.75it/s, loss=0.736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | train_loss=0.982194 | val_loss=0.866615 | val_acc=0.5996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 377/377 [00:11<00:00, 32.12it/s, loss=0.838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | train_loss=0.939962 | val_loss=0.882633 | val_acc=0.6017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 377/377 [00:11<00:00, 32.78it/s, loss=0.801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | train_loss=0.926299 | val_loss=0.864949 | val_acc=0.6008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 377/377 [00:11<00:00, 32.84it/s, loss=0.989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | train_loss=0.920890 | val_loss=0.860165 | val_acc=0.6025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 377/377 [00:11<00:00, 32.75it/s, loss=0.71] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | train_loss=0.914932 | val_loss=0.870057 | val_acc=0.6016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 377/377 [00:11<00:00, 32.66it/s, loss=0.726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | train_loss=0.915570 | val_loss=0.855486 | val_acc=0.6021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 377/377 [00:11<00:00, 32.55it/s, loss=0.771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | train_loss=0.910260 | val_loss=0.870256 | val_acc=0.6028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 377/377 [00:11<00:00, 32.59it/s, loss=0.74] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | train_loss=0.971809 | val_loss=0.871018 | val_acc=0.5940\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device, \"| GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# ---- classes from sample loader (your existing function is ok) ----\n",
    "def get_all_classes_from_loader(loader, max_batches=80):\n",
    "    s = set()\n",
    "    it = iter(loader)\n",
    "    for _ in range(max_batches):\n",
    "        try:\n",
    "            _, y = next(it)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        s.update(torch.unique(y).cpu().numpy().tolist())\n",
    "    return sorted(list(s))\n",
    "\n",
    "classes = get_all_classes_from_loader(train_loader, max_batches=80)\n",
    "print(\"Classes found (sample):\", classes)\n",
    "\n",
    "# ---- LUT mapping 0..255 -> new index, default -1 ----\n",
    "lut = np.full(256, -1, dtype=np.int64)\n",
    "for i, c in enumerate(classes):\n",
    "    if 0 <= c < 256:\n",
    "        lut[c] = i\n",
    "\n",
    "lut_t = torch.from_numpy(lut).to(device)\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "def remap_labels_lut(y):\n",
    "    # y: (B,N) uint/int -> mapped to 0..C-1, unknown becomes -1\n",
    "    y = y.to(device)\n",
    "    return lut_t[y.long()]\n",
    "\n",
    "# ---- Stable class weights (cap to avoid extreme weights) ----\n",
    "def compute_class_weights(loader, classes, max_batches=120):\n",
    "    counts = {c:0 for c in classes}\n",
    "    it = iter(loader)\n",
    "    for _ in tqdm(range(max_batches), desc=\"Computing weights\"):\n",
    "        try:\n",
    "            _, y = next(it)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        u, cts = np.unique(y.numpy(), return_counts=True)\n",
    "        for uu, cc in zip(u.tolist(), cts.tolist()):\n",
    "            if uu in counts:\n",
    "                counts[uu] += cc\n",
    "\n",
    "    freqs = np.array([counts[c] for c in classes], dtype=np.float64)\n",
    "    inv = 1.0 / np.maximum(freqs, 1.0)\n",
    "\n",
    "    inv = inv / inv.mean()          # mean=1\n",
    "    inv = np.clip(inv, 0.2, 5.0)    # IMPORTANT: cap weights\n",
    "\n",
    "    w = torch.tensor(inv, dtype=torch.float32)\n",
    "    print(\"Counts:\", counts)\n",
    "    print(\"Weights:\", {c: float(w[i]) for i, c in enumerate(classes)})\n",
    "    return w\n",
    "\n",
    "class_weights = compute_class_weights(train_loader, classes, max_batches=120).to(device)\n",
    "\n",
    "# ---- Model (same as before) ----\n",
    "class PointNetSmall(nn.Module):\n",
    "    def __init__(self, in_ch, num_classes):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(in_ch, 64), nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 256), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(256, 256), nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp1(x)\n",
    "        x = self.mlp2(x)\n",
    "        return x  # (B,N,C)\n",
    "\n",
    "model = PointNetSmall(in_ch=10, num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "# ---- IMPORTANT: lower LR ----\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
    "\n",
    "# ---- IMPORTANT: ignore_index for any unmapped label (-1) ----\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-1)\n",
    "\n",
    "# ---- Train / Val (float32, no autocast) ----\n",
    "def train_one_epoch(epoch, epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_pts = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} [train]\")\n",
    "    for X, y in pbar:\n",
    "        X = X.to(device, non_blocking=True).float()  # float32\n",
    "        y2 = remap_labels_lut(y)                     # (B,N) in 0..C-1 or -1\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits.reshape(-1, NUM_CLASSES), y2.reshape(-1))\n",
    "\n",
    "        # NaN guard (debug)\n",
    "        if torch.isnan(loss):\n",
    "            print(\"âŒ NaN loss detected. Stopping.\")\n",
    "            return float(\"nan\")\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # extra stability\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.item()) * y.numel()\n",
    "        total_pts  += y.numel()\n",
    "        pbar.set_postfix(loss=float(loss.item()))\n",
    "\n",
    "    return total_loss / max(total_pts, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_pts = 0\n",
    "    correct = 0\n",
    "    counted = 0\n",
    "\n",
    "    for X, y in val_loader:\n",
    "        X = X.to(device, non_blocking=True).float()\n",
    "        y2 = remap_labels_lut(y)\n",
    "\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits.reshape(-1, NUM_CLASSES), y2.reshape(-1))\n",
    "\n",
    "        total_loss += float(loss.item()) * y.numel()\n",
    "        total_pts  += y.numel()\n",
    "\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        mask = (y2 != -1)\n",
    "        correct += int((pred[mask] == y2[mask]).sum().item())\n",
    "        counted += int(mask.sum().item())\n",
    "\n",
    "    acc = correct / max(counted, 1)\n",
    "    return total_loss / max(total_pts, 1), acc\n",
    "\n",
    "EPOCHS = 10\n",
    "best = 1e9\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = train_one_epoch(ep, EPOCHS)\n",
    "    va, acc = evaluate()\n",
    "    print(f\"Epoch {ep}/{EPOCHS} | train_loss={tr:.6f} | val_loss={va:.6f} | val_acc={acc:.4f}\")\n",
    "\n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fc7262e-7d89-4f29-94cb-6728ca57adad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "MODEL_PATH = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"classes\": classes,\n",
    "    \"idx_to_class\": idx_to_class\n",
    "}, MODEL_PATH)\n",
    "\n",
    "print(\"Saved model to:\", MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f272d8c6-bc56-4db2-aa15-5fa02fc3aec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loaded model. Classes: [1, 2, 3, 6, 7, 12, 13]\n",
      "Total points: 12374846 | features: (12374846, 10)\n",
      "Saved LAS: D:/lidarrrrr/anbu/New folder/dl_predicted.las\n",
      "Saved LAZ: D:/lidarrrrr/anbu/New folder/dl_predicted.laz\n",
      "Pred class counts: {1: 11350, 6: 12363496}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import laspy\n",
    "\n",
    "# ----------------------------\n",
    "# EDIT PATHS\n",
    "# ----------------------------\n",
    "MODEL_PATH = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"   # the saved checkpoint\n",
    "IN_LAZ     = r\"D:/lidarrrrr/anbu/DX3035724 S.GIUSTO000001.laz\" # file to classify\n",
    "OUT_LAS    = r\"D:/lidarrrrr/anbu/New folder/dl_predicted.las\"\n",
    "OUT_LAZ    = r\"D:/lidarrrrr/anbu/New folder/dl_predicted.laz\"\n",
    "\n",
    "BLOCK_N = 4096\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ----------------------------\n",
    "# Model definition (MUST match training)\n",
    "# ----------------------------\n",
    "class PointNetSmall(nn.Module):\n",
    "    def __init__(self, in_ch, num_classes):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(in_ch, 64), nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 256), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(256, 256), nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128), nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp1(x)\n",
    "        x = self.mlp2(x)\n",
    "        return x  # (B,N,C)\n",
    "\n",
    "# ----------------------------\n",
    "# Load checkpoint\n",
    "# ----------------------------\n",
    "ckpt = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "classes = ckpt[\"classes\"]            # original class ids\n",
    "idx_to_class = ckpt[\"idx_to_class\"]  # idx -> original class\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "model = PointNetSmall(in_ch=10, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded model. Classes:\", classes)\n",
    "\n",
    "# ----------------------------\n",
    "# Feature builder (10 features) - IMPORTANT\n",
    "# This must match your block creator.\n",
    "# If your blocks were [x,y,z,intensity,return_number,num_returns,R,G,B,???],\n",
    "# update this accordingly.\n",
    "# ----------------------------\n",
    "def build_features(las):\n",
    "    xyz = np.vstack([las.x, las.y, las.z]).T.astype(np.float32)\n",
    "\n",
    "    # required dims\n",
    "    intensity = np.asarray(las.intensity, dtype=np.float32)\n",
    "    rn = np.asarray(las.return_number, dtype=np.float32)\n",
    "    nr = np.asarray(las.number_of_returns, dtype=np.float32)\n",
    "\n",
    "    # optional RGB (if missing, zeros)\n",
    "    if \"red\" in las.point_format.dimension_names:\n",
    "        r = np.asarray(las.red, dtype=np.float32)\n",
    "        g = np.asarray(las.green, dtype=np.float32)\n",
    "        b = np.asarray(las.blue, dtype=np.float32)\n",
    "    else:\n",
    "        r = np.zeros(len(xyz), dtype=np.float32)\n",
    "        g = np.zeros(len(xyz), dtype=np.float32)\n",
    "        b = np.zeros(len(xyz), dtype=np.float32)\n",
    "\n",
    "    # NOTE: your blocks have 10 features per point.\n",
    "    # Here we construct 10:\n",
    "    # [x, y, z, intensity, rn, nr, r, g, b, z_norm]\n",
    "    z_norm = (xyz[:,2] - xyz[:,2].mean()) / (xyz[:,2].std() + 1e-6)\n",
    "\n",
    "    X = np.stack([\n",
    "        xyz[:,0], xyz[:,1], xyz[:,2],\n",
    "        intensity, rn, nr,\n",
    "        r, g, b,\n",
    "        z_norm\n",
    "    ], axis=1).astype(np.float32)\n",
    "\n",
    "    # cleanup\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return X\n",
    "\n",
    "# ----------------------------\n",
    "# Predict in blocks\n",
    "# ----------------------------\n",
    "las = laspy.read(IN_LAZ)\n",
    "X_all = build_features(las)\n",
    "N = len(X_all)\n",
    "print(\"Total points:\", N, \"| features:\", X_all.shape)\n",
    "\n",
    "# pad to full blocks\n",
    "pad = (-N) % BLOCK_N\n",
    "if pad > 0:\n",
    "    X_pad = np.zeros((pad, X_all.shape[1]), dtype=np.float32)\n",
    "    X_all2 = np.vstack([X_all, X_pad])\n",
    "else:\n",
    "    X_all2 = X_all\n",
    "\n",
    "num_blocks = len(X_all2) // BLOCK_N\n",
    "pred_idx_all = np.zeros((len(X_all2),), dtype=np.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bi in range(num_blocks):\n",
    "        a = bi * BLOCK_N\n",
    "        b = a + BLOCK_N\n",
    "\n",
    "        xb = torch.from_numpy(X_all2[a:b]).unsqueeze(0).to(DEVICE)  # (1,4096,10)\n",
    "        logits = model(xb)                                          # (1,4096,C)\n",
    "        pred = logits.argmax(dim=-1).squeeze(0).cpu().numpy()        # (4096,)\n",
    "        pred_idx_all[a:b] = pred\n",
    "\n",
    "# remove padding\n",
    "pred_idx = pred_idx_all[:N]\n",
    "\n",
    "# map to original class ids\n",
    "pred_class = np.array([idx_to_class[int(i)] for i in pred_idx], dtype=np.uint8)\n",
    "\n",
    "# write output\n",
    "las.classification = pred_class\n",
    "os.makedirs(os.path.dirname(OUT_LAS), exist_ok=True)\n",
    "las.write(OUT_LAS)\n",
    "print(\"Saved LAS:\", OUT_LAS)\n",
    "\n",
    "try:\n",
    "    las.write(OUT_LAZ)\n",
    "    print(\"Saved LAZ:\", OUT_LAZ)\n",
    "except Exception as e:\n",
    "    print(\"LAZ write failed (ok):\", e)\n",
    "\n",
    "u, c = np.unique(pred_class, return_counts=True)\n",
    "print(\"Pred class counts:\", dict(zip(u.tolist(), c.tolist())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "798cf343-7a4a-4da9-b40a-d9f8eebb6b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.3125000e+00 7.0000000e+00 1.2836933e-02 1.9999981e-02 3.4340000e+04\n",
      " 1.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "d = np.load(train_files[0])\n",
    "print(d[\"X\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b909ff2-d454-427c-b71b-133e341fa01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (4096, 10)\n",
      "Feature mins: [-1.1875000e+01 -1.1500000e+01 -2.3716307e-01 -9.0000033e-02\n",
      "  2.7764000e+04  1.0000000e+00  1.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n",
      "Feature maxs: [6.5000000e+00 1.8000000e+01 4.9283695e-01 5.0000000e-01 4.1942000e+04\n",
      " 1.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      "Feature means: [ 8.0413818e-03 -2.9382324e-01  3.6525307e-08  4.3935556e-02\n",
      "  3.6071180e+04  1.0000000e+00  1.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "d = np.load(train_files[0])\n",
    "X = d[\"X\"]\n",
    "\n",
    "print(\"Shape:\", X.shape)\n",
    "\n",
    "print(\"Feature mins:\", X.min(axis=0))\n",
    "print(\"Feature maxs:\", X.max(axis=0))\n",
    "print(\"Feature means:\", X.mean(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78b6c5e4-333a-40f5-b25e-7baa5b47f295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sample feature vector: [6.3125000e+00 7.0000000e+00 1.2836933e-02 1.9999981e-02 3.4340000e+04\n",
      " 1.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      "Prediction sample feature vector: [6.3125000e+00 7.0000000e+00 1.2836933e-02 1.9999981e-02 3.4340000e+04\n",
      " 1.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "d = np.load(train_files[0])\n",
    "print(\"Training sample feature vector:\", d[\"X\"][0])\n",
    "print(\"Prediction sample feature vector:\", X[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9f5ff86-d1ca-46c7-bb9c-40c209516416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\anaconda\\envs\\lidar\\lib\\site-packages (2.10.0+cu126)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\envs\\lidar\\lib\\site-packages (4.67.3)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\envs\\lidar\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch tqdm numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ddf447-a7ae-41b6-984d-53a190a6d8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050\n",
      "Total blocks: 3543\n",
      "Train: 3011 Val: 532\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "PointCloudDataset.__init__() missing 1 required positional argument: 'class_to_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 70\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m points, labels\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# DATA LOADERS\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPointCloudDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m val_dataset   \u001b[38;5;241m=\u001b[39m PointCloudDataset(val_files)\n\u001b[0;32m     73\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: PointCloudDataset.__init__() missing 1 required positional argument: 'class_to_idx'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------------------------\n",
    "# SETTINGS\n",
    "# -------------------------\n",
    "DATASET_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "MODEL_PATH  = r\"D:/lidarrrrr/anbu/dl_models/pointnet_best.pt\"\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 15\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# -------------------------\n",
    "# LOAD FILE LIST\n",
    "# -------------------------\n",
    "files = sorted(glob.glob(DATASET_DIR + \"/*.npz\"))\n",
    "print(\"Total blocks:\", len(files))\n",
    "\n",
    "split = int(len(files) * 0.85)\n",
    "train_files = files[:split]\n",
    "val_files   = files[split:]\n",
    "\n",
    "print(\"Train:\", len(train_files), \"Val:\", len(val_files))\n",
    "\n",
    "# -------------------------\n",
    "# DATASET CLASS\n",
    "# -------------------------\n",
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, files, class_to_idx):\n",
    "        self.files = files\n",
    "        self.class_to_idx = class_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = np.load(self.files[idx])\n",
    "\n",
    "        points = data[\"X\"].astype(np.float32)\n",
    "        labels = data[\"y\"].astype(np.int64)\n",
    "\n",
    "        # normalize features\n",
    "        mean = points.mean(axis=0, keepdims=True)\n",
    "        std  = points.std(axis=0, keepdims=True) + 1e-6\n",
    "        points = (points - mean) / std\n",
    "\n",
    "        # remap labels\n",
    "        labels = np.vectorize(self.class_to_idx.get)(labels)\n",
    "\n",
    "        points = torch.from_numpy(points)\n",
    "        labels = torch.from_numpy(labels)\n",
    "\n",
    "        return points, labels\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# DATA LOADERS\n",
    "# -------------------------\n",
    "train_dataset = PointCloudDataset(train_files)\n",
    "val_dataset   = PointCloudDataset(val_files)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# -------------------------\n",
    "# FIND CLASSES\n",
    "# -------------------------\n",
    "def get_classes(loader, max_batches=50):\n",
    "    s = set()\n",
    "    it = iter(loader)\n",
    "    for _ in range(max_batches):\n",
    "        try:\n",
    "            _, y = next(it)\n",
    "            s.update(torch.unique(y).cpu().numpy().tolist())\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return sorted(list(s))\n",
    "\n",
    "classes = get_classes(train_loader)\n",
    "print(\"Classes:\", classes)\n",
    "\n",
    "class_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "idx_to_class = {i:c for c,i in class_to_idx.items()}\n",
    "NUM_CLASSES = len(classes)\n",
    "\n",
    "# -------------------------\n",
    "# COMPUTE CLASS WEIGHTS\n",
    "# -------------------------\n",
    "def compute_class_weights(loader):\n",
    "    counts = {c:0 for c in classes}\n",
    "\n",
    "    for pts, lbl in tqdm(loader, desc=\"Counting labels\"):\n",
    "        lbl = lbl.numpy()\n",
    "        for c in classes:\n",
    "            counts[c] += np.sum(lbl == c)\n",
    "\n",
    "    total = sum(counts.values())\n",
    "    weights = []\n",
    "\n",
    "    for c in classes:\n",
    "        w = total / (counts[c] + 1)\n",
    "        weights.append(w)\n",
    "\n",
    "    weights = np.array(weights)\n",
    "    weights = weights / weights.mean()\n",
    "\n",
    "    print(\"Counts:\", counts)\n",
    "    print(\"Weights:\", weights)\n",
    "\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "class_weights = compute_class_weights(train_loader).to(DEVICE)\n",
    "\n",
    "# -------------------------\n",
    "# MODEL (PointNet small)\n",
    "# -------------------------\n",
    "class PointNetSmall(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp1 = nn.Conv1d(10, 64, 1)\n",
    "        self.mlp2 = nn.Conv1d(64, 128, 1)\n",
    "        self.mlp3 = nn.Conv1d(128, 256, 1)\n",
    "\n",
    "        self.fc1 = nn.Conv1d(256, 128, 1)\n",
    "        self.fc2 = nn.Conv1d(128, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: B,N,10 â†’ B,10,N\n",
    "        x = x.transpose(1,2)\n",
    "\n",
    "        x = F.relu(self.mlp1(x))\n",
    "        x = F.relu(self.mlp2(x))\n",
    "        x = F.relu(self.mlp3(x))\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # B,C,N â†’ B,N,C\n",
    "        x = x.transpose(1,2)\n",
    "        return x\n",
    "\n",
    "model = PointNetSmall(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# -------------------------\n",
    "# TRAIN LOOP\n",
    "# -------------------------\n",
    "best_loss = 999\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    # ---- TRAIN ----\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [train]\")\n",
    "\n",
    "    for points, labels in pbar:\n",
    "        points = points.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(points)\n",
    "        loss = criterion(outputs.reshape(-1, NUM_CLASSES), labels.reshape(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix(loss=float(loss.item()))\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # ---- VALIDATE ----\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for points, labels in val_loader:\n",
    "            points = points.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            outputs = model(points)\n",
    "            loss = criterion(outputs.reshape(-1, NUM_CLASSES), labels.reshape(-1))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = outputs.argmax(dim=2)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.numel()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n",
    "\n",
    "    # ---- SAVE BEST MODEL ----\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "        torch.save({\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"classes\": classes\n",
    "        }, MODEL_PATH)\n",
    "        print(\"Saved best model\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f0b2f9a-ea65-4197-93a2-01bfcd71aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a4549-680a-4862-ad9c-0fd77745ad70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lidar]",
   "language": "python",
   "name": "conda-env-lidar-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
