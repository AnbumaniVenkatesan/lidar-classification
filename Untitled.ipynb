{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1d3e6ea-d56c-40bf-b3d1-2395adda57f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(torch.cuda.is_available())\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(torch.cuda.get_device_name(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mNo GPU\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff2889b6-15e9-40ea-aa06-e38ec9201ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.10.0-cp313-cp313-win_amd64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\anaconda\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\anaconda\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in d:\\anaconda\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in d:\\anaconda\\lib\\site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.10.0-cp313-cp313-win_amd64.whl (113.8 MB)\n",
      "   ---------------------------------------- 0.0/113.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/113.8 MB 6.3 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 2.9/113.8 MB 7.4 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 4.7/113.8 MB 8.2 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 7.1/113.8 MB 9.1 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 10.0/113.8 MB 10.1 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 13.1/113.8 MB 11.0 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 17.0/113.8 MB 12.1 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 21.2/113.8 MB 13.1 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 25.7/113.8 MB 14.1 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 30.7/113.8 MB 15.1 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 35.4/113.8 MB 15.7 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 41.2/113.8 MB 16.7 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 47.4/113.8 MB 17.7 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 54.3/113.8 MB 18.7 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 61.3/113.8 MB 19.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 68.7/113.8 MB 20.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 75.2/113.8 MB 21.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 78.9/113.8 MB 21.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 82.3/113.8 MB 20.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 85.7/113.8 MB 20.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 88.3/113.8 MB 20.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 92.5/113.8 MB 20.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 97.3/113.8 MB 20.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 102.5/113.8 MB 20.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 108.0/113.8 MB 20.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  113.5/113.8 MB 20.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 113.8/113.8 MB 20.5 MB/s  0:00:05\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d615a0e7-1033-4481-b264-83a8cf0bc0ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m.cuda.is_available())\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(torch.cuda.get_device_name(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mNo GPU\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef68157f-f9dd-4b06-996a-811cdf290bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "No GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5201afb-94b2-4b94-9239-be3e82a98b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8372da71-146a-4a52-844c-8aa366cff7cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1905595262.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpython -m ipykernel install --user --name lidar --display-name \"Python (lidar)\"\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!pip install ipykernel\n",
    "python -m ipykernel install --user --name lidar --display-name \"Python (lidar)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7a260ca-8e5f-45b6-a5b0-7563d2d2520e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in d:\\anaconda\\lib\\site-packages (6.31.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in d:\\anaconda\\lib\\site-packages (from ipykernel) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in d:\\anaconda\\lib\\site-packages (from ipykernel) (1.8.16)\n",
      "Requirement already satisfied: ipython>=7.23.1 in d:\\anaconda\\lib\\site-packages (from ipykernel) (9.7.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in d:\\anaconda\\lib\\site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in d:\\anaconda\\lib\\site-packages (from ipykernel) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in d:\\anaconda\\lib\\site-packages (from ipykernel) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in d:\\anaconda\\lib\\site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in d:\\anaconda\\lib\\site-packages (from ipykernel) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in d:\\anaconda\\lib\\site-packages (from ipykernel) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=25 in d:\\anaconda\\lib\\site-packages (from ipykernel) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in d:\\anaconda\\lib\\site-packages (from ipykernel) (6.5.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in d:\\anaconda\\lib\\site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: colorama>=0.4.4 in d:\\anaconda\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
      "Requirement already satisfied: decorator>=4.3.2 in d:\\anaconda\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in d:\\anaconda\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in d:\\anaconda\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in d:\\anaconda\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in d:\\anaconda\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in d:\\anaconda\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in d:\\anaconda\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\anaconda\\lib\\site-packages (from jedi>=0.18.1->ipython>=7.23.1->ipykernel) (0.8.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\lib\\site-packages (from jupyter-client>=8.0.0->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in d:\\anaconda\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.5.0)\n",
      "Requirement already satisfied: pywin32>=300 in d:\\anaconda\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (311)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\anaconda\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\anaconda\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in d:\\anaconda\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Installed kernelspec lidar in C:\\Users\\Elegance geo tech\\AppData\\Roaming\\jupyter\\kernels\\lidar\n"
     ]
    }
   ],
   "source": [
    "!pip install ipykernel\n",
    "!python -m ipykernel install --user --name lidar --display-name \"Python (lidar)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "120e1dbb-1b9f-4ee4-9e4f-8cb4ca6c9d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in d:\\anaconda\\envs\\lidar\\lib\\site-packages (26.0.1)\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torch-2.10.0%2Bcu126-cp310-cp310-win_amd64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.25.0%2Bcu126-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.10.0%2Bcu126-cp310-cp310-win_amd64.whl.metadata (7.1 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Downloading numpy-2.2.6-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading pillow-12.0.0-cp310-cp310-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp310-cp310-win_amd64.whl (17 kB)\n",
      "Downloading https://download.pytorch.org/whl/cu126/torch-2.10.0%2Bcu126-cp310-cp310-win_amd64.whl (2589.8 MB)\n",
      "   ---------------------------------------- 0.0/2.6 GB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.6 GB 58.1 MB/s eta 0:00:45\n",
      "   ---------------------------------------- 0.0/2.6 GB 35.6 MB/s eta 0:01:13\n",
      "   ---------------------------------------- 0.0/2.6 GB 28.9 MB/s eta 0:01:29\n",
      "   ---------------------------------------- 0.0/2.6 GB 26.9 MB/s eta 0:01:36\n",
      "   ---------------------------------------- 0.0/2.6 GB 23.0 MB/s eta 0:01:52\n",
      "   ---------------------------------------- 0.0/2.6 GB 25.4 MB/s eta 0:01:41\n",
      "   ---------------------------------------- 0.0/2.6 GB 21.3 MB/s eta 0:02:01\n",
      "    --------------------------------------- 0.0/2.6 GB 22.4 MB/s eta 0:01:55\n",
      "    --------------------------------------- 0.0/2.6 GB 22.7 MB/s eta 0:01:52\n",
      "    --------------------------------------- 0.0/2.6 GB 22.6 MB/s eta 0:01:53\n",
      "    --------------------------------------- 0.1/2.6 GB 21.7 MB/s eta 0:01:57\n",
      "    --------------------------------------- 0.1/2.6 GB 21.3 MB/s eta 0:01:59\n",
      "    --------------------------------------- 0.1/2.6 GB 21.3 MB/s eta 0:02:00\n",
      "    --------------------------------------- 0.1/2.6 GB 20.9 MB/s eta 0:02:01\n",
      "   - -------------------------------------- 0.1/2.6 GB 20.6 MB/s eta 0:02:03\n",
      "   - -------------------------------------- 0.1/2.6 GB 20.4 MB/s eta 0:02:04\n",
      "   - -------------------------------------- 0.1/2.6 GB 20.2 MB/s eta 0:02:05\n",
      "   - -------------------------------------- 0.1/2.6 GB 20.0 MB/s eta 0:02:06\n",
      "   - -------------------------------------- 0.1/2.6 GB 19.9 MB/s eta 0:02:07\n",
      "   - -------------------------------------- 0.1/2.6 GB 19.7 MB/s eta 0:02:08\n",
      "   - -------------------------------------- 0.1/2.6 GB 19.6 MB/s eta 0:02:08\n",
      "   - -------------------------------------- 0.1/2.6 GB 19.6 MB/s eta 0:02:08\n",
      "   - -------------------------------------- 0.1/2.6 GB 20.1 MB/s eta 0:02:05\n",
      "   - -------------------------------------- 0.1/2.6 GB 21.5 MB/s eta 0:01:56\n",
      "   - -------------------------------------- 0.1/2.6 GB 22.0 MB/s eta 0:01:53\n",
      "   - -------------------------------------- 0.1/2.6 GB 21.7 MB/s eta 0:01:54\n",
      "   - -------------------------------------- 0.1/2.6 GB 21.8 MB/s eta 0:01:53\n",
      "   - -------------------------------------- 0.1/2.6 GB 21.9 MB/s eta 0:01:53\n",
      "   -- ------------------------------------- 0.1/2.6 GB 21.4 MB/s eta 0:01:55\n",
      "   -- ------------------------------------- 0.1/2.6 GB 21.7 MB/s eta 0:01:53\n",
      "   -- ------------------------------------- 0.1/2.6 GB 21.6 MB/s eta 0:01:54\n",
      "   -- ------------------------------------- 0.1/2.6 GB 21.3 MB/s eta 0:01:55\n",
      "   -- ------------------------------------- 0.2/2.6 GB 21.5 MB/s eta 0:01:54\n",
      "   -- ------------------------------------- 0.2/2.6 GB 21.0 MB/s eta 0:01:56\n",
      "   -- ------------------------------------- 0.2/2.6 GB 21.3 MB/s eta 0:01:55\n",
      "   -- ------------------------------------- 0.2/2.6 GB 21.5 MB/s eta 0:01:53\n",
      "   -- ------------------------------------- 0.2/2.6 GB 21.8 MB/s eta 0:01:52\n",
      "   -- ------------------------------------- 0.2/2.6 GB 21.8 MB/s eta 0:01:52\n",
      "   -- ------------------------------------- 0.2/2.6 GB 21.6 MB/s eta 0:01:52\n",
      "   -- ------------------------------------- 0.2/2.6 GB 21.2 MB/s eta 0:01:54\n",
      "   -- ------------------------------------- 0.2/2.6 GB 21.5 MB/s eta 0:01:52\n",
      "   -- ------------------------------------- 0.2/2.6 GB 21.2 MB/s eta 0:01:54\n",
      "   --- ------------------------------------ 0.2/2.6 GB 21.5 MB/s eta 0:01:52\n",
      "   --- ------------------------------------ 0.2/2.6 GB 21.5 MB/s eta 0:01:52\n",
      "   --- ------------------------------------ 0.2/2.6 GB 21.4 MB/s eta 0:01:52\n",
      "   --- ------------------------------------ 0.2/2.6 GB 21.5 MB/s eta 0:01:51\n",
      "   --- ------------------------------------ 0.2/2.6 GB 21.4 MB/s eta 0:01:51\n",
      "   --- ------------------------------------ 0.2/2.6 GB 21.4 MB/s eta 0:01:51\n",
      "   --- ------------------------------------ 0.2/2.6 GB 21.0 MB/s eta 0:01:53\n",
      "   --- ------------------------------------ 0.2/2.6 GB 21.0 MB/s eta 0:01:53\n",
      "   --- ------------------------------------ 0.2/2.6 GB 21.0 MB/s eta 0:01:53\n",
      "   --- ------------------------------------ 0.2/2.6 GB 21.0 MB/s eta 0:01:53\n",
      "   --- ------------------------------------ 0.2/2.6 GB 21.0 MB/s eta 0:01:53\n",
      "   --- ------------------------------------ 0.2/2.6 GB 21.0 MB/s eta 0:01:52\n",
      "   --- ------------------------------------ 0.2/2.6 GB 21.0 MB/s eta 0:01:52\n",
      "   --- ------------------------------------ 0.3/2.6 GB 21.0 MB/s eta 0:01:52\n",
      "   --- ------------------------------------ 0.3/2.6 GB 21.0 MB/s eta 0:01:52\n",
      "   ---- ----------------------------------- 0.3/2.6 GB 21.0 MB/s eta 0:01:51\n",
      "   ---- ----------------------------------- 0.3/2.6 GB 21.0 MB/s eta 0:01:51\n",
      "   ---- ----------------------------------- 0.3/2.6 GB 21.5 MB/s eta 0:01:48\n",
      "   ---- ----------------------------------- 0.3/2.6 GB 21.6 MB/s eta 0:01:47\n",
      "   ---- ----------------------------------- 0.3/2.6 GB 21.6 MB/s eta 0:01:47\n",
      "   ---- ----------------------------------- 0.3/2.6 GB 21.0 MB/s eta 0:01:50\n",
      "   ---- ----------------------------------- 0.3/2.6 GB 20.8 MB/s eta 0:01:51\n",
      "   ---- ----------------------------------- 0.3/2.6 GB 21.6 MB/s eta 0:01:46\n",
      "   ---- ----------------------------------- 0.3/2.6 GB 22.1 MB/s eta 0:01:44\n",
      "   ---- ----------------------------------- 0.3/2.6 GB 22.1 MB/s eta 0:01:43\n",
      "   ---- ----------------------------------- 0.3/2.6 GB 22.1 MB/s eta 0:01:43\n",
      "   ----- ---------------------------------- 0.3/2.6 GB 22.5 MB/s eta 0:01:41\n",
      "   ----- ---------------------------------- 0.3/2.6 GB 22.6 MB/s eta 0:01:40\n",
      "   ----- ---------------------------------- 0.3/2.6 GB 22.8 MB/s eta 0:01:39\n",
      "   ----- ---------------------------------- 0.3/2.6 GB 22.9 MB/s eta 0:01:39\n",
      "   ----- ---------------------------------- 0.3/2.6 GB 23.1 MB/s eta 0:01:38\n",
      "   ----- ---------------------------------- 0.4/2.6 GB 23.2 MB/s eta 0:01:37\n",
      "   ----- ---------------------------------- 0.4/2.6 GB 23.2 MB/s eta 0:01:37\n",
      "   ----- ---------------------------------- 0.4/2.6 GB 23.1 MB/s eta 0:01:37\n",
      "   ----- ---------------------------------- 0.4/2.6 GB 22.9 MB/s eta 0:01:38\n",
      "   ----- ---------------------------------- 0.4/2.6 GB 22.6 MB/s eta 0:01:39\n",
      "   ----- ---------------------------------- 0.4/2.6 GB 22.4 MB/s eta 0:01:39\n",
      "   ----- ---------------------------------- 0.4/2.6 GB 22.6 MB/s eta 0:01:38\n",
      "   ----- ---------------------------------- 0.4/2.6 GB 22.6 MB/s eta 0:01:38\n",
      "   ------ --------------------------------- 0.4/2.6 GB 22.7 MB/s eta 0:01:37\n",
      "   ------ --------------------------------- 0.4/2.6 GB 22.9 MB/s eta 0:01:36\n",
      "   ------ --------------------------------- 0.4/2.6 GB 22.9 MB/s eta 0:01:36\n",
      "   ------ --------------------------------- 0.4/2.6 GB 22.8 MB/s eta 0:01:36\n",
      "   ------ --------------------------------- 0.4/2.6 GB 23.6 MB/s eta 0:01:33\n",
      "   ------ --------------------------------- 0.4/2.6 GB 23.2 MB/s eta 0:01:34\n",
      "   ------ --------------------------------- 0.4/2.6 GB 23.1 MB/s eta 0:01:34\n",
      "   ------ --------------------------------- 0.4/2.6 GB 23.2 MB/s eta 0:01:34\n",
      "   ------ --------------------------------- 0.4/2.6 GB 23.7 MB/s eta 0:01:31\n",
      "   ------ --------------------------------- 0.4/2.6 GB 23.4 MB/s eta 0:01:32\n",
      "   ------ --------------------------------- 0.4/2.6 GB 23.6 MB/s eta 0:01:31\n",
      "   ------ --------------------------------- 0.4/2.6 GB 23.3 MB/s eta 0:01:32\n",
      "   ------- -------------------------------- 0.5/2.6 GB 23.9 MB/s eta 0:01:30\n",
      "   ------- -------------------------------- 0.5/2.6 GB 23.6 MB/s eta 0:01:31\n",
      "   ------- -------------------------------- 0.5/2.6 GB 23.5 MB/s eta 0:01:31\n",
      "   ------- -------------------------------- 0.5/2.6 GB 23.6 MB/s eta 0:01:30\n",
      "   ------- -------------------------------- 0.5/2.6 GB 23.7 MB/s eta 0:01:30\n",
      "   ------- -------------------------------- 0.5/2.6 GB 23.8 MB/s eta 0:01:29\n",
      "   ------- -------------------------------- 0.5/2.6 GB 24.3 MB/s eta 0:01:27\n",
      "   ------- -------------------------------- 0.5/2.6 GB 24.4 MB/s eta 0:01:27\n",
      "   ------- -------------------------------- 0.5/2.6 GB 24.5 MB/s eta 0:01:26\n",
      "   ------- -------------------------------- 0.5/2.6 GB 24.6 MB/s eta 0:01:26\n",
      "   ------- -------------------------------- 0.5/2.6 GB 24.6 MB/s eta 0:01:25\n",
      "   ------- -------------------------------- 0.5/2.6 GB 24.8 MB/s eta 0:01:24\n",
      "   ------- -------------------------------- 0.5/2.6 GB 24.8 MB/s eta 0:01:24\n",
      "   -------- ------------------------------- 0.5/2.6 GB 24.9 MB/s eta 0:01:24\n",
      "   -------- ------------------------------- 0.5/2.6 GB 24.8 MB/s eta 0:01:24\n",
      "   -------- ------------------------------- 0.5/2.6 GB 24.6 MB/s eta 0:01:24\n",
      "   -------- ------------------------------- 0.5/2.6 GB 24.3 MB/s eta 0:01:25\n",
      "   -------- ------------------------------- 0.5/2.6 GB 24.1 MB/s eta 0:01:26\n",
      "   -------- ------------------------------- 0.5/2.6 GB 24.8 MB/s eta 0:01:23\n",
      "   -------- ------------------------------- 0.6/2.6 GB 25.3 MB/s eta 0:01:21\n",
      "   -------- ------------------------------- 0.6/2.6 GB 25.0 MB/s eta 0:01:22\n",
      "   -------- ------------------------------- 0.6/2.6 GB 24.7 MB/s eta 0:01:23\n",
      "   -------- ------------------------------- 0.6/2.6 GB 24.3 MB/s eta 0:01:24\n",
      "   -------- ------------------------------- 0.6/2.6 GB 24.1 MB/s eta 0:01:24\n",
      "   -------- ------------------------------- 0.6/2.6 GB 24.3 MB/s eta 0:01:23\n",
      "   --------- ------------------------------ 0.6/2.6 GB 24.1 MB/s eta 0:01:24\n",
      "   --------- ------------------------------ 0.6/2.6 GB 24.1 MB/s eta 0:01:24\n",
      "   --------- ------------------------------ 0.6/2.6 GB 24.1 MB/s eta 0:01:23\n",
      "   --------- ------------------------------ 0.6/2.6 GB 24.2 MB/s eta 0:01:23\n",
      "   --------- ------------------------------ 0.6/2.6 GB 24.2 MB/s eta 0:01:23\n",
      "   --------- ------------------------------ 0.6/2.6 GB 24.2 MB/s eta 0:01:22\n",
      "   --------- ------------------------------ 0.6/2.6 GB 24.3 MB/s eta 0:01:22\n",
      "   --------- ------------------------------ 0.6/2.6 GB 24.3 MB/s eta 0:01:22\n",
      "   --------- ------------------------------ 0.6/2.6 GB 24.3 MB/s eta 0:01:22\n",
      "   --------- ------------------------------ 0.6/2.6 GB 23.9 MB/s eta 0:01:23\n",
      "   --------- ------------------------------ 0.6/2.6 GB 23.9 MB/s eta 0:01:22\n",
      "   --------- ------------------------------ 0.6/2.6 GB 24.0 MB/s eta 0:01:22\n",
      "   --------- ------------------------------ 0.6/2.6 GB 24.1 MB/s eta 0:01:21\n",
      "   ---------- ----------------------------- 0.7/2.6 GB 24.2 MB/s eta 0:01:21\n",
      "   ---------- ----------------------------- 0.7/2.6 GB 24.0 MB/s eta 0:01:21\n",
      "   ---------- ----------------------------- 0.7/2.6 GB 23.8 MB/s eta 0:01:22\n",
      "   ---------- ----------------------------- 0.7/2.6 GB 24.1 MB/s eta 0:01:20\n",
      "   ---------- ----------------------------- 0.7/2.6 GB 24.2 MB/s eta 0:01:20\n",
      "   ---------- ----------------------------- 0.7/2.6 GB 23.9 MB/s eta 0:01:21\n",
      "   ---------- ----------------------------- 0.7/2.6 GB 23.9 MB/s eta 0:01:21\n",
      "   ---------- ----------------------------- 0.7/2.6 GB 24.0 MB/s eta 0:01:20\n",
      "   ---------- ----------------------------- 0.7/2.6 GB 23.6 MB/s eta 0:01:21\n",
      "   ---------- ----------------------------- 0.7/2.6 GB 23.6 MB/s eta 0:01:21\n",
      "   ---------- ----------------------------- 0.7/2.6 GB 23.5 MB/s eta 0:01:21\n",
      "   ---------- ----------------------------- 0.7/2.6 GB 23.4 MB/s eta 0:01:21\n",
      "   ---------- ----------------------------- 0.7/2.6 GB 23.3 MB/s eta 0:01:21\n",
      "   ---------- ----------------------------- 0.7/2.6 GB 23.2 MB/s eta 0:01:22\n",
      "   ----------- ---------------------------- 0.7/2.6 GB 23.1 MB/s eta 0:01:22\n",
      "   ----------- ---------------------------- 0.7/2.6 GB 23.1 MB/s eta 0:01:22\n",
      "   ----------- ---------------------------- 0.7/2.6 GB 23.0 MB/s eta 0:01:22\n",
      "   ----------- ---------------------------- 0.7/2.6 GB 22.8 MB/s eta 0:01:22\n",
      "   ----------- ---------------------------- 0.7/2.6 GB 23.1 MB/s eta 0:01:21\n",
      "   ----------- ---------------------------- 0.7/2.6 GB 23.3 MB/s eta 0:01:20\n",
      "   ----------- ---------------------------- 0.8/2.6 GB 23.8 MB/s eta 0:01:18\n",
      "   ----------- ---------------------------- 0.8/2.6 GB 23.8 MB/s eta 0:01:18\n",
      "   ----------- ---------------------------- 0.8/2.6 GB 23.8 MB/s eta 0:01:17\n",
      "   ----------- ---------------------------- 0.8/2.6 GB 23.7 MB/s eta 0:01:17\n",
      "   ----------- ---------------------------- 0.8/2.6 GB 23.6 MB/s eta 0:01:18\n",
      "   ----------- ---------------------------- 0.8/2.6 GB 23.6 MB/s eta 0:01:17\n",
      "   ------------ --------------------------- 0.8/2.6 GB 23.6 MB/s eta 0:01:17\n",
      "   ------------ --------------------------- 0.8/2.6 GB 23.5 MB/s eta 0:01:17\n",
      "   ------------ --------------------------- 0.8/2.6 GB 23.5 MB/s eta 0:01:17\n",
      "   ------------ --------------------------- 0.8/2.6 GB 23.5 MB/s eta 0:01:17\n",
      "   ------------ --------------------------- 0.8/2.6 GB 23.4 MB/s eta 0:01:17\n",
      "   ------------ --------------------------- 0.8/2.6 GB 23.3 MB/s eta 0:01:17\n",
      "   ------------ --------------------------- 0.8/2.6 GB 23.3 MB/s eta 0:01:17\n",
      "   ------------ --------------------------- 0.8/2.6 GB 23.3 MB/s eta 0:01:17\n",
      "   ------------ --------------------------- 0.8/2.6 GB 23.2 MB/s eta 0:01:17\n",
      "   ------------ --------------------------- 0.8/2.6 GB 23.2 MB/s eta 0:01:17\n",
      "   ------------ --------------------------- 0.8/2.6 GB 23.1 MB/s eta 0:01:17\n",
      "   ------------ --------------------------- 0.8/2.6 GB 23.2 MB/s eta 0:01:16\n",
      "   ------------ --------------------------- 0.8/2.6 GB 23.5 MB/s eta 0:01:15\n",
      "   ------------- -------------------------- 0.8/2.6 GB 23.6 MB/s eta 0:01:14\n",
      "   ------------- -------------------------- 0.9/2.6 GB 23.5 MB/s eta 0:01:14\n",
      "   ------------- -------------------------- 0.9/2.6 GB 23.4 MB/s eta 0:01:15\n",
      "   ------------- -------------------------- 0.9/2.6 GB 23.4 MB/s eta 0:01:15\n",
      "   ------------- -------------------------- 0.9/2.6 GB 22.6 MB/s eta 0:01:17\n",
      "   ------------- -------------------------- 0.9/2.6 GB 22.2 MB/s eta 0:01:18\n",
      "   ------------- -------------------------- 0.9/2.6 GB 22.2 MB/s eta 0:01:18\n",
      "   ------------- -------------------------- 0.9/2.6 GB 22.1 MB/s eta 0:01:18\n",
      "   ------------- -------------------------- 0.9/2.6 GB 22.2 MB/s eta 0:01:18\n",
      "   ------------- -------------------------- 0.9/2.6 GB 22.3 MB/s eta 0:01:17\n",
      "   ------------- -------------------------- 0.9/2.6 GB 23.1 MB/s eta 0:01:14\n",
      "   ------------- -------------------------- 0.9/2.6 GB 23.2 MB/s eta 0:01:14\n",
      "   ------------- -------------------------- 0.9/2.6 GB 23.3 MB/s eta 0:01:13\n",
      "   ------------- -------------------------- 0.9/2.6 GB 23.0 MB/s eta 0:01:14\n",
      "   -------------- ------------------------- 0.9/2.6 GB 22.9 MB/s eta 0:01:14\n",
      "   -------------- ------------------------- 0.9/2.6 GB 22.8 MB/s eta 0:01:14\n",
      "   -------------- ------------------------- 0.9/2.6 GB 22.9 MB/s eta 0:01:14\n",
      "   -------------- ------------------------- 0.9/2.6 GB 22.9 MB/s eta 0:01:13\n",
      "   -------------- ------------------------- 0.9/2.6 GB 22.9 MB/s eta 0:01:13\n",
      "   -------------- ------------------------- 0.9/2.6 GB 23.1 MB/s eta 0:01:12\n",
      "   -------------- ------------------------- 0.9/2.6 GB 23.1 MB/s eta 0:01:12\n",
      "   -------------- ------------------------- 0.9/2.6 GB 23.2 MB/s eta 0:01:11\n",
      "   -------------- ------------------------- 1.0/2.6 GB 23.3 MB/s eta 0:01:11\n",
      "   -------------- ------------------------- 1.0/2.6 GB 23.3 MB/s eta 0:01:11\n",
      "   -------------- ------------------------- 1.0/2.6 GB 23.5 MB/s eta 0:01:10\n",
      "   -------------- ------------------------- 1.0/2.6 GB 23.6 MB/s eta 0:01:09\n",
      "   -------------- ------------------------- 1.0/2.6 GB 23.6 MB/s eta 0:01:09\n",
      "   --------------- ------------------------ 1.0/2.6 GB 23.8 MB/s eta 0:01:08\n",
      "   --------------- ------------------------ 1.0/2.6 GB 23.9 MB/s eta 0:01:08\n",
      "   --------------- ------------------------ 1.0/2.6 GB 24.0 MB/s eta 0:01:07\n",
      "   --------------- ------------------------ 1.0/2.6 GB 23.9 MB/s eta 0:01:07\n",
      "   --------------- ------------------------ 1.0/2.6 GB 23.8 MB/s eta 0:01:07\n",
      "   --------------- ------------------------ 1.0/2.6 GB 23.6 MB/s eta 0:01:08\n",
      "   --------------- ------------------------ 1.0/2.6 GB 23.4 MB/s eta 0:01:08\n",
      "   --------------- ------------------------ 1.0/2.6 GB 23.1 MB/s eta 0:01:09\n",
      "   --------------- ------------------------ 1.0/2.6 GB 22.9 MB/s eta 0:01:09\n",
      "   --------------- ------------------------ 1.0/2.6 GB 23.1 MB/s eta 0:01:08\n",
      "   --------------- ------------------------ 1.0/2.6 GB 23.1 MB/s eta 0:01:08\n",
      "   --------------- ------------------------ 1.0/2.6 GB 22.8 MB/s eta 0:01:09\n",
      "   --------------- ------------------------ 1.0/2.6 GB 22.7 MB/s eta 0:01:09\n",
      "   --------------- ------------------------ 1.0/2.6 GB 22.3 MB/s eta 0:01:10\n",
      "   --------------- ------------------------ 1.0/2.6 GB 22.3 MB/s eta 0:01:10\n",
      "   ---------------- ----------------------- 1.0/2.6 GB 22.1 MB/s eta 0:01:11\n",
      "   ---------------- ----------------------- 1.0/2.6 GB 22.1 MB/s eta 0:01:10\n",
      "   ---------------- ----------------------- 1.0/2.6 GB 22.1 MB/s eta 0:01:10\n",
      "   ---------------- ----------------------- 1.1/2.6 GB 22.0 MB/s eta 0:01:10\n",
      "   ---------------- ----------------------- 1.1/2.6 GB 21.9 MB/s eta 0:01:10\n",
      "   ---------------- ----------------------- 1.1/2.6 GB 21.9 MB/s eta 0:01:11\n",
      "   ---------------- ----------------------- 1.1/2.6 GB 21.9 MB/s eta 0:01:10\n",
      "   ---------------- ----------------------- 1.1/2.6 GB 21.7 MB/s eta 0:01:11\n",
      "   ---------------- ----------------------- 1.1/2.6 GB 21.7 MB/s eta 0:01:11\n",
      "   ---------------- ----------------------- 1.1/2.6 GB 21.6 MB/s eta 0:01:11\n",
      "   ---------------- ----------------------- 1.1/2.6 GB 21.7 MB/s eta 0:01:10\n",
      "   ---------------- ----------------------- 1.1/2.6 GB 22.1 MB/s eta 0:01:09\n",
      "   ---------------- ----------------------- 1.1/2.6 GB 22.2 MB/s eta 0:01:08\n",
      "   ----------------- ---------------------- 1.1/2.6 GB 22.1 MB/s eta 0:01:08\n",
      "   ----------------- ---------------------- 1.1/2.6 GB 21.9 MB/s eta 0:01:08\n",
      "   ----------------- ---------------------- 1.1/2.6 GB 22.2 MB/s eta 0:01:07\n",
      "   ----------------- ---------------------- 1.1/2.6 GB 22.0 MB/s eta 0:01:08\n",
      "   ----------------- ---------------------- 1.1/2.6 GB 23.2 MB/s eta 0:01:04\n",
      "   ----------------- ---------------------- 1.1/2.6 GB 23.3 MB/s eta 0:01:03\n",
      "   ----------------- ---------------------- 1.1/2.6 GB 23.2 MB/s eta 0:01:03\n",
      "   ----------------- ---------------------- 1.1/2.6 GB 23.1 MB/s eta 0:01:03\n",
      "   ----------------- ---------------------- 1.1/2.6 GB 22.9 MB/s eta 0:01:04\n",
      "   ----------------- ---------------------- 1.1/2.6 GB 22.7 MB/s eta 0:01:04\n",
      "   ----------------- ---------------------- 1.2/2.6 GB 22.6 MB/s eta 0:01:04\n",
      "   ----------------- ---------------------- 1.2/2.6 GB 22.4 MB/s eta 0:01:05\n",
      "   ----------------- ---------------------- 1.2/2.6 GB 22.3 MB/s eta 0:01:05\n",
      "   ------------------ --------------------- 1.2/2.6 GB 22.0 MB/s eta 0:01:05\n",
      "   ------------------ --------------------- 1.2/2.6 GB 22.3 MB/s eta 0:01:04\n",
      "   ------------------ --------------------- 1.2/2.6 GB 22.4 MB/s eta 0:01:04\n",
      "   ------------------ --------------------- 1.2/2.6 GB 22.2 MB/s eta 0:01:04\n",
      "   ------------------ --------------------- 1.2/2.6 GB 22.3 MB/s eta 0:01:04\n",
      "   ------------------ --------------------- 1.2/2.6 GB 22.1 MB/s eta 0:01:04\n",
      "   ------------------ --------------------- 1.2/2.6 GB 22.2 MB/s eta 0:01:03\n",
      "   ------------------ --------------------- 1.2/2.6 GB 22.5 MB/s eta 0:01:02\n",
      "   ------------------ --------------------- 1.2/2.6 GB 22.6 MB/s eta 0:01:01\n",
      "   ------------------ --------------------- 1.2/2.6 GB 22.5 MB/s eta 0:01:02\n",
      "   ------------------ --------------------- 1.2/2.6 GB 22.7 MB/s eta 0:01:01\n",
      "   ------------------ --------------------- 1.2/2.6 GB 22.4 MB/s eta 0:01:01\n",
      "   ------------------ --------------------- 1.2/2.6 GB 22.6 MB/s eta 0:01:01\n",
      "   ------------------- -------------------- 1.2/2.6 GB 22.3 MB/s eta 0:01:02\n",
      "   ------------------- -------------------- 1.2/2.6 GB 22.4 MB/s eta 0:01:01\n",
      "   ------------------- -------------------- 1.2/2.6 GB 22.4 MB/s eta 0:01:01\n",
      "   ------------------- -------------------- 1.2/2.6 GB 22.0 MB/s eta 0:01:02\n",
      "   ------------------- -------------------- 1.2/2.6 GB 22.1 MB/s eta 0:01:01\n",
      "   ------------------- -------------------- 1.3/2.6 GB 22.1 MB/s eta 0:01:01\n",
      "   ------------------- -------------------- 1.3/2.6 GB 22.1 MB/s eta 0:01:00\n",
      "   ------------------- -------------------- 1.3/2.6 GB 21.9 MB/s eta 0:01:01\n",
      "   ------------------- -------------------- 1.3/2.6 GB 22.2 MB/s eta 0:01:00\n",
      "   ------------------- -------------------- 1.3/2.6 GB 22.5 MB/s eta 0:00:59\n",
      "   ------------------- -------------------- 1.3/2.6 GB 22.3 MB/s eta 0:00:59\n",
      "   ------------------- -------------------- 1.3/2.6 GB 22.3 MB/s eta 0:00:59\n",
      "   ------------------- -------------------- 1.3/2.6 GB 23.0 MB/s eta 0:00:57\n",
      "   -------------------- ------------------- 1.3/2.6 GB 23.1 MB/s eta 0:00:56\n",
      "   -------------------- ------------------- 1.3/2.6 GB 23.2 MB/s eta 0:00:56\n",
      "   -------------------- ------------------- 1.3/2.6 GB 23.3 MB/s eta 0:00:56\n",
      "   -------------------- ------------------- 1.3/2.6 GB 23.5 MB/s eta 0:00:55\n",
      "   -------------------- ------------------- 1.3/2.6 GB 23.7 MB/s eta 0:00:54\n",
      "   -------------------- ------------------- 1.3/2.6 GB 23.7 MB/s eta 0:00:54\n",
      "   -------------------- ------------------- 1.3/2.6 GB 23.6 MB/s eta 0:00:54\n",
      "   -------------------- ------------------- 1.3/2.6 GB 23.6 MB/s eta 0:00:54\n",
      "   -------------------- ------------------- 1.3/2.6 GB 24.0 MB/s eta 0:00:53\n",
      "   -------------------- ------------------- 1.3/2.6 GB 24.0 MB/s eta 0:00:52\n",
      "   -------------------- ------------------- 1.3/2.6 GB 23.7 MB/s eta 0:00:53\n",
      "   -------------------- ------------------- 1.3/2.6 GB 23.5 MB/s eta 0:00:53\n",
      "   -------------------- ------------------- 1.4/2.6 GB 23.1 MB/s eta 0:00:54\n",
      "   --------------------- ------------------ 1.4/2.6 GB 22.9 MB/s eta 0:00:54\n",
      "   --------------------- ------------------ 1.4/2.6 GB 22.8 MB/s eta 0:00:54\n",
      "   --------------------- ------------------ 1.4/2.6 GB 22.7 MB/s eta 0:00:54\n",
      "   --------------------- ------------------ 1.4/2.6 GB 22.6 MB/s eta 0:00:55\n",
      "   --------------------- ------------------ 1.4/2.6 GB 22.6 MB/s eta 0:00:54\n",
      "   --------------------- ------------------ 1.4/2.6 GB 22.5 MB/s eta 0:00:54\n",
      "   --------------------- ------------------ 1.4/2.6 GB 22.3 MB/s eta 0:00:55\n",
      "   --------------------- ------------------ 1.4/2.6 GB 22.2 MB/s eta 0:00:55\n",
      "   --------------------- ------------------ 1.4/2.6 GB 22.0 MB/s eta 0:00:55\n",
      "   --------------------- ------------------ 1.4/2.6 GB 21.9 MB/s eta 0:00:55\n",
      "   --------------------- ------------------ 1.4/2.6 GB 21.8 MB/s eta 0:00:55\n",
      "   --------------------- ------------------ 1.4/2.6 GB 21.7 MB/s eta 0:00:55\n",
      "   --------------------- ------------------ 1.4/2.6 GB 21.7 MB/s eta 0:00:55\n",
      "   --------------------- ------------------ 1.4/2.6 GB 21.9 MB/s eta 0:00:54\n",
      "   --------------------- ------------------ 1.4/2.6 GB 22.4 MB/s eta 0:00:53\n",
      "   ---------------------- ----------------- 1.4/2.6 GB 22.7 MB/s eta 0:00:52\n",
      "   ---------------------- ----------------- 1.4/2.6 GB 22.7 MB/s eta 0:00:52\n",
      "   ---------------------- ----------------- 1.4/2.6 GB 22.6 MB/s eta 0:00:52\n",
      "   ---------------------- ----------------- 1.4/2.6 GB 22.6 MB/s eta 0:00:51\n",
      "   ---------------------- ----------------- 1.4/2.6 GB 22.7 MB/s eta 0:00:51\n",
      "   ---------------------- ----------------- 1.4/2.6 GB 22.7 MB/s eta 0:00:51\n",
      "   ---------------------- ----------------- 1.4/2.6 GB 22.7 MB/s eta 0:00:51\n",
      "   ---------------------- ----------------- 1.5/2.6 GB 21.9 MB/s eta 0:00:53\n",
      "   ---------------------- ----------------- 1.5/2.6 GB 21.4 MB/s eta 0:00:54\n",
      "   ---------------------- ----------------- 1.5/2.6 GB 21.2 MB/s eta 0:00:54\n",
      "   ---------------------- ----------------- 1.5/2.6 GB 21.0 MB/s eta 0:00:54\n",
      "   ---------------------- ----------------- 1.5/2.6 GB 20.9 MB/s eta 0:00:54\n",
      "   ---------------------- ----------------- 1.5/2.6 GB 20.7 MB/s eta 0:00:55\n",
      "   ---------------------- ----------------- 1.5/2.6 GB 20.9 MB/s eta 0:00:54\n",
      "   ---------------------- ----------------- 1.5/2.6 GB 20.8 MB/s eta 0:00:54\n",
      "   ---------------------- ----------------- 1.5/2.6 GB 21.4 MB/s eta 0:00:52\n",
      "   ----------------------- ---------------- 1.5/2.6 GB 21.4 MB/s eta 0:00:52\n",
      "   ----------------------- ---------------- 1.5/2.6 GB 22.0 MB/s eta 0:00:50\n",
      "   ----------------------- ---------------- 1.5/2.6 GB 22.2 MB/s eta 0:00:49\n",
      "   ----------------------- ---------------- 1.5/2.6 GB 22.0 MB/s eta 0:00:49\n",
      "   ----------------------- ---------------- 1.5/2.6 GB 22.0 MB/s eta 0:00:49\n",
      "   ----------------------- ---------------- 1.5/2.6 GB 21.8 MB/s eta 0:00:49\n",
      "   ----------------------- ---------------- 1.5/2.6 GB 21.9 MB/s eta 0:00:49\n",
      "   ----------------------- ---------------- 1.5/2.6 GB 21.9 MB/s eta 0:00:49\n",
      "   ----------------------- ---------------- 1.5/2.6 GB 21.9 MB/s eta 0:00:48\n",
      "   ----------------------- ---------------- 1.5/2.6 GB 21.8 MB/s eta 0:00:49\n",
      "   ----------------------- ---------------- 1.5/2.6 GB 21.7 MB/s eta 0:00:48\n",
      "   ------------------------ --------------- 1.6/2.6 GB 21.6 MB/s eta 0:00:48\n",
      "   ------------------------ --------------- 1.6/2.6 GB 21.5 MB/s eta 0:00:49\n",
      "   ------------------------ --------------- 1.6/2.6 GB 21.4 MB/s eta 0:00:48\n",
      "   ------------------------ --------------- 1.6/2.6 GB 21.3 MB/s eta 0:00:48\n",
      "   ------------------------ --------------- 1.6/2.6 GB 21.3 MB/s eta 0:00:48\n",
      "   ------------------------ --------------- 1.6/2.6 GB 21.1 MB/s eta 0:00:49\n",
      "   ------------------------ --------------- 1.6/2.6 GB 21.1 MB/s eta 0:00:48\n",
      "   ------------------------ --------------- 1.6/2.6 GB 21.0 MB/s eta 0:00:48\n",
      "   ------------------------ --------------- 1.6/2.6 GB 21.4 MB/s eta 0:00:47\n",
      "   ------------------------ --------------- 1.6/2.6 GB 21.2 MB/s eta 0:00:48\n",
      "   ------------------------ --------------- 1.6/2.6 GB 21.1 MB/s eta 0:00:48\n",
      "   ------------------------ --------------- 1.6/2.6 GB 21.0 MB/s eta 0:00:48\n",
      "   ------------------------ --------------- 1.6/2.6 GB 21.1 MB/s eta 0:00:47\n",
      "   ------------------------ --------------- 1.6/2.6 GB 21.4 MB/s eta 0:00:46\n",
      "   ------------------------- -------------- 1.6/2.6 GB 21.3 MB/s eta 0:00:46\n",
      "   ------------------------- -------------- 1.6/2.6 GB 21.8 MB/s eta 0:00:45\n",
      "   ------------------------- -------------- 1.6/2.6 GB 22.1 MB/s eta 0:00:44\n",
      "   ------------------------- -------------- 1.6/2.6 GB 22.5 MB/s eta 0:00:43\n",
      "   ------------------------- -------------- 1.6/2.6 GB 23.0 MB/s eta 0:00:42\n",
      "   ------------------------- -------------- 1.6/2.6 GB 23.0 MB/s eta 0:00:42\n",
      "   ------------------------- -------------- 1.6/2.6 GB 22.5 MB/s eta 0:00:42\n",
      "   ------------------------- -------------- 1.7/2.6 GB 22.7 MB/s eta 0:00:41\n",
      "   ------------------------- -------------- 1.7/2.6 GB 23.5 MB/s eta 0:00:40\n",
      "   ------------------------- -------------- 1.7/2.6 GB 23.3 MB/s eta 0:00:40\n",
      "   ------------------------- -------------- 1.7/2.6 GB 23.1 MB/s eta 0:00:40\n",
      "   ------------------------- -------------- 1.7/2.6 GB 22.8 MB/s eta 0:00:40\n",
      "   -------------------------- ------------- 1.7/2.6 GB 22.5 MB/s eta 0:00:41\n",
      "   -------------------------- ------------- 1.7/2.6 GB 22.2 MB/s eta 0:00:41\n",
      "   -------------------------- ------------- 1.7/2.6 GB 22.4 MB/s eta 0:00:40\n",
      "   -------------------------- ------------- 1.7/2.6 GB 22.4 MB/s eta 0:00:40\n",
      "   -------------------------- ------------- 1.7/2.6 GB 22.2 MB/s eta 0:00:41\n",
      "   -------------------------- ------------- 1.7/2.6 GB 22.4 MB/s eta 0:00:40\n",
      "   -------------------------- ------------- 1.7/2.6 GB 22.1 MB/s eta 0:00:40\n",
      "   -------------------------- ------------- 1.7/2.6 GB 23.2 MB/s eta 0:00:38\n",
      "   -------------------------- ------------- 1.7/2.6 GB 23.3 MB/s eta 0:00:38\n",
      "   -------------------------- ------------- 1.7/2.6 GB 23.2 MB/s eta 0:00:38\n",
      "   -------------------------- ------------- 1.7/2.6 GB 23.1 MB/s eta 0:00:38\n",
      "   -------------------------- ------------- 1.7/2.6 GB 23.0 MB/s eta 0:00:38\n",
      "   -------------------------- ------------- 1.7/2.6 GB 22.8 MB/s eta 0:00:38\n",
      "   -------------------------- ------------- 1.7/2.6 GB 22.6 MB/s eta 0:00:38\n",
      "   -------------------------- ------------- 1.7/2.6 GB 22.5 MB/s eta 0:00:38\n",
      "   -------------------------- ------------- 1.7/2.6 GB 22.3 MB/s eta 0:00:39\n",
      "   -------------------------- ------------- 1.7/2.6 GB 22.1 MB/s eta 0:00:39\n",
      "   -------------------------- ------------- 1.7/2.6 GB 21.8 MB/s eta 0:00:39\n",
      "   --------------------------- ------------ 1.7/2.6 GB 21.7 MB/s eta 0:00:39\n",
      "   --------------------------- ------------ 1.8/2.6 GB 21.4 MB/s eta 0:00:40\n",
      "   --------------------------- ------------ 1.8/2.6 GB 21.4 MB/s eta 0:00:39\n",
      "   --------------------------- ------------ 1.8/2.6 GB 21.5 MB/s eta 0:00:39\n",
      "   --------------------------- ------------ 1.8/2.6 GB 21.6 MB/s eta 0:00:38\n",
      "   --------------------------- ------------ 1.8/2.6 GB 21.7 MB/s eta 0:00:38\n",
      "   --------------------------- ------------ 1.8/2.6 GB 21.8 MB/s eta 0:00:37\n",
      "   --------------------------- ------------ 1.8/2.6 GB 21.9 MB/s eta 0:00:37\n",
      "   --------------------------- ------------ 1.8/2.6 GB 21.7 MB/s eta 0:00:37\n",
      "   --------------------------- ------------ 1.8/2.6 GB 21.9 MB/s eta 0:00:36\n",
      "   --------------------------- ------------ 1.8/2.6 GB 21.9 MB/s eta 0:00:36\n",
      "   ---------------------------- ----------- 1.8/2.6 GB 22.0 MB/s eta 0:00:36\n",
      "   ---------------------------- ----------- 1.8/2.6 GB 22.0 MB/s eta 0:00:36\n",
      "   ---------------------------- ----------- 1.8/2.6 GB 22.0 MB/s eta 0:00:35\n",
      "   ---------------------------- ----------- 1.8/2.6 GB 22.1 MB/s eta 0:00:35\n",
      "   ---------------------------- ----------- 1.8/2.6 GB 22.1 MB/s eta 0:00:35\n",
      "   ---------------------------- ----------- 1.8/2.6 GB 22.2 MB/s eta 0:00:34\n",
      "   ---------------------------- ----------- 1.8/2.6 GB 22.2 MB/s eta 0:00:34\n",
      "   ---------------------------- ----------- 1.8/2.6 GB 22.3 MB/s eta 0:00:34\n",
      "   ---------------------------- ----------- 1.9/2.6 GB 22.4 MB/s eta 0:00:33\n",
      "   ---------------------------- ----------- 1.9/2.6 GB 22.4 MB/s eta 0:00:33\n",
      "   ---------------------------- ----------- 1.9/2.6 GB 22.4 MB/s eta 0:00:33\n",
      "   ---------------------------- ----------- 1.9/2.6 GB 22.4 MB/s eta 0:00:33\n",
      "   ---------------------------- ----------- 1.9/2.6 GB 22.4 MB/s eta 0:00:32\n",
      "   ----------------------------- ---------- 1.9/2.6 GB 22.6 MB/s eta 0:00:32\n",
      "   ----------------------------- ---------- 1.9/2.6 GB 22.6 MB/s eta 0:00:32\n",
      "   ----------------------------- ---------- 1.9/2.6 GB 22.0 MB/s eta 0:00:33\n",
      "   ----------------------------- ---------- 1.9/2.6 GB 22.0 MB/s eta 0:00:33\n",
      "   ----------------------------- ---------- 1.9/2.6 GB 22.0 MB/s eta 0:00:32\n",
      "   ----------------------------- ---------- 1.9/2.6 GB 21.6 MB/s eta 0:00:32\n",
      "   ----------------------------- ---------- 1.9/2.6 GB 21.5 MB/s eta 0:00:32\n",
      "   ----------------------------- ---------- 1.9/2.6 GB 21.2 MB/s eta 0:00:33\n",
      "   ----------------------------- ---------- 1.9/2.6 GB 22.1 MB/s eta 0:00:31\n",
      "   ----------------------------- ---------- 1.9/2.6 GB 21.8 MB/s eta 0:00:31\n",
      "   ----------------------------- ---------- 1.9/2.6 GB 21.7 MB/s eta 0:00:31\n",
      "   ----------------------------- ---------- 1.9/2.6 GB 21.6 MB/s eta 0:00:31\n",
      "   ----------------------------- ---------- 1.9/2.6 GB 21.4 MB/s eta 0:00:31\n",
      "   ----------------------------- ---------- 1.9/2.6 GB 21.5 MB/s eta 0:00:31\n",
      "   ------------------------------ --------- 1.9/2.6 GB 21.4 MB/s eta 0:00:31\n",
      "   ------------------------------ --------- 1.9/2.6 GB 21.5 MB/s eta 0:00:30\n",
      "   ------------------------------ --------- 1.9/2.6 GB 21.5 MB/s eta 0:00:30\n",
      "   ------------------------------ --------- 2.0/2.6 GB 21.1 MB/s eta 0:00:31\n",
      "   ------------------------------ --------- 2.0/2.6 GB 20.9 MB/s eta 0:00:31\n",
      "   ------------------------------ --------- 2.0/2.6 GB 21.1 MB/s eta 0:00:30\n",
      "   ------------------------------ --------- 2.0/2.6 GB 21.2 MB/s eta 0:00:30\n",
      "   ------------------------------ --------- 2.0/2.6 GB 20.9 MB/s eta 0:00:30\n",
      "   ------------------------------ --------- 2.0/2.6 GB 21.0 MB/s eta 0:00:30\n",
      "   ------------------------------ --------- 2.0/2.6 GB 21.1 MB/s eta 0:00:30\n",
      "   ------------------------------ --------- 2.0/2.6 GB 21.2 MB/s eta 0:00:29\n",
      "   ------------------------------ --------- 2.0/2.6 GB 21.4 MB/s eta 0:00:29\n",
      "   ------------------------------ --------- 2.0/2.6 GB 22.2 MB/s eta 0:00:27\n",
      "   ------------------------------ --------- 2.0/2.6 GB 23.2 MB/s eta 0:00:26\n",
      "   ------------------------------ --------- 2.0/2.6 GB 23.2 MB/s eta 0:00:26\n",
      "   ------------------------------- -------- 2.0/2.6 GB 23.5 MB/s eta 0:00:25\n",
      "   ------------------------------- -------- 2.0/2.6 GB 23.6 MB/s eta 0:00:25\n",
      "   ------------------------------- -------- 2.0/2.6 GB 23.4 MB/s eta 0:00:25\n",
      "   ------------------------------- -------- 2.0/2.6 GB 23.2 MB/s eta 0:00:25\n",
      "   ------------------------------- -------- 2.0/2.6 GB 23.2 MB/s eta 0:00:25\n",
      "   ------------------------------- -------- 2.0/2.6 GB 22.6 MB/s eta 0:00:25\n",
      "   ------------------------------- -------- 2.0/2.6 GB 22.6 MB/s eta 0:00:25\n",
      "   ------------------------------- -------- 2.0/2.6 GB 22.4 MB/s eta 0:00:25\n",
      "   ------------------------------- -------- 2.0/2.6 GB 22.5 MB/s eta 0:00:25\n",
      "   ------------------------------- -------- 2.1/2.6 GB 22.5 MB/s eta 0:00:24\n",
      "   ------------------------------- -------- 2.1/2.6 GB 22.5 MB/s eta 0:00:24\n",
      "   ------------------------------- -------- 2.1/2.6 GB 22.3 MB/s eta 0:00:24\n",
      "   ------------------------------- -------- 2.1/2.6 GB 22.3 MB/s eta 0:00:24\n",
      "   -------------------------------- ------- 2.1/2.6 GB 22.4 MB/s eta 0:00:24\n",
      "   -------------------------------- ------- 2.1/2.6 GB 22.4 MB/s eta 0:00:23\n",
      "   -------------------------------- ------- 2.1/2.6 GB 22.5 MB/s eta 0:00:23\n",
      "   -------------------------------- ------- 2.1/2.6 GB 22.6 MB/s eta 0:00:23\n",
      "   -------------------------------- ------- 2.1/2.6 GB 22.5 MB/s eta 0:00:22\n",
      "   -------------------------------- ------- 2.1/2.6 GB 22.6 MB/s eta 0:00:22\n",
      "   -------------------------------- ------- 2.1/2.6 GB 22.6 MB/s eta 0:00:22\n",
      "   -------------------------------- ------- 2.1/2.6 GB 22.6 MB/s eta 0:00:22\n",
      "   -------------------------------- ------- 2.1/2.6 GB 22.6 MB/s eta 0:00:22\n",
      "   -------------------------------- ------- 2.1/2.6 GB 22.6 MB/s eta 0:00:21\n",
      "   -------------------------------- ------- 2.1/2.6 GB 22.7 MB/s eta 0:00:21\n",
      "   -------------------------------- ------- 2.1/2.6 GB 22.7 MB/s eta 0:00:21\n",
      "   --------------------------------- ------ 2.1/2.6 GB 22.6 MB/s eta 0:00:21\n",
      "   --------------------------------- ------ 2.1/2.6 GB 22.3 MB/s eta 0:00:21\n",
      "   --------------------------------- ------ 2.1/2.6 GB 22.3 MB/s eta 0:00:21\n",
      "   --------------------------------- ------ 2.1/2.6 GB 22.9 MB/s eta 0:00:20\n",
      "   --------------------------------- ------ 2.2/2.6 GB 22.6 MB/s eta 0:00:20\n",
      "   --------------------------------- ------ 2.2/2.6 GB 22.8 MB/s eta 0:00:19\n",
      "   --------------------------------- ------ 2.2/2.6 GB 23.0 MB/s eta 0:00:19\n",
      "   --------------------------------- ------ 2.2/2.6 GB 22.9 MB/s eta 0:00:19\n",
      "   --------------------------------- ------ 2.2/2.6 GB 22.6 MB/s eta 0:00:19\n",
      "   --------------------------------- ------ 2.2/2.6 GB 22.7 MB/s eta 0:00:19\n",
      "   --------------------------------- ------ 2.2/2.6 GB 22.7 MB/s eta 0:00:18\n",
      "   --------------------------------- ------ 2.2/2.6 GB 22.8 MB/s eta 0:00:18\n",
      "   --------------------------------- ------ 2.2/2.6 GB 22.4 MB/s eta 0:00:18\n",
      "   --------------------------------- ------ 2.2/2.6 GB 22.5 MB/s eta 0:00:18\n",
      "   ---------------------------------- ----- 2.2/2.6 GB 22.6 MB/s eta 0:00:18\n",
      "   ---------------------------------- ----- 2.2/2.6 GB 22.7 MB/s eta 0:00:17\n",
      "   ---------------------------------- ----- 2.2/2.6 GB 23.5 MB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 2.2/2.6 GB 23.9 MB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 2.2/2.6 GB 23.9 MB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 2.2/2.6 GB 23.9 MB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 2.2/2.6 GB 22.8 MB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 2.2/2.6 GB 23.8 MB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 2.2/2.6 GB 23.7 MB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 2.2/2.6 GB 23.6 MB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 2.3/2.6 GB 23.3 MB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 2.3/2.6 GB 22.9 MB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 2.3/2.6 GB 22.6 MB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 2.3/2.6 GB 22.4 MB/s eta 0:00:15\n",
      "   ----------------------------------- ---- 2.3/2.6 GB 22.5 MB/s eta 0:00:15\n",
      "   ----------------------------------- ---- 2.3/2.6 GB 22.1 MB/s eta 0:00:15\n",
      "   ----------------------------------- ---- 2.3/2.6 GB 22.0 MB/s eta 0:00:15\n",
      "   ----------------------------------- ---- 2.3/2.6 GB 22.1 MB/s eta 0:00:15\n",
      "   ----------------------------------- ---- 2.3/2.6 GB 22.2 MB/s eta 0:00:14\n",
      "   ----------------------------------- ---- 2.3/2.6 GB 21.9 MB/s eta 0:00:14\n",
      "   ----------------------------------- ---- 2.3/2.6 GB 22.1 MB/s eta 0:00:14\n",
      "   ----------------------------------- ---- 2.3/2.6 GB 22.6 MB/s eta 0:00:13\n",
      "   ----------------------------------- ---- 2.3/2.6 GB 22.8 MB/s eta 0:00:13\n",
      "   ----------------------------------- ---- 2.3/2.6 GB 22.9 MB/s eta 0:00:13\n",
      "   ----------------------------------- ---- 2.3/2.6 GB 22.9 MB/s eta 0:00:13\n",
      "   ----------------------------------- ---- 2.3/2.6 GB 22.2 MB/s eta 0:00:13\n",
      "   ----------------------------------- ---- 2.3/2.6 GB 21.8 MB/s eta 0:00:13\n",
      "   ----------------------------------- ---- 2.3/2.6 GB 21.7 MB/s eta 0:00:13\n",
      "   ----------------------------------- ---- 2.3/2.6 GB 21.7 MB/s eta 0:00:13\n",
      "   ------------------------------------ --- 2.3/2.6 GB 21.9 MB/s eta 0:00:12\n",
      "   ------------------------------------ --- 2.3/2.6 GB 22.4 MB/s eta 0:00:11\n",
      "   ------------------------------------ --- 2.4/2.6 GB 22.7 MB/s eta 0:00:11\n",
      "   ------------------------------------ --- 2.4/2.6 GB 22.6 MB/s eta 0:00:11\n",
      "   ------------------------------------ --- 2.4/2.6 GB 22.5 MB/s eta 0:00:11\n",
      "   ------------------------------------ --- 2.4/2.6 GB 22.4 MB/s eta 0:00:10\n",
      "   ------------------------------------ --- 2.4/2.6 GB 22.5 MB/s eta 0:00:10\n",
      "   ------------------------------------ --- 2.4/2.6 GB 22.6 MB/s eta 0:00:10\n",
      "   ------------------------------------ --- 2.4/2.6 GB 22.5 MB/s eta 0:00:10\n",
      "   ------------------------------------ --- 2.4/2.6 GB 22.4 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 2.4/2.6 GB 22.3 MB/s eta 0:00:09\n",
      "   ------------------------------------- -- 2.4/2.6 GB 22.3 MB/s eta 0:00:09\n",
      "   ------------------------------------- -- 2.4/2.6 GB 22.3 MB/s eta 0:00:09\n",
      "   ------------------------------------- -- 2.4/2.6 GB 22.6 MB/s eta 0:00:09\n",
      "   ------------------------------------- -- 2.4/2.6 GB 22.3 MB/s eta 0:00:09\n",
      "   ------------------------------------- -- 2.4/2.6 GB 22.2 MB/s eta 0:00:08\n",
      "   ------------------------------------- -- 2.4/2.6 GB 22.1 MB/s eta 0:00:08\n",
      "   ------------------------------------- -- 2.4/2.6 GB 22.0 MB/s eta 0:00:08\n",
      "   ------------------------------------- -- 2.4/2.6 GB 21.9 MB/s eta 0:00:08\n",
      "   ------------------------------------- -- 2.4/2.6 GB 21.9 MB/s eta 0:00:08\n",
      "   ------------------------------------- -- 2.4/2.6 GB 21.9 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 2.4/2.6 GB 22.3 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 2.5/2.6 GB 23.1 MB/s eta 0:00:06\n",
      "   -------------------------------------- - 2.5/2.6 GB 23.0 MB/s eta 0:00:06\n",
      "   -------------------------------------- - 2.5/2.6 GB 22.9 MB/s eta 0:00:06\n",
      "   -------------------------------------- - 2.5/2.6 GB 22.8 MB/s eta 0:00:06\n",
      "   -------------------------------------- - 2.5/2.6 GB 22.8 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 2.5/2.6 GB 22.6 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 2.5/2.6 GB 23.6 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 2.5/2.6 GB 23.2 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 2.5/2.6 GB 22.9 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 2.5/2.6 GB 23.1 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 2.5/2.6 GB 23.2 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 2.5/2.6 GB 23.3 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 2.5/2.6 GB 23.5 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.5/2.6 GB 23.7 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.5/2.6 GB 23.8 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.5/2.6 GB 24.0 MB/s eta 0:00:03\n",
      "   ---------------------------------------  2.5/2.6 GB 24.3 MB/s eta 0:00:02\n",
      "   ---------------------------------------  2.5/2.6 GB 24.5 MB/s eta 0:00:02\n",
      "   ---------------------------------------  2.6/2.6 GB 24.5 MB/s eta 0:00:02\n",
      "   ---------------------------------------  2.6/2.6 GB 24.5 MB/s eta 0:00:02\n",
      "   ---------------------------------------  2.6/2.6 GB 24.2 MB/s eta 0:00:02\n",
      "   ---------------------------------------  2.6/2.6 GB 23.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 23.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 25.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 GB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 GB 17.0 MB/s  0:01:59\n",
      "Downloading https://download.pytorch.org/whl/cu126/torchvision-0.25.0%2Bcu126-cp310-cp310-win_amd64.whl (8.2 MB)\n",
      "   ---------------------------------------- 0.0/8.2 MB ? eta -:--:--\n",
      "   -------------------------------------- - 7.9/8.2 MB 40.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.2/8.2 MB 36.2 MB/s  0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.10.0%2Bcu126-cp310-cp310-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 32.6 MB/s  0:00:00\n",
      "Downloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.8/1.7 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 4.7 MB/s  0:00:00\n",
      "Downloading pillow-12.0.0-cp310-cp310-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.0/7.0 MB 5.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.1/7.0 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.4/7.0 MB 5.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.7/7.0 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.0/7.0 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 5.5 MB/s  0:00:01\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.0/6.3 MB 6.3 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.4/6.3 MB 6.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.0/6.3 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 6.2 MB/s  0:00:01\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 8.8 MB/s  0:00:00\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/12.9 MB 6.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.9/12.9 MB 6.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.2/12.9 MB 6.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.8/12.9 MB 7.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 7.3/12.9 MB 7.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.9/12.9 MB 7.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.5/12.9 MB 7.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.3/12.9 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 7.4 MB/s  0:00:01\n",
      "Installing collected packages: mpmath, sympy, pillow, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision, torchaudio\n",
      "\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   ----------------------------------------  0/12 [mpmath]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   --- ------------------------------------  1/12 [sympy]\n",
      "   ------ ---------------------------------  2/12 [pillow]\n",
      "   ------ ---------------------------------  2/12 [pillow]\n",
      "   ------ ---------------------------------  2/12 [pillow]\n",
      "   ------ ---------------------------------  2/12 [pillow]\n",
      "   ------ ---------------------------------  2/12 [pillow]\n",
      "   ------ ---------------------------------  2/12 [pillow]\n",
      "   ------ ---------------------------------  2/12 [pillow]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ---------- -----------------------------  3/12 [numpy]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   ------------- --------------------------  4/12 [networkx]\n",
      "   -------------------- -------------------  6/12 [fsspec]\n",
      "   -------------------- -------------------  6/12 [fsspec]\n",
      "   -------------------- -------------------  6/12 [fsspec]\n",
      "   -------------------- -------------------  6/12 [fsspec]\n",
      "   ----------------------- ----------------  7/12 [filelock]\n",
      "   -------------------------- -------------  8/12 [jinja2]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   ------------------------------ ---------  9/12 [torch]\n",
      "   --------------------------------- ------ 10/12 [torchvision]\n",
      "   --------------------------------- ------ 10/12 [torchvision]\n",
      "   --------------------------------- ------ 10/12 [torchvision]\n",
      "   --------------------------------- ------ 10/12 [torchvision]\n",
      "   --------------------------------- ------ 10/12 [torchvision]\n",
      "   --------------------------------- ------ 10/12 [torchvision]\n",
      "   --------------------------------- ------ 10/12 [torchvision]\n",
      "   --------------------------------- ------ 10/12 [torchvision]\n",
      "   --------------------------------- ------ 10/12 [torchvision]\n",
      "   --------------------------------- ------ 10/12 [torchvision]\n",
      "   --------------------------------- ------ 10/12 [torchvision]\n",
      "   --------------------------------- ------ 10/12 [torchvision]\n",
      "   --------------------------------- ------ 10/12 [torchvision]\n",
      "   ------------------------------------ --- 11/12 [torchaudio]\n",
      "   ------------------------------------ --- 11/12 [torchaudio]\n",
      "   ------------------------------------ --- 11/12 [torchaudio]\n",
      "   ------------------------------------ --- 11/12 [torchaudio]\n",
      "   ------------------------------------ --- 11/12 [torchaudio]\n",
      "   ---------------------------------------- 12/12 [torchaudio]\n",
      "\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.20.0 fsspec-2025.12.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.6 pillow-12.0.0 sympy-1.14.0 torch-2.10.0+cu126 torchaudio-2.10.0+cu126 torchvision-0.25.0+cu126\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52a25249-be64-44ef-835b-d17ea9b6b508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\anaconda\\envs\\lidar\\lib\\site-packages (2.2.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10f4cd4f-4675-4aee-9d7e-4ae460c87e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: laspy in d:\\anaconda\\envs\\lidar\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from laspy) (2.2.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install laspy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80bd63f3-8c47-4472-bd47-9f7f3d83cd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lazrs in d:\\anaconda\\envs\\lidar\\lib\\site-packages (0.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lazrs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3df1f30b-71f3-4a24-a98e-f6d69490d49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in d:\\anaconda\\envs\\lidar\\lib\\site-packages (4.67.3)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1849f00-ae05-43f2-993c-16d48cdf3b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in d:\\anaconda\\envs\\lidar\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\anaconda\\envs\\lidar\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8335535e-cf35-4e0e-a8cf-8d9b287a0e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.10.0+cu126\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd8cd650-9cc8-421d-83db-ad4efd73d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELED_DIR = r\"D:/lidarrrrr/anbu/training_labeled\"      # your 9 labeled LAZ files\n",
    "OUT_DIR     = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"    # blocks will be saved here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f625b06-e157-4aa8-af8f-84a47a387121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled files: 10\n",
      "DX3011148 ULMIANO000001.laz blocks: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 103/103 [00:00<00:00, 153.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DX3011148 ULMIANO000002.laz blocks: 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 112/112 [00:00<00:00, 152.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DX3011148 ULMIANO000003.laz blocks: 399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 399/399 [00:02<00:00, 148.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DX3011148 ULMIANO000004.laz blocks: 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 82/82 [00:00<00:00, 145.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DX3011148 ULMIANO000005.laz blocks: 695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 695/695 [00:05<00:00, 137.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DX3011148 ULMIANO000006.laz blocks: 448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 448/448 [00:02<00:00, 161.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DX3011148 ULMIANO000007.laz blocks: 348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 348/348 [00:02<00:00, 159.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DX3011148 ULMIANO000008.laz blocks: 508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 508/508 [00:03<00:00, 161.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DX3011148 ULMIANO000009.laz blocks: 230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 230/230 [00:01<00:00, 158.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt013390.laz blocks: 618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 618/618 [00:07<00:00, 81.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total blocks saved: 3543\n",
      "Blocks folder: D:/lidarrrrr/anbu/dl_dataset/blocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import laspy\n",
    "from tqdm import tqdm\n",
    "\n",
    "LABELED_DIR = r\"D:/lidarrrrr/anbu/training_labeled\"\n",
    "OUT_DIR     = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "BLOCK_SIZE = 30.0\n",
    "STRIDE     = 20.0\n",
    "N_POINTS   = 4096          # RTX 3050 safer than 8192\n",
    "MIN_POINTS = 1500\n",
    "\n",
    "VALID_CLASSES = set([1,2,3,6,7,9,10,12,13])\n",
    "\n",
    "def get_dim(las, name, fallback=0.0):\n",
    "    try:\n",
    "        return np.asarray(las[name])\n",
    "    except Exception:\n",
    "        return np.full(len(las.x), fallback, dtype=np.float32)\n",
    "\n",
    "def build_hag_from_ground(x, y, z, cls, ground_class=2, cell=3.0):\n",
    "    minx, miny = x.min(), y.min()\n",
    "    gx = np.floor((x - minx)/cell).astype(np.int32)\n",
    "    gy = np.floor((y - miny)/cell).astype(np.int32)\n",
    "    cell_min = {}\n",
    "    g_idx = np.where(cls == ground_class)[0]\n",
    "    for i in g_idx:\n",
    "        k = (int(gx[i]), int(gy[i]))\n",
    "        zi = float(z[i])\n",
    "        if (k not in cell_min) or (zi < cell_min[k]):\n",
    "            cell_min[k] = zi\n",
    "\n",
    "    hag = np.zeros(len(z), dtype=np.float32)\n",
    "    has_ground = np.zeros(len(z), dtype=bool)\n",
    "    for i in range(len(z)):\n",
    "        k = (int(gx[i]), int(gy[i]))\n",
    "        if k in cell_min:\n",
    "            hag[i] = float(z[i] - cell_min[k])\n",
    "            has_ground[i] = True\n",
    "    return hag, has_ground\n",
    "\n",
    "def split_into_blocks(x, y, idx_all, block_size, stride):\n",
    "    xmin, xmax = x.min(), x.max()\n",
    "    ymin, ymax = y.min(), y.max()\n",
    "    xs = np.arange(xmin, xmax, stride)\n",
    "    ys = np.arange(ymin, ymax, stride)\n",
    "    blocks = []\n",
    "    for x0 in xs:\n",
    "        for y0 in ys:\n",
    "            x1 = x0 + block_size\n",
    "            y1 = y0 + block_size\n",
    "            m = (x >= x0) & (x < x1) & (y >= y0) & (y < y1)\n",
    "            ids = idx_all[m]\n",
    "            if len(ids) >= MIN_POINTS:\n",
    "                blocks.append(ids)\n",
    "    return blocks\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(LABELED_DIR, \"*.la*\")))\n",
    "print(\"Labeled files:\", len(files))\n",
    "\n",
    "block_id = 0\n",
    "for fp in files:\n",
    "    las = laspy.read(fp)\n",
    "    x = np.asarray(las.x, dtype=np.float32)\n",
    "    y = np.asarray(las.y, dtype=np.float32)\n",
    "    z = np.asarray(las.z, dtype=np.float32)\n",
    "    cls = np.asarray(las.classification, dtype=np.int32)\n",
    "\n",
    "    keep = np.array([c in VALID_CLASSES for c in cls], dtype=bool)\n",
    "    idx_all = np.where(keep)[0]\n",
    "\n",
    "    intensity = get_dim(las, \"intensity\", 0.0).astype(np.float32)\n",
    "    rn = get_dim(las, \"return_number\", 1.0).astype(np.float32)\n",
    "    nr = get_dim(las, \"number_of_returns\", 1.0).astype(np.float32)\n",
    "\n",
    "    # RGB optional\n",
    "    red   = get_dim(las, \"red\", 0.0).astype(np.float32)\n",
    "    green = get_dim(las, \"green\", 0.0).astype(np.float32)\n",
    "    blue  = get_dim(las, \"blue\", 0.0).astype(np.float32)\n",
    "\n",
    "    hag, has_ground = build_hag_from_ground(x, y, z, cls, ground_class=2, cell=3.0)\n",
    "    idx_all = idx_all[has_ground[idx_all]]\n",
    "\n",
    "    if len(idx_all) < MIN_POINTS:\n",
    "        print(\"Skip (few points):\", os.path.basename(fp))\n",
    "        continue\n",
    "\n",
    "    blocks = split_into_blocks(x[idx_all], y[idx_all], idx_all, BLOCK_SIZE, STRIDE)\n",
    "    print(os.path.basename(fp), \"blocks:\", len(blocks))\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "    for ids in tqdm(blocks):\n",
    "        if len(ids) >= N_POINTS:\n",
    "            choose = rng.choice(ids, N_POINTS, replace=False)\n",
    "        else:\n",
    "            choose = rng.choice(ids, N_POINTS, replace=True)\n",
    "\n",
    "        px, py, pz = x[choose], y[choose], z[choose]\n",
    "        ph = hag[choose]\n",
    "\n",
    "        # normalize xyz per block\n",
    "        cx, cy, cz = px.mean(), py.mean(), pz.mean()\n",
    "        pxn, pyn, pzn = px - cx, py - cy, pz - cz\n",
    "\n",
    "        X = np.stack([\n",
    "            pxn, pyn, pzn,\n",
    "            ph,\n",
    "            intensity[choose],\n",
    "            rn[choose],\n",
    "            nr[choose],\n",
    "            red[choose], green[choose], blue[choose],\n",
    "        ], axis=1).astype(np.float32)\n",
    "\n",
    "        yb = cls[choose].astype(np.int64)\n",
    "\n",
    "        out_path = os.path.join(OUT_DIR, f\"block_{block_id:07d}.npz\")\n",
    "        np.savez_compressed(out_path, X=X, y=yb)\n",
    "        block_id += 1\n",
    "\n",
    "print(\"Total blocks saved:\", block_id)\n",
    "print(\"Blocks folder:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0356889e-62e9-43e8-a6c3-d70682129238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu | GPU: None\n",
      "Blocks: 3543 | Train: 3012 | Val: 531\n",
      "Computing class weights (quick pass on train set)...\n",
      "Class counts (sample): {1: 965848, 2: 849365, 3: 131950, 6: 67922, 7: 309, 9: 1, 10: 1, 12: 32246, 13: 360}\n",
      "Weights: {1: 0.004311848431825638, 2: 0.0045980182476341724, 3: 0.011665763333439827, 6: 0.016259703785181046, 7: 0.24106748402118683, 9: 4.237579822540283, 10: 4.237579822540283, 12: 0.023598257452249527, 13: 0.22334004938602448}\n",
      "Input features per point: 10 | Num classes: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elegance geo tech\\AppData\\Local\\Temp\\ipykernel_14144\\2728789207.py:195: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
      "Epoch 1/25 [train]:   0%|          | 0/502 [00:00<?, ?it/s]D:\\anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1118: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DL TRAINING: Multi-class Point Cloud Classification (PointNet-Seg style)\n",
    "# - Input: .npz blocks with X (N, F) and y (N,)\n",
    "# - Output: saved model .pt + class mapping\n",
    "# - GPU: RTX 3050 supported (uses AMP for speed)\n",
    "# ============================================================\n",
    "\n",
    "import os, glob, json, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# EDIT THESE PATHS\n",
    "# -----------------------------\n",
    "BLOCKS_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "OUT_DIR    = r\"D:/lidarrrrr/anbu/dl_models\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = os.path.join(OUT_DIR, \"pointnet_seg_best.pt\")\n",
    "MAP_PATH   = os.path.join(OUT_DIR, \"class_mapping.json\")\n",
    "\n",
    "# -----------------------------\n",
    "# YOUR VALID CLASSES (scope)\n",
    "# -----------------------------\n",
    "VALID_CLASSES = [1, 2, 3, 6, 7, 9, 10, 12, 13]  # labels in your LAZ\n",
    "# we remap them to 0..C-1 for training\n",
    "to_idx = {c:i for i,c in enumerate(VALID_CLASSES)}\n",
    "to_lbl = {i:c for i,c in enumerate(VALID_CLASSES)}\n",
    "NUM_CLASSES = len(VALID_CLASSES)\n",
    "\n",
    "# -----------------------------\n",
    "# TRAINING SETTINGS (RTX 3050)\n",
    "# -----------------------------\n",
    "EPOCHS      = 25\n",
    "BATCH_SIZE  = 6        # safe for RTX 3050 (try 8 if you have enough VRAM)\n",
    "LR          = 1e-3\n",
    "NUM_WORKERS = 2        # increase to 4 if stable\n",
    "PIN_MEMORY  = True\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE, \"| GPU:\", torch.cuda.get_device_name(0) if DEVICE==\"cuda\" else \"None\")\n",
    "\n",
    "# -----------------------------\n",
    "# DATASET\n",
    "# -----------------------------\n",
    "class NPZBlocks(Dataset):\n",
    "    def __init__(self, files, to_idx):\n",
    "        self.files = files\n",
    "        self.to_idx = to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        d = np.load(self.files[i])\n",
    "        X = d[\"X\"].astype(np.float32)          # (N, F)\n",
    "        y = d[\"y\"].astype(np.int32)            # (N,)\n",
    "\n",
    "        # map labels -> 0..C-1\n",
    "        y2 = np.full_like(y, 0, dtype=np.int64)\n",
    "        for k, v in self.to_idx.items():\n",
    "            y2[y == k] = v\n",
    "\n",
    "        return torch.from_numpy(X), torch.from_numpy(y2)\n",
    "\n",
    "# -----------------------------\n",
    "# SIMPLE PointNet-Seg MODEL\n",
    "# (Per-point MLP + Global Feature + Per-point classifier)\n",
    "# -----------------------------\n",
    "class PointNetSeg(nn.Module):\n",
    "    def __init__(self, in_ch, num_classes):\n",
    "        super().__init__()\n",
    "        # per-point feature extractor\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(in_ch, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 256), nn.ReLU(),\n",
    "        )\n",
    "        # global feature\n",
    "        self.global_fc = nn.Sequential(\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # per-point classifier (concat local + global)\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(256 + 256, 256), nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, F)\n",
    "        B, N, Fdim = x.shape\n",
    "        feat = self.mlp1(x)             # (B, N, 256)\n",
    "        g = feat.max(dim=1).values      # (B, 256) global maxpool\n",
    "        g = self.global_fc(g)           # (B, 256)\n",
    "        g_rep = g.unsqueeze(1).repeat(1, N, 1)  # (B, N, 256)\n",
    "        z = torch.cat([feat, g_rep], dim=2)     # (B, N, 512)\n",
    "        out = self.mlp2(z)              # (B, N, C)\n",
    "        return out\n",
    "\n",
    "# -----------------------------\n",
    "# METRICS\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def confusion_matrix(pred, targ, num_classes):\n",
    "    # pred,targ: (M,) int\n",
    "    cm = torch.zeros((num_classes, num_classes), dtype=torch.int64, device=pred.device)\n",
    "    k = (targ >= 0) & (targ < num_classes)\n",
    "    inds = num_classes * targ[k] + pred[k]\n",
    "    cm.view(-1).scatter_add_(0, inds, torch.ones_like(inds, dtype=torch.int64))\n",
    "    return cm\n",
    "\n",
    "def compute_iou_f1(cm):\n",
    "    # cm: (C,C) on CPU\n",
    "    C = cm.shape[0]\n",
    "    iou = np.zeros(C, dtype=np.float64)\n",
    "    f1  = np.zeros(C, dtype=np.float64)\n",
    "    for c in range(C):\n",
    "        tp = cm[c,c]\n",
    "        fp = cm[:,c].sum() - tp\n",
    "        fn = cm[c,:].sum() - tp\n",
    "        denom_iou = tp + fp + fn\n",
    "        denom_f1  = 2*tp + fp + fn\n",
    "        iou[c] = (tp / denom_iou) if denom_iou > 0 else 0.0\n",
    "        f1[c]  = (2*tp / denom_f1) if denom_f1 > 0 else 0.0\n",
    "    return iou, f1\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD FILES + SPLIT\n",
    "# -----------------------------\n",
    "files = sorted(glob.glob(os.path.join(BLOCKS_DIR, \"*.npz\")))\n",
    "if not files:\n",
    "    raise RuntimeError(\"No .npz blocks found in: \" + BLOCKS_DIR)\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(files)\n",
    "\n",
    "n_total = len(files)\n",
    "n_val = int(0.15 * n_total)   # 15% val\n",
    "val_files = files[:n_val]\n",
    "train_files = files[n_val:]\n",
    "\n",
    "print(\"Blocks:\", n_total, \"| Train:\", len(train_files), \"| Val:\", len(val_files))\n",
    "\n",
    "train_ds = NPZBlocks(train_files, to_idx)\n",
    "val_ds   = NPZBlocks(val_files, to_idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=False)\n",
    "\n",
    "# -----------------------------\n",
    "# CLASS WEIGHTS (handles imbalance)\n",
    "# -----------------------------\n",
    "print(\"Computing class weights (quick pass on train set)...\")\n",
    "counts = np.zeros(NUM_CLASSES, dtype=np.int64)\n",
    "\n",
    "# sample up to 500 blocks to estimate (fast)\n",
    "sample_files = train_files[:min(500, len(train_files))]\n",
    "for fp in sample_files:\n",
    "    d = np.load(fp)\n",
    "    y = d[\"y\"].astype(np.int32)\n",
    "    for k, v in to_idx.items():\n",
    "        counts[v] += int((y == k).sum())\n",
    "\n",
    "counts = np.maximum(counts, 1)\n",
    "freq = counts / counts.sum()\n",
    "weights = (1.0 / np.sqrt(freq)).astype(np.float32)  # stable weighting\n",
    "weights = weights / weights.mean()\n",
    "\n",
    "print(\"Class counts (sample):\", dict(zip(VALID_CLASSES, counts.tolist())))\n",
    "print(\"Weights:\", dict(zip(VALID_CLASSES, weights.tolist())))\n",
    "\n",
    "class_weights = torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "# -----------------------------\n",
    "# MODEL / OPTIM\n",
    "# -----------------------------\n",
    "# infer feature count from one block\n",
    "tmp = np.load(train_files[0])\n",
    "IN_CH = tmp[\"X\"].shape[1]\n",
    "print(\"Input features per point:\", IN_CH, \"| Num classes:\", NUM_CLASSES)\n",
    "\n",
    "model = PointNetSeg(in_ch=IN_CH, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "best_miou = -1.0\n",
    "\n",
    "# -----------------------------\n",
    "# TRAIN LOOP\n",
    "# -----------------------------\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for X, y in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} [train]\"):\n",
    "        X = X.to(DEVICE, non_blocking=True)   # (B,N,F)\n",
    "        y = y.to(DEVICE, non_blocking=True)   # (B,N)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
    "            logits = model(X)                 # (B,N,C)\n",
    "            loss = F.cross_entropy(logits.reshape(-1, NUM_CLASSES),\n",
    "                                   y.reshape(-1),\n",
    "                                   weight=class_weights)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / max(len(train_loader), 1)\n",
    "\n",
    "    # -----------------------------\n",
    "    # VALIDATION\n",
    "    # -----------------------------\n",
    "    model.eval()\n",
    "    cm_all = torch.zeros((NUM_CLASSES, NUM_CLASSES), dtype=torch.int64, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(val_loader, desc=f\"Epoch {epoch}/{EPOCHS} [val]\"):\n",
    "            X = X.to(DEVICE, non_blocking=True)\n",
    "            y = y.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            logits = model(X)\n",
    "            pred = logits.argmax(dim=2)        # (B,N)\n",
    "\n",
    "            cm_all += confusion_matrix(pred.reshape(-1), y.reshape(-1), NUM_CLASSES)\n",
    "\n",
    "    cm_cpu = cm_all.detach().cpu().numpy()\n",
    "    iou, f1 = compute_iou_f1(cm_cpu)\n",
    "    miou = float(iou.mean())\n",
    "    mf1  = float(f1.mean())\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}: loss={avg_loss:.4f} | mIoU={miou:.4f} | mF1={mf1:.4f}\")\n",
    "\n",
    "    # print per-class (short)\n",
    "    for i, c in enumerate(VALID_CLASSES):\n",
    "        print(f\"  class {c:>2} | IoU={iou[i]:.3f} | F1={f1[i]:.3f}\")\n",
    "\n",
    "    # save best\n",
    "    if miou > best_miou:\n",
    "        best_miou = miou\n",
    "        torch.save({\"model\": model.state_dict(),\n",
    "                    \"in_ch\": IN_CH,\n",
    "                    \"num_classes\": NUM_CLASSES,\n",
    "                    \"valid_classes\": VALID_CLASSES}, MODEL_PATH)\n",
    "        with open(MAP_PATH, \"w\") as f:\n",
    "            json.dump({\"VALID_CLASSES\": VALID_CLASSES, \"to_idx\": to_idx, \"to_lbl\": to_lbl}, f, indent=2)\n",
    "        print(\"\\n Saved BEST model:\", MODEL_PATH)\n",
    "\n",
    "print(\"\\nDONE. Best mIoU:\", best_miou)\n",
    "print(\"Model:\", MODEL_PATH)\n",
    "print(\"Mapping:\", MAP_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47334b33-f4e6-424a-9dc7-3d064b6bfc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All classes present: [0, 1, 2, 3, 4, 5, 6, 7, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 69]\n",
      "0 318\n",
      "1 51068845\n",
      "2 50569916\n",
      "3 5461257\n",
      "4 11553105\n",
      "5 9630910\n",
      "6 3856499\n",
      "7 4521\n",
      "11 260336\n",
      "12 2446246\n",
      "13 54233\n",
      "14 213769\n",
      "15 1708\n",
      "16 132851\n",
      "17 15704\n",
      "18 8188\n",
      "19 17581\n",
      "20 15002\n",
      "21 23851\n",
      "22 19039\n",
      "69 3468\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import laspy\n",
    "\n",
    "LABELED_DIR = r\"D:/lidarrrrr/anbu/training_labeled\"\n",
    "files = sorted(glob.glob(LABELED_DIR + \"/*.la*\"))\n",
    "\n",
    "all_u = set()\n",
    "counts = {}\n",
    "for fp in files:\n",
    "    las = laspy.read(fp)\n",
    "    cls = np.asarray(las.classification, dtype=np.int32)\n",
    "    u,c = np.unique(cls, return_counts=True)\n",
    "    for k,v in zip(u,c):\n",
    "        all_u.add(int(k))\n",
    "        counts[int(k)] = counts.get(int(k),0) + int(v)\n",
    "\n",
    "print(\"All classes present:\", sorted(all_u))\n",
    "for k in sorted(counts):\n",
    "    print(k, counts[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2473f86-b0d4-4bdd-8f23-acdcac2ac7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting laspy\n",
      "  Using cached laspy-2.7.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting lazrs\n",
      "  Downloading lazrs-0.8.1-cp313-cp313-win_amd64.whl.metadata (75 bytes)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from laspy) (2.3.5)\n",
      "Using cached laspy-2.7.0-py3-none-any.whl (86 kB)\n",
      "Downloading lazrs-0.8.1-cp313-cp313-win_amd64.whl (425 kB)\n",
      "Installing collected packages: lazrs, laspy\n",
      "\n",
      "   -------------------- ------------------- 1/2 [laspy]\n",
      "   ---------------------------------------- 2/2 [laspy]\n",
      "\n",
      "Successfully installed laspy-2.7.0 lazrs-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install laspy lazrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc83059e-c7f9-44f4-9694-23d54d86beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "BLOCKS_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "OUT_DIR    = r\"D:/lidarrrrr/anbu/dl_models\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = os.path.join(OUT_DIR, \"pointnet_seg_best.pt\")\n",
    "MAP_PATH   = os.path.join(OUT_DIR, \"class_mapping.json\")\n",
    "\n",
    "#  Only classes that exist in your labeled data\n",
    "VALID_CLASSES = [1, 2, 3, 6, 7, 12, 13]\n",
    "to_idx = {c:i for i,c in enumerate(VALID_CLASSES)}\n",
    "to_lbl = {i:c for i,c in enumerate(VALID_CLASSES)}\n",
    "NUM_CLASSES = len(VALID_CLASSES)\n",
    "\n",
    "EPOCHS      = 25\n",
    "BATCH_SIZE  = 6\n",
    "LR          = 1e-3\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY  = True\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE, \"| GPU:\", torch.cuda.get_device_name(0) if DEVICE==\"cuda\" else \"None\")\n",
    "\n",
    "# FAST START (reduce training set)\n",
    "train_files = train_files[:1200]   # try 800 / 1200\n",
    "val_files   = val_files[:200]\n",
    "\n",
    "class NPZBlocks(Dataset):\n",
    "    def __init__(self, files, to_idx):\n",
    "        self.files = files\n",
    "        self.to_idx = to_idx\n",
    "\n",
    "    def __len__(self): return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        d = np.load(self.files[i])\n",
    "        X = d[\"X\"].astype(np.float32)\n",
    "        y = d[\"y\"].astype(np.int32)\n",
    "\n",
    "        # map to 0..C-1 (drop anything not in VALID_CLASSES)\n",
    "        keep = np.zeros_like(y, dtype=bool)\n",
    "        for c in self.to_idx.keys():\n",
    "            keep |= (y == c)\n",
    "        X = X[keep]\n",
    "        y = y[keep]\n",
    "\n",
    "        y2 = np.zeros_like(y, dtype=np.int64)\n",
    "        for c, j in self.to_idx.items():\n",
    "            y2[y == c] = j\n",
    "\n",
    "        return torch.from_numpy(X), torch.from_numpy(y2)\n",
    "\n",
    "def collate_pad(batch):\n",
    "    # all blocks are fixed N points already, but after filtering keep we may have fewer\n",
    "    # so we pad back to max in batch\n",
    "    xs, ys = zip(*batch)\n",
    "    maxn = max(x.shape[0] for x in xs)\n",
    "    Fdim = xs[0].shape[1]\n",
    "\n",
    "    Xb = torch.zeros((len(xs), maxn, Fdim), dtype=torch.float32)\n",
    "    yb = torch.zeros((len(xs), maxn), dtype=torch.long)\n",
    "\n",
    "    for i,(x,y) in enumerate(zip(xs,ys)):\n",
    "        n = x.shape[0]\n",
    "        Xb[i,:n] = x\n",
    "        yb[i,:n] = y\n",
    "        if n < maxn:\n",
    "            # pad labels with background = Default (class 1 -> mapped index)\n",
    "            yb[i,n:] = to_idx[1]\n",
    "    return Xb, yb\n",
    "\n",
    "class PointNetSeg(nn.Module):\n",
    "    def __init__(self, in_ch, num_classes):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(in_ch, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 256), nn.ReLU(),\n",
    "        )\n",
    "        self.global_fc = nn.Sequential(\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(512, 256), nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.mlp1(x)               # (B,N,256)\n",
    "        g = feat.max(dim=1).values        # (B,256)\n",
    "        g = self.global_fc(g)             # (B,256)\n",
    "        g = g.unsqueeze(1).expand(-1, feat.shape[1], -1)\n",
    "        z = torch.cat([feat, g], dim=2)   # (B,N,512)\n",
    "        return self.mlp2(z)               # (B,N,C)\n",
    "\n",
    "@torch.no_grad()\n",
    "def confusion_matrix(pred, targ, C):\n",
    "    cm = torch.zeros((C,C), dtype=torch.int64, device=pred.device)\n",
    "    k = (targ >= 0) & (targ < C)\n",
    "    inds = C*targ[k] + pred[k]\n",
    "    cm.view(-1).scatter_add_(0, inds, torch.ones_like(inds, dtype=torch.int64))\n",
    "    return cm\n",
    "\n",
    "def compute_iou_f1(cm):\n",
    "    C = cm.shape[0]\n",
    "    iou = np.zeros(C)\n",
    "    f1  = np.zeros(C)\n",
    "    for c in range(C):\n",
    "        tp = cm[c,c]\n",
    "        fp = cm[:,c].sum() - tp\n",
    "        fn = cm[c,:].sum() - tp\n",
    "        denom_iou = tp + fp + fn\n",
    "        denom_f1  = 2*tp + fp + fn\n",
    "        iou[c] = (tp/denom_iou) if denom_iou>0 else 0\n",
    "        f1[c]  = (2*tp/denom_f1) if denom_f1>0 else 0\n",
    "    return iou, f1\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(BLOCKS_DIR, \"*.npz\")))\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(files)\n",
    "\n",
    "n_val = int(0.15*len(files))\n",
    "val_files = files[:n_val]\n",
    "train_files = files[n_val:]\n",
    "\n",
    "print(\"Blocks:\", len(files), \"| Train:\", len(train_files), \"| Val:\", len(val_files))\n",
    "\n",
    "# ---- build per-block weights to oversample rare class-7 blocks\n",
    "print(\"Building block sampler (boost class 7 blocks)...\")\n",
    "block_w = np.ones(len(train_files), dtype=np.float32)\n",
    "\n",
    "for i,fp in enumerate(tqdm(train_files[:2000])):  # scan first 2000 blocks (fast enough)\n",
    "    y = np.load(fp)[\"y\"].astype(np.int32)\n",
    "    if np.any(y == 7):\n",
    "        block_w[i] *= 6.0     # oversample blocks containing outliers\n",
    "    if np.any(y == 13):\n",
    "        block_w[i] *= 2.0     # mild boost for class 13\n",
    "\n",
    "sampler = WeightedRandomSampler(weights=block_w, num_samples=len(train_files), replacement=True)\n",
    "\n",
    "train_ds = NPZBlocks(train_files, to_idx)\n",
    "val_ds   = NPZBlocks(val_files, to_idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "                          collate_fn=collate_pad)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "                          collate_fn=collate_pad)\n",
    "\n",
    "# ---- class weights from sampled blocks, clipped to avoid crazy values\n",
    "counts = np.zeros(NUM_CLASSES, dtype=np.int64)\n",
    "sample_files = train_files[:min(600, len(train_files))]\n",
    "for fp in sample_files:\n",
    "    y = np.load(fp)[\"y\"].astype(np.int32)\n",
    "    for c,j in to_idx.items():\n",
    "        counts[j] += int((y == c).sum())\n",
    "\n",
    "counts = np.maximum(counts, 1)\n",
    "freq = counts / counts.sum()\n",
    "w = (1.0 / np.sqrt(freq)).astype(np.float32)\n",
    "w = np.clip(w / w.mean(), 0.5, 8.0)   #  clip\n",
    "print(\"Counts(sample):\", dict(zip(VALID_CLASSES, counts.tolist())))\n",
    "print(\"Weights:\", dict(zip(VALID_CLASSES, w.tolist())))\n",
    "\n",
    "class_weights = torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "tmp = np.load(train_files[0])\n",
    "IN_CH = tmp[\"X\"].shape[1]\n",
    "print(\"Input features:\", IN_CH, \"| Classes:\", NUM_CLASSES)\n",
    "\n",
    "model = PointNetSeg(IN_CH, NUM_CLASSES).to(DEVICE)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "best_miou = -1.0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    loss_sum = 0.0\n",
    "\n",
    "    for X,y in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} [train]\"):\n",
    "        X = X.to(DEVICE, non_blocking=True)\n",
    "        y = y.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with torch.autocast(device_type=\"cuda\", enabled=(DEVICE==\"cuda\")):\n",
    "            logits = model(X)  # (B,N,C)\n",
    "            loss = F.cross_entropy(logits.reshape(-1, NUM_CLASSES),\n",
    "                                   y.reshape(-1),\n",
    "                                   weight=class_weights)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        loss_sum += float(loss.item())\n",
    "\n",
    "    sch.step()\n",
    "    avg_loss = loss_sum / max(len(train_loader),1)\n",
    "\n",
    "    # val\n",
    "    model.eval()\n",
    "    cm = torch.zeros((NUM_CLASSES, NUM_CLASSES), dtype=torch.int64, device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        for X,y in tqdm(val_loader, desc=f\"Epoch {epoch}/{EPOCHS} [val]\"):\n",
    "            X = X.to(DEVICE, non_blocking=True)\n",
    "            y = y.to(DEVICE, non_blocking=True)\n",
    "            logits = model(X)\n",
    "            pred = logits.argmax(dim=2)\n",
    "            cm += confusion_matrix(pred.reshape(-1), y.reshape(-1), NUM_CLASSES)\n",
    "\n",
    "    cm_cpu = cm.cpu().numpy()\n",
    "    iou, f1 = compute_iou_f1(cm_cpu)\n",
    "    miou = float(iou.mean())\n",
    "    mf1  = float(f1.mean())\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}: loss={avg_loss:.4f} | mIoU={miou:.4f} | mF1={mf1:.4f}\")\n",
    "    for i,c in enumerate(VALID_CLASSES):\n",
    "        print(f\"  class {c:>2} | IoU={iou[i]:.3f} | F1={f1[i]:.3f}\")\n",
    "\n",
    "    if miou > best_miou:\n",
    "        best_miou = miou\n",
    "        torch.save({\"model\": model.state_dict(),\n",
    "                    \"in_ch\": IN_CH,\n",
    "                    \"valid_classes\": VALID_CLASSES}, MODEL_PATH)\n",
    "        with open(MAP_PATH,\"w\") as f:\n",
    "            json.dump({\"VALID_CLASSES\": VALID_CLASSES, \"to_idx\": to_idx, \"to_lbl\": to_lbl}, f, indent=2)\n",
    "        print(\"\\n Saved BEST model:\", MODEL_PATH)\n",
    "\n",
    "print(\"\\nDONE. Best mIoU:\", best_miou)\n",
    "print(\"Model:\", MODEL_PATH)\n",
    "print(\"Mapping:\", MAP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff2652-6667-439f-af94-91411fbef95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# PATHS\n",
    "# -----------------------------\n",
    "BLOCKS_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "OUT_DIR    = r\"D:/lidarrrrr/anbu/dl_models\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = os.path.join(OUT_DIR, \"pointnet_seg_best.pt\")\n",
    "MAP_PATH   = os.path.join(OUT_DIR, \"class_mapping.json\")\n",
    "\n",
    "# -----------------------------\n",
    "# CLASSES (only ones in data)\n",
    "# -----------------------------\n",
    "VALID_CLASSES = [1,2,3,6,7,12,13]\n",
    "to_idx = {c:i for i,c in enumerate(VALID_CLASSES)}\n",
    "to_lbl = {i:c for i,c in enumerate(VALID_CLASSES)}\n",
    "NUM_CLASSES = len(VALID_CLASSES)\n",
    "\n",
    "# -----------------------------\n",
    "# TRAIN SETTINGS (FAST)\n",
    "# -----------------------------\n",
    "EPOCHS      = 20\n",
    "BATCH_SIZE  = 6\n",
    "LR          = 1e-3\n",
    "NUM_WORKERS = 6\n",
    "PIN_MEMORY  = True\n",
    "MAX_POINTS  = 2048    # downsample per block (faster)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE, \"| GPU:\", torch.cuda.get_device_name(0) if DEVICE==\"cuda\" else \"None\")\n",
    "\n",
    "# -----------------------------\n",
    "# DATASET\n",
    "# -----------------------------\n",
    "class NPZBlocks(Dataset):\n",
    "    def __init__(self, files, to_idx):\n",
    "        self.files = files\n",
    "        self.to_idx = to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        d = np.load(self.files[i])\n",
    "        X = d[\"X\"].astype(np.float32)\n",
    "        y = d[\"y\"].astype(np.int32)\n",
    "\n",
    "        # keep only valid classes\n",
    "        mask = np.isin(y, list(self.to_idx.keys()))\n",
    "        X = X[mask]\n",
    "        y = y[mask]\n",
    "\n",
    "        # downsample points\n",
    "        if len(X) > MAX_POINTS:\n",
    "            idx = np.random.choice(len(X), MAX_POINTS, replace=False)\n",
    "            X = X[idx]\n",
    "            y = y[idx]\n",
    "\n",
    "        # map labels\n",
    "        y2 = np.zeros_like(y, dtype=np.int64)\n",
    "        for c, j in self.to_idx.items():\n",
    "            y2[y == c] = j\n",
    "\n",
    "        return torch.from_numpy(X), torch.from_numpy(y2)\n",
    "\n",
    "def collate_pad(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    maxn = max(x.shape[0] for x in xs)\n",
    "    Fdim = xs[0].shape[1]\n",
    "\n",
    "    Xb = torch.zeros((len(xs), maxn, Fdim), dtype=torch.float32)\n",
    "    yb = torch.zeros((len(xs), maxn), dtype=torch.long)\n",
    "\n",
    "    for i,(x,y) in enumerate(zip(xs,ys)):\n",
    "        n = x.shape[0]\n",
    "        Xb[i,:n] = x\n",
    "        yb[i,:n] = y\n",
    "\n",
    "    return Xb, yb\n",
    "\n",
    "# -----------------------------\n",
    "# MODEL\n",
    "# -----------------------------\n",
    "class PointNetSeg(nn.Module):\n",
    "    def __init__(self, in_ch, num_classes):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(in_ch, 64), nn.ReLU(),\n",
    "            nn.Linear(64,128), nn.ReLU(),\n",
    "            nn.Linear(128,256), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(512,256), nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256,128), nn.ReLU(),\n",
    "            nn.Linear(128,num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        feat = self.mlp1(x)\n",
    "        g = feat.max(dim=1).values\n",
    "        g = g.unsqueeze(1).expand(-1, feat.shape[1], -1)\n",
    "        z = torch.cat([feat,g], dim=2)\n",
    "        return self.mlp2(z)\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD BLOCK FILES\n",
    "# -----------------------------\n",
    "files = sorted(glob.glob(os.path.join(BLOCKS_DIR,\"*.npz\")))\n",
    "np.random.shuffle(files)\n",
    "\n",
    "# FAST TRAIN subset\n",
    "train_files = files[:1200]\n",
    "val_files   = files[1200:1400]\n",
    "\n",
    "print(\"Train blocks:\",len(train_files))\n",
    "print(\"Val blocks:\",len(val_files))\n",
    "\n",
    "train_ds = NPZBlocks(train_files,to_idx)\n",
    "val_ds   = NPZBlocks(val_files,to_idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=NUM_WORKERS,\n",
    "                          pin_memory=PIN_MEMORY,\n",
    "                          persistent_workers=True,\n",
    "                          collate_fn=collate_pad)\n",
    "\n",
    "val_loader = DataLoader(val_ds,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=False,\n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        pin_memory=PIN_MEMORY,\n",
    "                        persistent_workers=True,\n",
    "                        collate_fn=collate_pad)\n",
    "\n",
    "# -----------------------------\n",
    "# INIT MODEL\n",
    "# -----------------------------\n",
    "tmp = np.load(train_files[0])\n",
    "IN_CH = tmp[\"X\"].shape[1]\n",
    "\n",
    "model = PointNetSeg(IN_CH,NUM_CLASSES).to(DEVICE)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "# -----------------------------\n",
    "# TRAIN LOOP\n",
    "# -----------------------------\n",
    "best_loss = 999\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for X,y in tqdm(train_loader,desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        X = X.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        logits = model(X)\n",
    "        loss = F.cross_entropy(logits.reshape(-1,NUM_CLASSES),\n",
    "                               y.reshape(-1))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss/len(train_loader)\n",
    "    print(\"Train loss:\",avg_loss)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X,y in val_loader:\n",
    "            X = X.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            logits = model(X)\n",
    "            loss = F.cross_entropy(logits.reshape(-1,NUM_CLASSES),\n",
    "                                   y.reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(\"Val loss:\",val_loss)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(\"Saved best model\")\n",
    "\n",
    "print(\"Training finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab5a311-b5e7-4837-be27-9f1bf0b83798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "BLOCKS_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\"\n",
    "OUT_DIR    = r\"D:/lidarrrrr/anbu/dl_models\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = os.path.join(OUT_DIR, \"pointnet_seg_best.pt\")\n",
    "\n",
    "VALID_CLASSES = [1,2,3,6,7,12,13]\n",
    "to_idx = {c:i for i,c in enumerate(VALID_CLASSES)}\n",
    "NUM_CLASSES = len(VALID_CLASSES)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de9079e-f32c-44e5-be17-8af7b4df68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Allocated MB:\", torch.cuda.memory_allocated()/1024/1024)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,      # important\n",
    "    pin_memory=True     # important for GPU\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "points = points.to(device, non_blocking=True)\n",
    "labels = labels.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "736a0efb-ea78-47ae-b39d-1ab8ea47695e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.10.0+cpu\n",
      "CUDA available: False\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FULL TRAINING CODE (PyTorch)  Point Cloud Block Classifier\n",
    "# Works with your saved blocks (.npz) and uses GPU (RTX 3050)\n",
    "# Includes:\n",
    "#   - fast DataLoader (num_workers + pin_memory)\n",
    "#   - non_blocking GPU transfers\n",
    "#   - class weights for imbalance\n",
    "#   - tqdm progress\n",
    "#   - saves best model\n",
    "# ============================================================\n",
    "\n",
    "import os, glob, math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# ----------------------------\n",
    "# EDIT THESE PATHS\n",
    "# ----------------------------\n",
    "BLOCKS_DIR = r\"D:/lidarrrrr/anbu/dl_dataset/blocks\" # <-- your folder that contains 3543 .npz blocks\n",
    "OUT_DIR    = r\"D:/lidarrrrr/anbu/dl_models\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_OUT  = os.path.join(OUT_DIR, \"pointnet_block_classifier_best.pt\")\n",
    "\n",
    "# ----------------------------\n",
    "# SETTINGS\n",
    "# ----------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 8               # RTX 3050 safe (increase if VRAM allows)\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NUM_WORKERS = 4              # try 4; if any crash set 2\n",
    "PIN_MEMORY = True\n",
    "VAL_SPLIT = 0.15\n",
    "non_blocking=True\n",
    "\n",
    "# you said feature count per point = 10 in your blocks\n",
    "# points array shape must be (N, F) where F=10\n",
    "FEATURES_PER_POINT = 10\n",
    "\n",
    "# Your training showed these classes exist in blocks\n",
    "# We'll build mapping dynamically from labels found in blocks.\n",
    "# ----------------------------\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DATASET\n",
    "# ============================================================\n",
    "class NPZBlocksDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each .npz must contain:\n",
    "      - points: float32 array (N, F)  (F=10)\n",
    "      - labels: int array (N,)\n",
    "    \"\"\"\n",
    "    def __init__(self, blocks_dir):\n",
    "        self.files = sorted(glob.glob(os.path.join(blocks_dir, \"*.npz\")))\n",
    "        if not self.files:\n",
    "            raise RuntimeError(f\"No .npz blocks found in: {blocks_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "        d = np.load(path)\n",
    "        pts = d[\"points\"].astype(np.float32)     # (N,F)\n",
    "        lbl = d[\"labels\"].astype(np.int64)       # (N,)\n",
    "        # return as torch\n",
    "        return torch.from_numpy(pts), torch.from_numpy(lbl)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SIMPLE POINTNET-LIKE MODEL (per-point segmentation)\n",
    "# Input: (B, N, F)\n",
    "# Output: (B, N, C)\n",
    "# ============================================================\n",
    "class PointNetSmall(nn.Module):\n",
    "    def __init__(self, in_ch, num_classes):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(in_ch, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,N,F)\n",
    "        feat = self.mlp1(x)          # (B,N,256)\n",
    "        out  = self.mlp2(feat)       # (B,N,C)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# UTILS: compute class mapping and weights (fast sampling)\n",
    "# ============================================================\n",
    "def scan_classes_and_weights(dataset, max_blocks=400):\n",
    "    \"\"\"\n",
    "    Reads up to max_blocks blocks and estimates class counts.\n",
    "    Returns:\n",
    "      classes_sorted, class_to_idx, weights_tensor\n",
    "    \"\"\"\n",
    "    n = min(len(dataset), max_blocks)\n",
    "    counts = {}\n",
    "    rng = np.random.default_rng(0)\n",
    "    pick = rng.choice(len(dataset), size=n, replace=False)\n",
    "\n",
    "    for i in tqdm(pick, desc=\"Counting classes (sample)\"):\n",
    "        _, y = dataset[i]\n",
    "        y = y.numpy()\n",
    "        u, c = np.unique(y, return_counts=True)\n",
    "        for uu, cc in zip(u.tolist(), c.tolist()):\n",
    "            counts[uu] = counts.get(uu, 0) + cc\n",
    "\n",
    "    classes_sorted = sorted(counts.keys())\n",
    "    class_to_idx = {c:i for i,c in enumerate(classes_sorted)}\n",
    "\n",
    "    # weights = inverse frequency (normalized)\n",
    "    freqs = np.array([counts[c] for c in classes_sorted], dtype=np.float64)\n",
    "    inv = 1.0 / np.maximum(freqs, 1.0)\n",
    "    inv = inv / inv.mean()  # normalize mean to 1\n",
    "    weights = torch.tensor(inv, dtype=torch.float32)\n",
    "\n",
    "    print(\"Classes found:\", classes_sorted)\n",
    "    print(\"Counts(sample):\", counts)\n",
    "    print(\"Weights:\", {c: float(weights[class_to_idx[c]]) for c in classes_sorted})\n",
    "\n",
    "    return classes_sorted, class_to_idx, weights\n",
    "\n",
    "\n",
    "def remap_labels(y, class_to_idx):\n",
    "    \"\"\"\n",
    "    y: (B,N) long labels in original classes\n",
    "    returns mapped y: (B,N) in [0..C-1]\n",
    "    \"\"\"\n",
    "    # vectorized remap using numpy-like trick\n",
    "    # but easiest: build a LUT for small labels range\n",
    "    max_label = int(y.max().item())\n",
    "    lut = torch.full((max_label + 1,), -1, dtype=torch.long, device=y.device)\n",
    "    for k, v in class_to_idx.items():\n",
    "        if k <= max_label:\n",
    "            lut[k] = v\n",
    "    ym = lut[y]\n",
    "    return ym\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN / EVAL\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, class_to_idx):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_pts = 0\n",
    "    correct = 0\n",
    "\n",
    "    for pts, y in loader:\n",
    "        pts = pts.to(DEVICE, non_blocking=True)          # (B,N,F)\n",
    "        y   = y.to(DEVICE, non_blocking=True)            # (B,N)\n",
    "        y_m = remap_labels(y, class_to_idx)              # (B,N)\n",
    "\n",
    "        logits = model(pts)                              # (B,N,C)\n",
    "        loss = criterion(logits.reshape(-1, logits.shape[-1]), y_m.reshape(-1))\n",
    "\n",
    "        total_loss += float(loss.item()) * y.numel()\n",
    "        total_pts  += y.numel()\n",
    "\n",
    "        pred = logits.argmax(dim=-1)                     # (B,N)\n",
    "        correct += int((pred == y_m).sum().item())\n",
    "\n",
    "    avg_loss = total_loss / max(total_pts, 1)\n",
    "    acc = correct / max(total_pts, 1)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, class_to_idx, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_pts = 0\n",
    "    correct = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"train\", leave=False)\n",
    "    for pts, y in pbar:\n",
    "        pts = pts.to(DEVICE, non_blocking=True)\n",
    "        y   = y.to(DEVICE, non_blocking=True)\n",
    "        y_m = remap_labels(y, class_to_idx)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=(DEVICE==\"cuda\")):\n",
    "            logits = model(pts)  # (B,N,C)\n",
    "            loss = criterion(logits.reshape(-1, logits.shape[-1]), y_m.reshape(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += float(loss.item()) * y.numel()\n",
    "        total_pts  += y.numel()\n",
    "\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct += int((pred == y_m).sum().item())\n",
    "\n",
    "        pbar.set_postfix(loss=float(loss.item()))\n",
    "\n",
    "    avg_loss = total_loss / max(total_pts, 1)\n",
    "    acc = correct / max(total_pts, 1)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "def main():\n",
    "    dataset = NPZBlocksDataset(BLOCKS_DIR)\n",
    "    print(\"Blocks found:\", len(dataset))\n",
    "\n",
    "    # Split train/val\n",
    "    val_len = int(len(dataset) * VAL_SPLIT)\n",
    "    train_len = len(dataset) - val_len\n",
    "    train_ds, val_ds = random_split(dataset, [train_len, val_len],\n",
    "                                    generator=torch.Generator().manual_seed(42))\n",
    "    print(f\"Train blocks: {len(train_ds)} | Val blocks: {len(val_ds)}\")\n",
    "\n",
    "    # Compute class mapping + weights from TRAIN (sample)\n",
    "    # NOTE: random_split returns Subset, so dataset[i] still works\n",
    "    classes_sorted, class_to_idx, weights = scan_classes_and_weights(train_ds, max_blocks=500)\n",
    "    num_classes = len(classes_sorted)\n",
    "\n",
    "    # DataLoaders (FAST)\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        persistent_workers=(NUM_WORKERS > 0),\n",
    "        prefetch_factor=2 if NUM_WORKERS > 0 else None,\n",
    "        drop_last=False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        persistent_workers=(NUM_WORKERS > 0),\n",
    "        prefetch_factor=2 if NUM_WORKERS > 0 else None,\n",
    "        drop_last=False\n",
    "    )\n",
    "    print(\"STEP 1: main() started\")\n",
    "\n",
    "    print(\"STEP 2: building dataset...\")\n",
    "    train_dataset = MyDataset(TRAIN_DIR)\n",
    "    val_dataset   = MyDataset(VAL_DIR)\n",
    "    print(\"STEP 3: dataset ok\", len(train_dataset), len(val_dataset))\n",
    "\n",
    "    print(\"STEP 4: building loaders...\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=0, pin_memory=False)   # IMPORTANT: 0 for debug\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                            num_workers=0, pin_memory=False)\n",
    "    print(\"STEP 5: loaders ok\")\n",
    "\n",
    "    print(\"STEP 6: getting one batch...\")\n",
    "    pts, lbl = next(iter(train_loader))\n",
    "    print(\"STEP 7: first batch shapes:\", pts.shape, lbl.shape)\n",
    "\n",
    "    # stop here for now\n",
    "    return\n",
    "\n",
    "    # Model\n",
    "    model = PointNetSmall(in_ch=FEATURES_PER_POINT, num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "    # Loss with class weights\n",
    "    weights = weights.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "    best_val = 1e9\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, class_to_idx, scaler)\n",
    "        va_loss, va_acc = evaluate(model, val_loader, criterion, class_to_idx)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d}/{EPOCHS} | \"\n",
    "              f\"train loss={tr_loss:.4f} acc={tr_acc:.4f} | \"\n",
    "              f\"val loss={va_loss:.4f} acc={va_acc:.4f}\")\n",
    "\n",
    "        # Save best\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            ckpt = {\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"classes_sorted\": classes_sorted,   # maps idx->original class\n",
    "                \"class_to_idx\": class_to_idx,\n",
    "                \"features_per_point\": FEATURES_PER_POINT\n",
    "            }\n",
    "            torch.save(ckpt, MODEL_OUT)\n",
    "            print(\" Saved best model:\", MODEL_OUT)\n",
    "\n",
    "        # quick GPU memory print\n",
    "        if DEVICE == \"cuda\":\n",
    "            print(\"GPU allocated MB:\", torch.cuda.memory_allocated()/1024/1024)\n",
    "\n",
    "    print(\"DONE training.\")\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f332d40-1e23-4ef2-ac4b-5fa8e636291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e2f281-b3fb-430d-b58b-365deb7a1fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f3829-164a-446b-99be-316f6a3c8fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lidar)",
   "language": "python",
   "name": "lidar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
